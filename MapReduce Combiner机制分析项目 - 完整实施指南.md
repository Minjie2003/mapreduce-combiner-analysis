# MapReduce Combineræœºåˆ¶åˆ†æé¡¹ç›® - å®Œæ•´å®æ–½æŒ‡å—

æˆ‘ä¼šç»™ä½ ä¸€ä¸ª**è¶…è¯¦ç»†çš„ã€å¯ç›´æ¥æ‰§è¡Œ**çš„å®Œæ•´æŒ‡å—ã€‚

------

## ğŸ“‹ é¡¹ç›®æ€»è§ˆ

**ç›®æ ‡**: é€šè¿‡å®éªŒéªŒè¯Combineråœ¨ä¸åŒæ•°æ®åˆ†å¸ƒä¸‹å¯¹MapReduceæ€§èƒ½çš„å½±å“

**æ—¶é—´**: 28å¤©

**äº¤ä»˜ç‰©**:

- å¯è¿è¡Œçš„ä»£ç 
- å®Œæ•´çš„å®éªŒæ•°æ®
- è¯¦ç»†çš„åˆ†ææŠ¥å‘Š

------

# ğŸ—“ï¸ Week 1: ç¯å¢ƒæ­å»º + ç†è®ºå‡†å¤‡

## Day 1-2: Dockerç¯å¢ƒæ­å»º

### Step 1: å®‰è£…Docker Desktop

**Windowsç”¨æˆ·ï¼š**

bash

```bash
# 1. ä¸‹è½½Docker Desktop
# è®¿é—®: https://www.docker.com/products/docker-desktop/

# 2. å®‰è£…å¹¶é‡å¯ç”µè„‘

# 3. éªŒè¯å®‰è£…
docker --version
docker-compose --version
```

**é…ç½®Dockerèµ„æºï¼š**

- æ‰“å¼€Docker Desktop
- Settings â†’ Resources
- Memory: è‡³å°‘ 8GB
- CPUs: è‡³å°‘ 4æ ¸
- Disk: è‡³å°‘ 50GB

### Step 2: åˆ›å»ºé¡¹ç›®ç»“æ„

bash

```bash
# åˆ›å»ºé¡¹ç›®æ ¹ç›®å½•
mkdir mapreduce-combiner-analysis
cd mapreduce-combiner-analysis

# åˆ›å»ºå­ç›®å½•
mkdir -p code/mapreduce/with-combiner
mkdir -p code/mapreduce/without-combiner
mkdir -p code/data-generator
mkdir -p code/scripts
mkdir -p code/utils
mkdir -p docs/images
mkdir -p results/raw-data
mkdir -p results/charts
mkdir -p cluster-config
```

### Step 3: åˆ›å»ºDockeré…ç½®

**åˆ›å»º `cluster-config/docker-compose.yml`:**

docker-compose.yml

Code 

version: '3.8' services:  namenode:    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8    container_name: namenode    restart: always    ports:      - 9870:9870  # NameNode Web UI      - 9000:9000  # HDFSç«¯å£    volumes:      - hadoop_

**åˆ›å»º `cluster-config/hadoop.env`:**

hadoop.env

Code 

\# Coreé…ç½® CORE_CONF_fs_defaultFS=hdfs://namenode:9000 CORE_CONF_hadoop_http_staticuser_user=root CORE_CONF_hadoop_proxyuser_hue_hosts=* CORE_CONF_hadoop_proxyuser_hue_groups=* CORE_CONF_io_compression_codecs=org.apache.hadoop.io.compress.SnappyCodec

### Step 4: å¯åŠ¨é›†ç¾¤

bash

```bash
# è¿›å…¥é…ç½®ç›®å½•
cd cluster-config

# åˆ›å»ºå…±äº«ç›®å½•
mkdir shared

# å¯åŠ¨é›†ç¾¤ï¼ˆé¦–æ¬¡å¯åŠ¨ä¼šä¸‹è½½é•œåƒï¼Œéœ€è¦5-10åˆ†é’Ÿï¼‰
docker-compose up -d

# æŸ¥çœ‹å¯åŠ¨çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—ï¼ˆç¡®ä¿æ²¡æœ‰é”™è¯¯ï¼‰
docker-compose logs -f

# ç­‰å¾…æ‰€æœ‰æœåŠ¡å¯åŠ¨ï¼ˆçº¦2-3åˆ†é’Ÿï¼‰
```

### Step 5: éªŒè¯é›†ç¾¤

**1. æ£€æŸ¥Webç•Œé¢ï¼š**

- NameNode: http://localhost:9870
- YARN: http://localhost:8088
- JobHistory: http://localhost:8188

**æˆªå›¾è¦æ±‚ï¼š**

- æ‰“å¼€NameNodeç•Œé¢ï¼Œæˆªå›¾æ˜¾ç¤º"Live Nodes: 3"
- æ‰“å¼€YARNç•Œé¢ï¼Œæˆªå›¾æ˜¾ç¤ºResourceManagerè¿è¡Œæ­£å¸¸
- ç¡®ä¿æˆªå›¾ä¸­åŒ…å«ä½ çš„ç”µè„‘ç”¨æˆ·åæˆ–æ—¶é—´æˆ³

**2. æµ‹è¯•HDFSï¼š**

bash

```bash
# è¿›å…¥namenodeå®¹å™¨
docker exec -it namenode bash

# æŸ¥çœ‹HDFSçŠ¶æ€
hdfs dfsadmin -report

# åº”è¯¥çœ‹åˆ°3ä¸ªDataNodeï¼Œæ¯ä¸ªçŠ¶æ€ä¸ºLive

# åˆ›å»ºæµ‹è¯•ç›®å½•
hdfs dfs -mkdir -p /user/root/test

# æµ‹è¯•æ–‡ä»¶ä¸Šä¼ 
echo "Hello Hadoop Cluster" > /tmp/test.txt
hdfs dfs -put /tmp/test.txt /user/root/test/

# æŸ¥çœ‹æ–‡ä»¶
hdfs dfs -ls /user/root/test/
hdfs dfs -cat /user/root/test/test.txt

# é€€å‡ºå®¹å™¨
exit
```

**æˆªå›¾ï¼š**

- `hdfs dfsadmin -report` çš„è¾“å‡º
- `hdfs dfs -ls` çš„è¾“å‡º

------

## Day 3: è¿è¡ŒWordCountéªŒè¯

### Step 1: å‡†å¤‡æµ‹è¯•æ•°æ®

bash

```bash
docker exec -it namenode bash

# åˆ›å»ºè¾“å…¥æ•°æ®
cat > /tmp/input.txt << EOF
hello world hello hadoop
hello docker hello mapreduce
hadoop mapreduce yarn hdfs
combiner reducer mapper shuffle
data processing big data
EOF

# ä¸Šä¼ åˆ°HDFS
hdfs dfs -mkdir -p /input
hdfs dfs -put /tmp/input.txt /input/
hdfs dfs -ls /input/
```

### Step 2: è¿è¡Œè‡ªå¸¦WordCount

bash

~~~bash
# è¿è¡ŒWordCountç¤ºä¾‹
hadoop jar /opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar \
  wordcount /input /output

# æŸ¥çœ‹ç»“æœ
hdfs dfs -cat /output/part-r-00000
```

**é¢„æœŸè¾“å‡ºï¼š**
```
big     1
combiner        1
data    2
docker  1
hadoop  2
...
~~~

### Step 3: åœ¨YARNæŸ¥çœ‹ä½œä¸š

1. è®¿é—® http://localhost:8088

2. ç‚¹å‡» "Applications" â†’ æ‰¾åˆ°åˆšæ‰çš„WordCountä½œä¸š

3. ç‚¹å‡» "ApplicationMaster" â†’ æŸ¥çœ‹ä½œä¸šè¯¦æƒ…

4. æˆªå›¾ä¿å­˜

   ï¼š

   - ä½œä¸šå®ŒæˆçŠ¶æ€
   - æ‰§è¡Œæ—¶é—´
   - Map/Reduceä»»åŠ¡æ•°é‡

### Step 4: æŸ¥çœ‹ä½œä¸šæ—¥å¿—

bash

```bash
# æŸ¥çœ‹ä½œä¸šå†å²
yarn application -list -appStates ALL

# è·å–Application ID (æ ¼å¼: application_xxxxxxxxxx_xxxx)
# æŸ¥çœ‹æ—¥å¿—
yarn logs -applicationId <Application_ID>
```

**é€€å‡ºå®¹å™¨ï¼š**

bash

```bash
exit
```

------

## Day 4: ç¼–å†™ç¯å¢ƒæ–‡æ¡£

åˆ›å»º `docs/setup-guide.md`ï¼Œè®°å½•ï¼š

1. é›†ç¾¤é…ç½®ä¿¡æ¯

   ï¼š

   - èŠ‚ç‚¹æ•°é‡ï¼š1 NameNode + 3 DataNode
   - èµ„æºé…ç½®ï¼šCPUã€å†…å­˜ã€ç£ç›˜
   - Hadoopç‰ˆæœ¬ï¼š3.2.1
   - JDKç‰ˆæœ¬ï¼š8

2. éƒ¨ç½²æ­¥éª¤

   ï¼š

   - Dockerå®‰è£…è¿‡ç¨‹
   - é…ç½®æ–‡ä»¶è¯´æ˜
   - å¯åŠ¨å‘½ä»¤

3. éªŒè¯æˆªå›¾

   ï¼š

   - æ’å…¥æ‰€æœ‰ä¹‹å‰çš„æˆªå›¾
   - æ·»åŠ è¯´æ˜æ–‡å­—

4. **å¸¸ç”¨å‘½ä»¤é€ŸæŸ¥**ï¼š

bash

```bash
   # å¯åŠ¨é›†ç¾¤
   docker-compose up -d
   
   # åœæ­¢é›†ç¾¤
   docker-compose down
   
   # æŸ¥çœ‹æ—¥å¿—
   docker-compose logs [service_name]
   
   # è¿›å…¥å®¹å™¨
   docker exec -it namenode bash
   
   # HDFSå‘½ä»¤
   hdfs dfs -ls /
   hdfs dfs -put local_file hdfs_path
   hdfs dfs -get hdfs_path local_file
```

------

## Day 5-7: ç†è®ºå­¦ä¹  + å®éªŒè®¾è®¡

### Combineræœºåˆ¶ç ”ç©¶ï¼ˆæˆå‘˜C+Dè´Ÿè´£ï¼‰

**ä»»åŠ¡ï¼š**

1. é˜…è¯»Hadoopå®˜æ–¹æ–‡æ¡£å…³äºCombinerçš„éƒ¨åˆ†
2. ç†è§£Combinerçš„å·¥ä½œåŸç†
3. åˆ†æCombinerçš„é€‚ç”¨åœºæ™¯

**è¾“å‡ºæ–‡æ¡£ `docs/combiner-theory.md`ï¼š**

markdown

```markdown
# Combineræœºåˆ¶åŸç†

## 1. ä»€ä¹ˆæ˜¯Combinerï¼Ÿ
Combineræ˜¯è¿è¡Œåœ¨Mapç«¯çš„"æœ¬åœ°Reducer"ï¼Œåœ¨æ•°æ®å‘é€åˆ°Reduceä¹‹å‰è¿›è¡Œé¢„èšåˆã€‚

## 2. å·¥ä½œæµç¨‹
Map â†’ Combiner â†’ Shuffle â†’ Reducer

## 3. ä¼˜åŠ¿
- å‡å°‘ç½‘ç»œä¼ è¾“æ•°æ®é‡
- é™ä½Reducerè´Ÿè½½
- æå‡æ•´ä½“æ€§èƒ½

## 4. é™åˆ¶æ¡ä»¶
- å¿…é¡»æ»¡è¶³ç»“åˆå¾‹å’Œäº¤æ¢å¾‹
- ä¾‹å¦‚ï¼šæ±‚å’Œâœ“ã€æ±‚å¹³å‡å€¼âœ—

## 5. é€‚ç”¨åœºæ™¯
- WordCount
- æ±‚å’Œã€è®¡æ•°
- æ‰¾æœ€å¤§/æœ€å°å€¼

## 6. ä¸é€‚ç”¨åœºæ™¯
- æ±‚å¹³å‡å€¼
- æ±‚ä¸­ä½æ•°
- éœ€è¦å…¨å±€è§†å›¾çš„è®¡ç®—
```

### å®éªŒæ–¹æ¡ˆè®¾è®¡

**åˆ›å»º `docs/experiment-design.md`ï¼š**

å®éªŒè®¾è®¡æ–¹æ¡ˆ

Document 

\# MapReduce Combineræœºåˆ¶åˆ†æ - å®éªŒè®¾è®¡æ–¹æ¡ˆ ## 1. ç ”ç©¶é—®é¢˜ ### æ ¸å¿ƒé—®é¢˜ 1. Combinerèƒ½å¦æœ‰æ•ˆå‡å°‘Shuffleé˜¶æ®µçš„æ•°æ®é‡ï¼Ÿ 2. åœ¨ä¸åŒkeyåˆ†å¸ƒä¸‹ï¼ŒCombinerçš„æ€§èƒ½æå‡æ•ˆæœæœ‰ä½•å·®å¼‚ï¼Ÿ 3. æ˜¯å¦æ‰€æœ‰åœºæ™¯éƒ½é€‚åˆä½¿ç”¨Combinerï¼Ÿ ### å‡è®¾ - H1: Combinerèƒ½æ˜¾è‘—å‡å°‘Shuffleæ•°æ®é‡ï¼ˆé¢„æœŸå‡å°‘50%+ï¼‰ - H2: æ•°æ®å€¾æ–œè¶Šä¸¥é‡ï¼ŒCombineræ•ˆæœè¶Šä¸æ˜æ˜¾ - H3: æŸäº›è®¡ç®—åœºæ™¯ä¸é€‚åˆä½¿ç”¨Combiner

------

# ğŸ—“ï¸ Week 2: ä»£ç å¼€å‘

## Day 8-10: æ•°æ®ç”Ÿæˆå™¨å¼€å‘

### Step 1: åˆ›å»ºæ•°æ®ç”Ÿæˆå™¨

**åˆ›å»º `code/data-generator/DataGenerator.java`:**

DataGenerator.java

Code 

import java.io.*; import java.util.*; /** * æ•°æ®ç”Ÿæˆå™¨ - ç”Ÿæˆä¸åŒåˆ†å¸ƒçš„WordCountæµ‹è¯•æ•°æ® */ public class DataGenerator {        private static final String[] WORDS = {        "hadoop", "mapreduce", "combiner", "reducer", "mapper",         "shuffle", "partitio

### Step 2: ç¼–è¯‘å’Œæµ‹è¯•æ•°æ®ç”Ÿæˆå™¨

bash

```bash
# ç¼–è¯‘
cd code/data-generator
javac DataGenerator.java

# ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼ˆå…ˆç”Ÿæˆå°æ•°æ®æµ‹è¯•ï¼‰
java DataGenerator uniform test_uniform_10mb.txt 10
java DataGenerator skewed test_skewed_10mb_light.txt 10 0.5
java DataGenerator skewed test_skewed_10mb_heavy.txt 10 2.0

# æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶
ls -lh *.txt
```

### Step 3: ç”Ÿæˆå®Œæ•´å®éªŒæ•°æ®é›†

**åˆ›å»ºç”Ÿæˆè„šæœ¬ `code/data-generator/generate_all.sh`:**

bash

```bash
#!/bin/bash

echo "å¼€å§‹ç”Ÿæˆæ‰€æœ‰æ•°æ®é›†..."

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p ../../datasets

# å‡åŒ€åˆ†å¸ƒæ•°æ®é›†
echo "=== ç”Ÿæˆå‡åŒ€åˆ†å¸ƒæ•°æ®é›† ==="
java DataGenerator uniform ../../datasets/uniform_100mb.txt 100
java DataGenerator uniform ../../datasets/uniform_500mb.txt 500
java DataGenerator uniform ../../datasets/uniform_1gb.txt 1000

# å€¾æ–œåˆ†å¸ƒæ•°æ®é›†
echo "=== ç”Ÿæˆå€¾æ–œåˆ†å¸ƒæ•°æ®é›† ==="
# è½»åº¦å€¾æ–œ (alpha=0.5)
java DataGenerator skewed ../../datasets/skewed_light_100mb.txt 100 0.5
java DataGenerator skewed ../../datasets/skewed_light_500mb.txt 500 0.5

# ä¸­åº¦å€¾æ–œ (alpha=1.0)
java DataGenerator skewed ../../datasets/skewed_medium_100mb.txt 100 1.0
java DataGenerator skewed ../../datasets/skewed_medium_500mb.txt 500 1.0

# é‡åº¦å€¾æ–œ (alpha=2.0)
java DataGenerator skewed ../../datasets/skewed_heavy_100mb.txt 100 2.0
java DataGenerator skewed ../../datasets/skewed_heavy_500mb.txt 500 2.0

echo "æ‰€æœ‰æ•°æ®é›†ç”Ÿæˆå®Œæˆ!"
ls -lh ../../datasets/
```

bash

```bash
# ç»™è„šæœ¬æ‰§è¡Œæƒé™
chmod +x generate_all.sh

# è¿è¡Œï¼ˆè¿™ä¼šéœ€è¦ä¸€äº›æ—¶é—´ï¼‰
./generate_all.sh
```

------

## Day 11-13: MapReduceä½œä¸šå¼€å‘

### Step 1: ä¸å¸¦Combinerçš„WordCount

**åˆ›å»º `code/mapreduce/without-combiner/WordCountNoCombiner.java`:**

WordCountNoCombiner.java

Code 

import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apa

### Step 2: å¸¦Combinerçš„WordCount

**åˆ›å»º `code/mapreduce/with-combiner/WordCountWithCombiner.java`:**

WordCountWithCombiner.java

Code 

import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apa

### Step 3: ç¼–è¯‘MapReduceä½œä¸š

**åˆ›å»ºç¼–è¯‘è„šæœ¬ `code/scripts/compile.sh`:**

bash

```bash
#!/bin/bash

echo "å¼€å§‹ç¼–è¯‘MapReduceä½œä¸š..."

# è®¾ç½®Hadoop classpath
HADOOP_CLASSPATH=$(docker exec namenode hadoop classpath)

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p ../build

# ç¼–è¯‘ä¸å¸¦Combinerçš„ç‰ˆæœ¬
echo "ç¼–è¯‘ WordCountNoCombiner..."
javac -classpath "$HADOOP_CLASSPATH" \
    -d ../build \
    ../mapreduce/without-combiner/WordCountNoCombiner.java

# æ‰“åŒ…
cd ../build
jar -cvf WordCountNoCombiner.jar *.class
cd ../scripts

# ç¼–è¯‘å¸¦Combinerçš„ç‰ˆæœ¬
echo "ç¼–è¯‘ WordCountWithCombiner..."
javac -classpath "$HADOOP_CLASSPATH" \
    -d ../build \
    ../mapreduce/with-combiner/WordCountWithCombiner.java

# æ‰“åŒ…
cd ../build
jar -cvf WordCountWithCombiner.jar *.class
cd ../scripts

echo "ç¼–è¯‘å®Œæˆ!"
ls -lh ../build/*.jar
```

**Windowsä¸Šçš„æ›¿ä»£æ–¹æ¡ˆ - åœ¨Dockerå®¹å™¨å†…ç¼–è¯‘ï¼š**

bash

```bash
# 1. å°†ä»£ç å¤åˆ¶åˆ°å…±äº«ç›®å½•
cp -r code/mapreduce cluster-config/shared/

# 2. è¿›å…¥namenodeå®¹å™¨ç¼–è¯‘
docker exec -it namenode bash

# 3. åœ¨å®¹å™¨å†…ç¼–è¯‘
cd /shared/mapreduce

# ç¼–è¯‘ä¸å¸¦Combinerç‰ˆæœ¬
javac -classpath $(hadoop classpath) without-combiner/WordCountNoCombiner.java
cd without-combiner
jar -cvf WordCountNoCombiner.jar *.class
cd ..

# ç¼–è¯‘å¸¦Combinerç‰ˆæœ¬
javac -classpath $(hadoop classpath) with-combiner/WordCountWithCombiner.java
cd with-combiner
jar -cvf WordCountWithCombiner.jar *.class
cd ..

exit
```

------

## Day 14: è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬

**åˆ›å»º `code/scripts/run_experiment.sh`:**

run_experiment.sh

Code 

\#!/bin/bash # å®éªŒè‡ªåŠ¨åŒ–è„šæœ¬ # ç”¨æ³•: ./run_experiment.sh <dataset_name> <use_combiner> <run_number> if [ $# -ne 3 ]; then    echo "ç”¨æ³•: $0 <dataset_name> <use_combiner:yes|no> <run_number>"    echo "ç¤ºä¾‹: $0 uniform_100mb yes 1"    exit 1 fi DATASET=$1 USE

------

# ğŸ—“ï¸ Week 3: å®éªŒæ‰§è¡Œ

## Day 15-20: æ‰¹é‡è¿è¡Œå®éªŒ

### Step 1: åˆ›å»ºæ‰¹é‡å®éªŒè„šæœ¬

**åˆ›å»º `code/scripts/run_all_experiments.sh`:**

bash

```bash
#!/bin/bash

# æ‰¹é‡è¿è¡Œæ‰€æœ‰å®éªŒç»„åˆ
# æ¯ç»„å®éªŒè¿è¡Œ3æ¬¡å–å¹³å‡

echo "å¼€å§‹æ‰¹é‡å®éªŒ..."
echo "é¢„è®¡æ€»æ—¶é—´: çº¦2-3å°æ—¶"

# æ•°æ®é›†åˆ—è¡¨
DATASETS=(
    "uniform_100mb"
    "uniform_500mb"
    "skewed_light_100mb"
    "skewed_light_500mb"
    "skewed_medium_100mb"
    "skewed_medium_500mb"
    "skewed_heavy_100mb"
    "skewed_heavy_500mb"
)

# å¯¹æ¯ä¸ªæ•°æ®é›†è¿è¡Œå®éªŒ
for dataset in "${DATASETS[@]}"; do
    echo ""
    echo "================================================"
    echo "æµ‹è¯•æ•°æ®é›†: $dataset"
    echo "================================================"
    
    # è¿è¡Œ3æ¬¡ä¸å¸¦Combinerçš„å®éªŒ
    for run in 1 2 3; do
        echo "è¿è¡Œ: ä¸å¸¦Combiner - ç¬¬${run}æ¬¡"
        ./run_experiment.sh $dataset no $run
        sleep 30  # ç­‰å¾…é›†ç¾¤ç¨³å®š
    done
    
    # è¿è¡Œ3æ¬¡å¸¦Combinerçš„å®éªŒ
    for run in 1 2 3; do
        echo "è¿è¡Œ: å¸¦Combiner - ç¬¬${run}æ¬¡"
        ./run_experiment.sh $dataset yes $run
        sleep 30
    done
    
    echo "æ•°æ®é›† $dataset å®Œæˆ"
done

echo ""
echo "================================================"
echo "æ‰€æœ‰å®éªŒå®Œæˆ!"
echo "================================================"
echo "å¼€å§‹æ±‡æ€»ç»“æœ..."

# ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
python3 ../utils/summarize_results.py
```

bash

```bash
# ç»™è„šæœ¬æ‰§è¡Œæƒé™
chmod +x run_experiment.sh
chmod +x run_all_experiments.sh
```

### Step 2: åˆ›å»ºç»“æœæ±‡æ€»è„šæœ¬

**åˆ›å»º `code/utils/summarize_results.py`:**

summarize_results.py

Code 

\#!/usr/bin/env python3 """ å®éªŒç»“æœæ±‡æ€»è„šæœ¬ è§£ææ‰€æœ‰å®éªŒæ—¥å¿—ï¼Œç”ŸæˆCSVæ ¼å¼çš„æ±‡æ€»æ•°æ® """ import os import re import csv from collections import defaultdict import statistics def parse_result_file(filepath):    """è§£æå•ä¸ªç»“æœæ–‡ä»¶ï¼Œæå–å…³é”®æŒ‡æ ‡"""    metrics = {}        with open(filepa

### Step 3: å¼€å§‹å®éªŒï¼ˆåˆ†å·¥æ‰§è¡Œï¼‰

**æ‰§è¡Œè®¡åˆ’ï¼š**

1. **Day 15-16**ï¼šè¿è¡Œå°è§„æ¨¡æ•°æ®é›†ï¼ˆ100MBï¼‰

bash

```bash
   # æµ‹è¯•å•ä¸ªå®éªŒ
   cd code/scripts
   ./run_experiment.sh uniform_100mb no 1
   ./run_experiment.sh uniform_100mb yes 1
   
   # æ£€æŸ¥ç»“æœ
   cat ../../results/raw-data/uniform_100mb_without_combiner_run1.txt
```

1. **Day 17-18**ï¼šè¿è¡Œä¸­è§„æ¨¡æ•°æ®é›†ï¼ˆ500MBï¼‰

bash

```bash
   # å¯ä»¥å¹¶è¡Œè¿è¡Œå¤šä¸ªæ•°æ®é›†
   ./run_experiment.sh uniform_500mb no 1 &
   ./run_experiment.sh skewed_light_500mb no 1 &
   wait
```

1. Day 19-20

   ï¼šè¡¥å……å®éªŒ + å¼‚å¸¸å¤„ç†

   - é‡è·‘å¤±è´¥çš„å®éªŒ
   - éªŒè¯æ•°æ®å®Œæ•´æ€§
   - ç”Ÿæˆåˆæ­¥æ±‡æ€»

------

# ğŸ—“ï¸ Week 4: æ•°æ®åˆ†æä¸æŠ¥å‘Š

## Day 21-23: æ•°æ®å¯è§†åŒ–

### Step 1: åˆ›å»ºå¯è§†åŒ–è„šæœ¬

**åˆ›å»º `code/utils/visualize_results.py`:**

visualize_results.py

Code 

\#!/usr/bin/env python3 """ å®éªŒç»“æœå¯è§†åŒ–è„šæœ¬ ç”Ÿæˆè®ºæ–‡è´¨é‡çš„å›¾è¡¨ """ import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import numpy as np import os # è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼ plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans'] pl

### Step 2: è¿è¡Œå¯è§†åŒ–

bash

```bash
# å®‰è£…ä¾èµ–
pip3 install pandas matplotlib seaborn numpy

# ç”Ÿæˆå›¾è¡¨
cd code/utils
python3 visualize_results.py
```

------

## Day 24-27: æ’°å†™æŠ¥å‘Š

### æœ€ç»ˆREADME.mdç»“æ„

æŒ‰ç…§è€å¸ˆè¦æ±‚çš„æ¨¡æ¿å¡«å†™ï¼š

README.mdï¼ˆå®Œæ•´ç‰ˆï¼‰

Document 

\# MapReduce Combineræœºåˆ¶åˆ†æ ## ç ”ç©¶ç›®çš„ æ¢ç©¶MapReduceä¸­Combinerå¯¹ä½œä¸šæ€§èƒ½çš„å½±å“ï¼Œä¸ºåˆ†å¸ƒå¼è®¡ç®—åœºæ™¯ä¸‹çš„æ€§èƒ½ä¼˜åŒ–æä¾›å®è·µä¾æ®ã€‚ ## ç ”ç©¶å†…å®¹ åˆ†æCombineråœ¨MapReduceæ‰§è¡Œè¿‡ç¨‹ä¸­çš„ä½œç”¨ä¸æ•ˆæœã€‚å…·ä½“åŒ…æ‹¬ï¼š 1. Combineræ˜¯å¦èƒ½å¤Ÿæœ‰æ•ˆå‡å°‘Shuffleé˜¶æ®µçš„æ•°æ®é‡ï¼Ÿ 2. åœ¨ä¸åŒçš„keyåˆ†å¸ƒï¼ˆå‡åŒ€åˆ†å¸ƒä¸æ•°æ®å€¾æ–œï¼‰ä¸‹ï¼Œå…¶æ€§èƒ½æå‡æ•ˆæœæœ‰ä½•å·®å¼‚ï¼Ÿ 3. æ˜¯å¦æ‰€æœ‰åœºæ™¯éƒ½é€‚åˆä½¿ç”¨Combinerï¼Ÿ **ã€æ–°å¢ç ”ç©¶å†…å®¹ã€‘** 4. ä¸åŒæ•°æ®è§„

------

## Day 28: æœ€ç»ˆæ£€æŸ¥ä¸æäº¤

### æ£€æŸ¥æ¸…å•

bash

```bash
# 1. ç¡®ä¿æ‰€æœ‰ä»£ç å¯è¿è¡Œ
cd code/mapreduce/with-combiner
jar -tf WordCountWithCombiner.jar

# 2. éªŒè¯æ‰€æœ‰æ•°æ®æ–‡ä»¶å­˜åœ¨
ls -lh datasets/
ls -lh results/raw-data/
ls -lh results/charts/

# 3. æ£€æŸ¥æ–‡æ¡£å®Œæ•´æ€§
- [ ] README.md å®Œæ•´
- [ ] docs/setup-guide.md
- [ ] docs/experiment-design.md
- [ ] æ‰€æœ‰æˆªå›¾å·²æ’å…¥

# 4. Gitæäº¤
git add .
git commit -m "feat: å®ŒæˆMapReduce Combineræœºåˆ¶åˆ†æé¡¹ç›®"
git push origin main
```

------

## ğŸ’¡ å…³é”®Tips

### Gitåä½œæœ€ä½³å®è·µ

bash

```bash
# ç¬¬ä¸€æ¬¡å…‹éš†
git clone <your-repo-url>
cd mapreduce-combiner-analysis

# åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
git checkout -b feature/your-name-your-task

# æäº¤ä½ çš„å·¥ä½œ
git add .
git commit -m "feat: å®ŒæˆXXXåŠŸèƒ½"
git push origin feature/your-name-your-task

# åˆ›å»ºPull Requeståœ¨GitHubä¸Š
# å…¶ä»–æˆå‘˜å®¡æŸ¥ååˆå¹¶
```

### å¸¸è§é—®é¢˜è§£å†³

**Q1: Dockerå®¹å™¨æ— æ³•å¯åŠ¨**

bash

```bash
# æ£€æŸ¥ç«¯å£å ç”¨
netstat -ano | findstr 9870

# é‡å¯Docker
docker-compose down
docker-compose up -d
```

**Q2: Hadoopä½œä¸šå¤±è´¥**

bash

```bash
# æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
docker exec namenode bash
yarn logs -applicationId <app_id>

# æ£€æŸ¥HDFSç©ºé—´
hdfs dfs -df -h
```

**Q3: æ•°æ®å€¾æ–œä¸æ˜æ˜¾**

bash

```bash
# å¢å¤§alphaå‚æ•°
java DataGenerator skewed test.txt 100 3.0

# éªŒè¯åˆ†å¸ƒ
python3 -c "
import collections
words = open('test.txt').read().split()
c = collections.Counter(words)
print('Top 10:', c.most_common(10))
"
```

------

## ğŸ“š å‚è€ƒèµ„æ–™

1. Hadoopå®˜æ–¹æ–‡æ¡£: https://hadoop.apache.org/docs/r3.2.1/
2. MapReduceè®ºæ–‡: Dean, Jeffrey, and Sanjay Ghemawat. "MapReduce: simplified data processing on large clusters."
3. Dockeræ–‡æ¡£: https://docs.docker.com/

------

## ğŸ¯ æ€»ç»“

è¿™ä¸ª28å¤©çš„è¯¦ç»†è®¡åˆ’æ¶µç›–äº†ï¼š

1. âœ… ç¯å¢ƒæ­å»ºï¼ˆDockeré›†ç¾¤ï¼‰
2. âœ… æ•°æ®ç”Ÿæˆï¼ˆ8ä¸ªä¸åŒåˆ†å¸ƒçš„æ•°æ®é›†ï¼‰
3. âœ… ä»£ç å¼€å‘ï¼ˆå¸¦/ä¸å¸¦Combinerä¸¤ä¸ªç‰ˆæœ¬ï¼‰
4. âœ… è‡ªåŠ¨åŒ–å®éªŒï¼ˆæ‰¹é‡æ‰§è¡Œè„šæœ¬ï¼‰
5. âœ… æ•°æ®åˆ†æï¼ˆç»Ÿè®¡è„šæœ¬ï¼‰
6. âœ… å¯è§†åŒ–ï¼ˆ5å¼ ä¸“ä¸šå›¾è¡¨ï¼‰
7. âœ… å®Œæ•´æŠ¥å‘Šï¼ˆç¬¦åˆæ¨¡æ¿è¦æ±‚ï¼‰