# Hadoop MapReduce Combineræ€§èƒ½åˆ†æé¡¹ç›®

# ğŸ“‹ é¡¹ç›®æ¦‚è¿°

**ç ”ç©¶ç›®æ ‡**: é€šè¿‡å®éªŒéªŒè¯Combineråœ¨ä¸åŒæ•°æ®åˆ†å¸ƒä¸‹å¯¹MapReduceæ€§èƒ½çš„å½±å“

**æ ¸å¿ƒç ”ç©¶é—®é¢˜:**

1. åˆ†æCombineråœ¨MapReduceæ‰§è¡Œè¿‡ç¨‹ä¸­çš„ä½œç”¨ä¸æ•ˆæœ
2. Combineræ˜¯å¦èƒ½å¤Ÿæœ‰æ•ˆå‡å°‘Shuffleé˜¶æ®µçš„æ•°æ®é‡?
3. åœ¨ä¸åŒçš„keyåˆ†å¸ƒ(å‡åŒ€åˆ†å¸ƒä¸æ•°æ®å€¾æ–œ)ä¸‹,æ€§èƒ½æå‡æ•ˆæœæœ‰ä½•å·®å¼‚?
4. æ˜¯å¦æ‰€æœ‰åœºæ™¯éƒ½é€‚åˆä½¿ç”¨Combiner?

**é¡¹ç›®ç»“æ„:**

```
â”œâ”€code
â”‚  â”œâ”€cluster-config
â”‚  â”œâ”€data
â”‚  â”‚  â”‚  skewed_data.txt
â”‚  â”‚  â”‚  uniform_data.txt
â”‚  â”‚  â”‚  unique_data.txt
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€data-generator
â”‚  â”‚          generate_all_data.py
â”‚  â”‚          validate_data.py
â”‚  â”‚
â”‚  â”œâ”€mapreduce
â”‚  â”‚  â”œâ”€with-combiner
â”‚  â”‚  â”‚  â””â”€com
â”‚  â”‚  â”‚      â””â”€hadoop
â”‚  â”‚  â”‚          â””â”€wordcount
â”‚  â”‚  â”‚              â””â”€with
â”‚  â”‚  â”‚                      WordCountDriver.java
â”‚  â”‚  â”‚                      WordCountMapper.java
â”‚  â”‚  â”‚                      WordCountReducer.java
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€without-combiner
â”‚  â”‚      â””â”€com
â”‚  â”‚          â””â”€hadoop
â”‚  â”‚              â””â”€wordcount
â”‚  â”‚                  â””â”€without
â”‚  â”‚                          WordCountDriver.java
â”‚  â”‚                          WordCountMapper.java
â”‚  â”‚                          WordCountReducer.java
â”‚  â”‚
â”‚  â”œâ”€results
â”‚  â”‚      performance_metrics.csv
â”‚  â”‚      skewed_without_combiner.log
â”‚  â”‚      skewed_with_combiner.log
â”‚  â”‚      uniform_without_combiner.log
â”‚  â”‚      uniform_with_combiner.log
â”‚  â”‚      unique_without_combiner.log
â”‚  â”‚      unique_with_combiner.log
â”‚  â”‚
â”‚  â””â”€scripts
â”‚          analyze_results.sh
â”‚          compile_and_package.sh
â”‚          run_experiments.sh
â”‚          upload_data.sh

```

# Step1ï¼šä¼ªåˆ†å¸ƒå¼é›†ç¾¤ç¯å¢ƒæ­å»º

## 1. é¢„å‡†å¤‡

### 1.1 è™šæ‹Ÿæœºç½‘ç»œé…ç½®

**æŸ¥çœ‹IPåœ°å€**

~~~bash
ip addr
```

**é›†ç¾¤IPè§„åˆ’:**
```
192.168.204.132  master
192.168.204.133  worker01
192.168.204.134  worker02
~~~

**ä¿®æ”¹ä¸»æœºå** (åœ¨å„è‡ªè™šæ‹Ÿæœºä¸Šæ‰§è¡Œ):

```bash
hostnamectl set-hostname master
hostnamectl set-hostname worker01
hostnamectl set-hostname worker02
```

**é…ç½®é™æ€IP:**

```bash
vi /etc/sysconfig/network-scripts/ifcfg-ens33
```

ä¿®æ”¹å†…å®¹:

```ini
BOOTPROTO="static"
IPADDR=192.168.204.132      # å„èŠ‚ç‚¹ä½¿ç”¨å¯¹åº”IP
NETMASK=255.255.255.0
GATEWAY=192.168.204.2
DNS1=8.8.8.8
```

**é…ç½®ä¸»æœºæ˜ å°„:**

~~~bash
vi /etc/hosts
```

æ·»åŠ å†…å®¹:
```
192.168.204.132  master
192.168.204.133  worker01
192.168.204.134  worker02
~~~

**é‡å¯å¹¶æµ‹è¯•ç½‘ç»œ:**

```bash
reboot

# é‡å¯åæµ‹è¯•
ifconfig
ping baidu.com
```

### 1.2 SSHå…å¯†ç™»å½•é…ç½®

**é…ç½®YUMæºå¹¶å®‰è£…SSH:**

```bash
# é…ç½®é˜¿é‡Œäº‘YUMæº
vi /etc/yum.repos.d/CentOS-Base.repo
# ä¿®æ”¹ä¸º: http://mirrors.aliyun.com/centos/$releasever/os/$basearch/

# å®‰è£…SSHæœåŠ¡
yum install openssh-server -y
```

**ç”Ÿæˆå¹¶åˆ†å‘å¯†é’¥:**

```bash
# ç”ŸæˆSSHå¯†é’¥å¯¹
ssh-keygen -t rsa

# åœ¨masterèŠ‚ç‚¹å¤åˆ¶å…¬é’¥åˆ°æœ¬åœ°
ssh-copy-id master

# åˆ†å‘å…¬é’¥åˆ°å…¶ä»–èŠ‚ç‚¹
scp /root/.ssh/authorized_keys worker01:/root/.ssh
scp /root/.ssh/authorized_keys worker02:/root/.ssh
```

**åˆ›å»ºå·¥ä½œç›®å½•:**

```bash
mkdir -p /export/data       # å­˜æ”¾æ•°æ®æ–‡ä»¶
mkdir -p /export/servers    # è½¯ä»¶å®‰è£…ç›®å½•
mkdir -p /export/software   # å®‰è£…åŒ…å­˜æ”¾ç›®å½•
```

**å®‰è£…åŸºç¡€å·¥å…·:**

```bash
yum install lrzsz -y
```

------

## 2. JDKå®‰è£…ä¸é…ç½®

### 2.1 å¸è½½ç³»ç»Ÿè‡ªå¸¦JDK



```bash
# æŸ¥çœ‹å·²å®‰è£…çš„JDK
rpm -qa | grep jdk

# å¸è½½æ‰€æœ‰JDKåŒ…
rpm -e --nodeps java-1.7.0-openjdk-headless-1.7.0.261-2.6.22.2.el7_8.x86_64
rpm -e --nodeps copy-jdk-configs-3.3-10.el7_5.noarch
rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.262.b10-1.el7.x86_64
rpm -e --nodeps java-1.8.0-openjdk-1.8.0.262.b10-1.el7.x86_64
rpm -e --nodeps java-1.7.0-openjdk-1.7.0.261-2.6.22.2.el7_8.x86_64
```

### 2.2 å®‰è£…JDK 1.8

**ä¸‹è½½åœ°å€:** [Oracle JDK 8u181](https://www.oracle.com/java/technologies/javase/javase8-archive-downloads.html)

```bash
cd /export/software

# ä¸Šä¼ JDKå®‰è£…åŒ…
rz  # é€‰æ‹© jdk-8u181-linux-x64.tar.gz

# è§£å‹åˆ°å®‰è£…ç›®å½•
tar zxvf jdk-8u181-linux-x64.tar.gz -C /export/servers/

# é‡å‘½å
cd /export/servers
mv jdk1.8.0_181 jdk
```

**é…ç½®ç¯å¢ƒå˜é‡:**

```bash
vi /etc/profile
```

æ·»åŠ ä»¥ä¸‹å†…å®¹:

```bash
export JAVA_HOME=/export/servers/jdk
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
```

é‡æ–°åŠ è½½å¹¶éªŒè¯:

```bash
source /etc/profile
java -version
```

**åˆ†å‘JDKåˆ°å…¶ä»–èŠ‚ç‚¹:**

```bash
cd /export/servers
scp -r jdk worker01:$PWD
scp -r jdk worker02:$PWD

scp /etc/profile worker01:/etc/profile
scp /etc/profile worker02:/etc/profile

ssh worker01 "source /etc/profile"
ssh worker02 "source /etc/profile"
```

------

## 3. Hadoopå®‰è£…ä¸é…ç½®

### 3.1 å®‰è£…Hadoop

**ä¸‹è½½åœ°å€:** [Hadoop 2.7.3](https://archive.apache.org/dist/hadoop/core/hadoop-2.7.3/hadoop-2.7.3.tar.gz)

```bash
cd /export/software
rz  # é€‰æ‹© hadoop-2.7.3.tar.gz

tar zxvf hadoop-2.7.3.tar.gz -C /export/servers/
cd /export/servers
mv hadoop-2.7.3 hadoop
```

**é…ç½®ç¯å¢ƒå˜é‡:**

```bash
vi /etc/profile
```

æ·»åŠ :

```bash
export HADOOP_HOME=/export/servers/hadoop
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
```

```bash
source /etc/profile
hadoop version
```

### 3.2 ä¿®æ”¹Hadoopé…ç½®æ–‡ä»¶

é…ç½®æ–‡ä»¶ä½ç½®: `/export/servers/hadoop/etc/hadoop`

**(1) hadoop-env.sh**

```bash
vi hadoop-env.sh

#æ·»åŠ ä»¥ä¸‹å†…å®¹
export JAVA_HOME=/export/servers/jdk
```

**(2) core-site.xml**

```bash
vi core-site.xml
```

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://master:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/export/servers/hadoop/tmp</value>
    </property>
</configuration>
```

**(3) hdfs-site.xml**

```bash
vi hdfs-site.xml
```

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>worker01:50090</value>
    </property>
</configuration>
```

**(4) mapred-site.xml**

```bash
cp mapred-site.xml.template mapred-site.xml
vi mapred-site.xml
```

```xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
```

**(5) yarn-site.xml**

```xml
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>
```

**(6) slaves**

~~~bash
vi slaves
```

ä¿®æ”¹ä¸º:
```
master
worker01
worker02
~~~

**åˆ†å‘Hadoopåˆ°å…¶ä»–èŠ‚ç‚¹:**

```bash
scp /etc/profile worker01:/etc/profile
scp /etc/profile worker02:/etc/profile

scp -r /export/ worker01:/
scp -r /export/ worker02:/

ssh worker01 "source /etc/profile"
ssh worker02 "source /etc/profile"
```

------

## 4. å¯åŠ¨Hadoopé›†ç¾¤

### 4.1 æ ¼å¼åŒ–HDFS

```bash
# æ³¨æ„:ä»…åœ¨é¦–æ¬¡å¯åŠ¨æ—¶æ‰§è¡Œä¸€æ¬¡
hdfs namenode -format
```

### 4.2 å…³é—­é˜²ç«å¢™

```bash
# åœ¨æ‰€æœ‰èŠ‚ç‚¹æ‰§è¡Œ
systemctl stop firewalld
systemctl disable firewalld
```

### 4.3 å¯åŠ¨é›†ç¾¤

```bash
start-dfs.sh    # å¯åŠ¨HDFS
start-yarn.sh   # å¯åŠ¨YARN
# æˆ–ä¸€é”®å¯åŠ¨: start-all.sh
```

### 4.4 éªŒè¯é›†ç¾¤çŠ¶æ€

```bash
jps
```

**é¢„æœŸè¾“å‡º:**

- **masterèŠ‚ç‚¹:** NameNode, SecondaryNameNode, DataNode, ResourceManager, NodeManager
- **workerèŠ‚ç‚¹:** DataNode, NodeManager

### 4.5 Webç•Œé¢è®¿é—®

- **HDFSç®¡ç†ç•Œé¢:** http://192.168.204.132:50070
  - ![image-20251125003542119](.\images\image-20251125003542119.png)
- **YARNèµ„æºç®¡ç†ç•Œé¢:** http://192.168.204.132:8088
  - ![image-20251125003455570](.\images\image-20251125003455570.png)

# Step 2ï¼šæ•°æ®ç”Ÿæˆ

æœ¬éƒ¨åˆ†å°†ç”Ÿæˆä¸¤ç§ä¸åŒåˆ†å¸ƒç‰¹å¾çš„æµ‹è¯•æ•°æ®ï¼Œç”¨äºéªŒè¯Combineråœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚

## 1. æ•°æ®ç”Ÿæˆç­–ç•¥

æˆ‘ä»¬å°†ç”Ÿæˆä¸‰ç§æ•°æ®é›†æ¥æ¨¡æ‹Ÿä¸åŒçš„å®é™…åœºæ™¯ï¼š

- **uniform**: 10000ä¸ªword_xxxxxå‡åŒ€åˆ†å¸ƒ
- **skewed**: 200ä¸ªhot_word_xxx (80%é¢‘ç‡) + 9800ä¸ªcold_word_xxxx (20%é¢‘ç‡)
- **unique**: 50000000ä¸ªuuid_xxxxxxxx (æ¯ä¸ªå”¯ä¸€)

------

## 2. Pythonæ•°æ®ç”Ÿæˆè„šæœ¬

**å»ºè®®æ–¹æ¡ˆï¼šåœ¨Windowsæœ¬åœ°ç”Ÿæˆæ•°æ®ï¼Œç„¶åé€šè¿‡rzä¸Šä¼ åˆ°è™šæ‹Ÿæœº**

- ä¸éœ€è¦åœ¨è™šæ‹Ÿæœºå®‰è£…Pythonç¯å¢ƒ
- Windowsä¸ŠPythonç¯å¢ƒé…ç½®æ›´æ–¹ä¾¿
- ç”Ÿæˆé€Ÿåº¦å¯èƒ½æ›´å¿«ï¼ˆæœ¬åœ°èµ„æºæ›´å……è¶³ï¼‰
- å¯ä»¥åœ¨æœ¬åœ°å…ˆéªŒè¯æ•°æ®å†ä¸Šä¼ 

**åœ¨æœ¬åœ°Windowsåˆ›å»ºé¡¹ç›®ç›®å½•**

```powershell
# åœ¨ä½ çš„é¡¹ç›®ç›®å½•ä¸‹
cd code/data-generator
```

### 2.1 ç”Ÿæˆæ•°æ®

åˆ›å»º `New-Item generate_all_data.py -ItemType File`:

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€çš„æ•°æ®ç”Ÿæˆè„šæœ¬
ç”Ÿæˆä¸‰ç§ä¸åŒåˆ†å¸ƒç‰¹å¾çš„WordCountæµ‹è¯•æ•°æ®
æ‰€æœ‰æ•°æ®æ ¼å¼ç»Ÿä¸€ï¼šæ¯è¡ŒåŒ…å«å¤šä¸ªå•è¯
"""

import random
import time
import os

# ==================== ç»Ÿä¸€é…ç½® ====================
NUM_LINES = 1000000          # æ€»è¡Œæ•°ï¼š100ä¸‡è¡Œ
WORDS_PER_LINE = 50          # æ¯è¡Œå•è¯æ•°ï¼š50ä¸ª
NUM_UNIQUE_WORDS = 10000     # å”¯ä¸€å•è¯æ€»æ•°ï¼š1ä¸‡ä¸ª

# æ•°æ®å€¾æ–œé…ç½®
HOT_WORD_COUNT = 200         # çƒ­ç‚¹å•è¯æ•°ï¼š200ä¸ª (2%)
COLD_WORD_COUNT = 9800       # å†·é—¨å•è¯æ•°ï¼š9800ä¸ª (98%)
HOT_RATIO = 0.8              # çƒ­ç‚¹å•è¯é¢‘ç‡å æ¯”ï¼š80%

# è¾“å‡ºç›®å½•ï¼ˆä¸Šçº§ç›®å½•çš„dataæ–‡ä»¶å¤¹ï¼‰
OUTPUT_DIR = "../data"

# ==================== å·¥å…·å‡½æ•° ====================
def print_header(title):
    """æ‰“å°æ ‡é¢˜"""
    print("\n" + "="*70)
    print(f"  {title}")
    print("="*70)

def print_progress(current, total, start_time, interval=100000):
    """æ‰“å°è¿›åº¦æ¡"""
    if (current + 1) % interval == 0:
        elapsed = time.time() - start_time
        progress = (current + 1) / total * 100
        speed = (current + 1) / elapsed
        eta = (total - current - 1) / speed
        print(f"è¿›åº¦: {progress:5.1f}% ({current+1:>8,}/{total:,}) | "
              f"è€—æ—¶: {elapsed:6.1f}s | é€Ÿåº¦: {speed:6.0f} è¡Œ/s | "
              f"é¢„è®¡å‰©ä½™: {eta:6.1f}s")

def print_summary(output_file, num_lines, words_per_line, num_unique, 
                 total_time, extra_info=None):
    """æ‰“å°ç”Ÿæˆæ‘˜è¦"""
    total_words = num_lines * words_per_line
    file_size = os.path.getsize(output_file) / (1024 * 1024)  # MB
    
    print("\n" + "-"*70)
    print("ã€æ–‡ä»¶ä¿¡æ¯ã€‘")
    print(f"  æ–‡ä»¶è·¯å¾„: {output_file}")
    print(f"  æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
    print(f"  æ€»è¡Œæ•°: {num_lines:,}")
    print(f"  æ€»å•è¯æ•°: {total_words:,}")
    print(f"  å”¯ä¸€å•è¯æ•°: {num_unique:,}")
    
    print(f"\nã€æ€§èƒ½æŒ‡æ ‡ã€‘")
    print(f"  ç”Ÿæˆè€—æ—¶: {total_time:.2f} ç§’")
    print(f"  å¤„ç†é€Ÿåº¦: {num_lines/total_time:.0f} è¡Œ/ç§’")
    
    print(f"\nã€Combineræ•ˆæœé¢„ä¼°ã€‘")
    print(f"  Mapè¾“å‡ºè®°å½•æ•°: {total_words:,}")
    print(f"  Combineråè®°å½•æ•°: ~{num_unique:,}")
    print(f"  æ•°æ®å‹ç¼©æ¯”: ~{total_words/num_unique:.1f}:1")
    
    if extra_info:
        print(f"\nã€ç‰¹æ®Šè¯´æ˜ã€‘")
        for line in extra_info:
            print(f"  {line}")
    
    print("="*70)

# ==================== 1. å‡åŒ€åˆ†å¸ƒæ•°æ® ====================
def generate_uniform_data():
    """ç”Ÿæˆå‡åŒ€åˆ†å¸ƒçš„æ•°æ®"""
    output_file = os.path.join(OUTPUT_DIR, "uniform_data.txt")
    
    print_header("ç”Ÿæˆå‡åŒ€åˆ†å¸ƒæ•°æ® - æ¨¡æ‹Ÿç»“æ„åŒ–æ—¥å¿—åœºæ™¯")
    print(f"é…ç½®: {NUM_LINES:,}è¡Œ Ã— {WORDS_PER_LINE}è¯/è¡Œ = {NUM_LINES*WORDS_PER_LINE:,}ä¸ªå•è¯")
    print(f"å”¯ä¸€å•è¯æ•°: {NUM_UNIQUE_WORDS:,}")
    print(f"ç†è®ºæ¯ä¸ªå•è¯å‡ºç°æ¬¡æ•°: {NUM_LINES*WORDS_PER_LINE/NUM_UNIQUE_WORDS:.0f}")
    print()
    
    start_time = time.time()
    
    # ç”Ÿæˆå•è¯æ± 
    print("ç”Ÿæˆå•è¯æ± ...")
    word_pool = [f"word_{i:05d}" for i in range(NUM_UNIQUE_WORDS)]
    
    # å†™å…¥æ•°æ®
    print(f"å†™å…¥æ•°æ®: {output_file}")
    print()
    
    with open(output_file, 'w') as f:
        for i in range(NUM_LINES):
            words = [random.choice(word_pool) for _ in range(WORDS_PER_LINE)]
            f.write(' '.join(words) + '\n')
            print_progress(i, NUM_LINES, start_time)
    
    total_time = time.time() - start_time
    
    extra_info = [
        "âœ“ æ‰€æœ‰å•è¯å‡ºç°é¢‘ç‡ç›¸è¿‘",
        "âœ“ Combineré¢„æœŸæ•ˆæœæ˜¾è‘—",
        "âœ“ é€‚åˆéªŒè¯Combinerçš„åŸºæœ¬åŠŸèƒ½"
    ]
    
    print_summary(output_file, NUM_LINES, WORDS_PER_LINE, 
                 NUM_UNIQUE_WORDS, total_time, extra_info)
    
    return output_file

# ==================== 2. æ•°æ®å€¾æ–œåˆ†å¸ƒ ====================
def generate_skewed_data():
    """ç”Ÿæˆæ•°æ®å€¾æ–œçš„æ•°æ®"""
    output_file = os.path.join(OUTPUT_DIR, "skewed_data.txt")
    
    print_header("ç”Ÿæˆæ•°æ®å€¾æ–œåˆ†å¸ƒæ•°æ® - æ¨¡æ‹Ÿçƒ­ç‚¹è¯æ±‡åœºæ™¯")
    print(f"é…ç½®: {NUM_LINES:,}è¡Œ Ã— {WORDS_PER_LINE}è¯/è¡Œ = {NUM_LINES*WORDS_PER_LINE:,}ä¸ªå•è¯")
    print(f"çƒ­ç‚¹å•è¯: {HOT_WORD_COUNT}ä¸ª (å {HOT_WORD_COUNT/(HOT_WORD_COUNT+COLD_WORD_COUNT)*100:.1f}%), é¢‘ç‡å æ¯”{HOT_RATIO*100:.0f}%")
    print(f"å†·é—¨å•è¯: {COLD_WORD_COUNT}ä¸ª (å {COLD_WORD_COUNT/(HOT_WORD_COUNT+COLD_WORD_COUNT)*100:.1f}%), é¢‘ç‡å æ¯”{(1-HOT_RATIO)*100:.0f}%")
    
    total_words = NUM_LINES * WORDS_PER_LINE
    hot_total = int(total_words * HOT_RATIO)
    cold_total = total_words - hot_total
    hot_avg = hot_total / HOT_WORD_COUNT
    cold_avg = cold_total / COLD_WORD_COUNT
    
    print(f"å€¾æ–œæ¯”ä¾‹: {hot_avg/cold_avg:.1f}:1 (çƒ­ç‚¹è¯å¹³å‡å‡ºç°{hot_avg:.0f}æ¬¡ vs å†·é—¨è¯{cold_avg:.0f}æ¬¡)")
    print()
    
    start_time = time.time()
    
    # ç”Ÿæˆå•è¯æ± 
    print("ç”Ÿæˆå•è¯æ± ...")
    hot_words = [f"hot_word_{i:03d}" for i in range(HOT_WORD_COUNT)]
    cold_words = [f"cold_word_{i:04d}" for i in range(COLD_WORD_COUNT)]
    
    # å†™å…¥æ•°æ®
    print(f"å†™å…¥æ•°æ®: {output_file}")
    print()
    
    with open(output_file, 'w') as f:
        for i in range(NUM_LINES):
            words = []
            for _ in range(WORDS_PER_LINE):
                if random.random() < HOT_RATIO:
                    words.append(random.choice(hot_words))
                else:
                    words.append(random.choice(cold_words))
            f.write(' '.join(words) + '\n')
            print_progress(i, NUM_LINES, start_time)
    
    total_time = time.time() - start_time
    
    extra_info = [
        f"âš ï¸  æ•°æ®å­˜åœ¨æ˜¾è‘—å€¾æ–œ (80-20åŸåˆ™)",
        f"âœ“ Combinerå¯¹çƒ­ç‚¹è¯æ•ˆæœæ˜¾è‘—",
        f"âš ï¸  å¯èƒ½å¯¼è‡´Reducerè´Ÿè½½ä¸å‡è¡¡",
        f"âœ“ é€‚åˆéªŒè¯Combineråœ¨å€¾æ–œåœºæ™¯çš„è¡¨ç°"
    ]
    
    print_summary(output_file, NUM_LINES, WORDS_PER_LINE,
                 HOT_WORD_COUNT + COLD_WORD_COUNT, total_time, extra_info)
    
    return output_file

# ==================== 3. é«˜å”¯ä¸€æ€§æ•°æ® ====================
def generate_unique_data():
    """ç”Ÿæˆé«˜å”¯ä¸€æ€§çš„æ•°æ®ï¼ˆå‡ ä¹æ— é‡å¤ï¼‰"""
    output_file = os.path.join(OUTPUT_DIR, "unique_data.txt")
    
    print_header("ç”Ÿæˆé«˜å”¯ä¸€æ€§æ•°æ® - æ¨¡æ‹Ÿå”¯ä¸€æ ‡è¯†ç¬¦åœºæ™¯")
    print(f"é…ç½®: {NUM_LINES:,}è¡Œ Ã— {WORDS_PER_LINE}è¯/è¡Œ = {NUM_LINES*WORDS_PER_LINE:,}ä¸ªå•è¯")
    print(f"ç›®æ ‡å”¯ä¸€æ€§: >95%")
    print(f"åœºæ™¯: UUIDã€ä¼šè¯IDã€å”¯ä¸€æ ‡è¯†ç¬¦ç­‰")
    print()
    
    start_time = time.time()
    
    # å†™å…¥æ•°æ®
    print(f"å†™å…¥æ•°æ®: {output_file}")
    print()
    
    word_counter = 0
    with open(output_file, 'w') as f:
        for i in range(NUM_LINES):
            words = []
            for _ in range(WORDS_PER_LINE):
                # æ ¼å¼: uuid_åºå· (ä¿è¯å”¯ä¸€æ€§)
                words.append(f"uuid_{word_counter:08d}")
                word_counter += 1
            f.write(' '.join(words) + '\n')
            print_progress(i, NUM_LINES, start_time)
    
    total_time = time.time() - start_time
    
    total_words = NUM_LINES * WORDS_PER_LINE
    
    extra_info = [
        f"âœ— å‡ ä¹æ¯ä¸ªå•è¯éƒ½æ˜¯å”¯ä¸€çš„ (100%å”¯ä¸€æ€§)",
        f"âœ— Combineræ— æ³•å‡å°‘æ•°æ®é‡",
        f"âœ— ä½¿ç”¨Combineråè€Œå¢åŠ è®¡ç®—å¼€é”€",
        f"âœ“ é€‚åˆéªŒè¯'ä¸æ˜¯æ‰€æœ‰åœºæ™¯éƒ½é€‚åˆCombiner'"
    ]
    
    print_summary(output_file, NUM_LINES, WORDS_PER_LINE,
                 total_words, total_time, extra_info)
    
    return output_file

# ==================== ä¸»å‡½æ•° ====================
def main():
    """ä¸»å‡½æ•°ï¼šç”Ÿæˆæ‰€æœ‰æµ‹è¯•æ•°æ®"""
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        print(f"åˆ›å»ºè¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    
    print("\n" + "â–ˆ"*70)
    print("  MapReduce Combiner æ€§èƒ½åˆ†æ - æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨")
    print("â–ˆ"*70)
    print(f"\nç»Ÿä¸€é…ç½®:")
    print(f"  - æ€»è¡Œæ•°: {NUM_LINES:,}")
    print(f"  - æ¯è¡Œå•è¯æ•°: {WORDS_PER_LINE}")
    print(f"  - æ€»å•è¯æ•°: {NUM_LINES * WORDS_PER_LINE:,}")
    print(f"  - è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"\nå°†ç”Ÿæˆä¸‰ç§æ•°æ®é›†:")
    print(f"  1. uniform_data.txt  - å‡åŒ€åˆ†å¸ƒ")
    print(f"  2. skewed_data.txt   - æ•°æ®å€¾æ–œ")
    print(f"  3. unique_data.txt   - é«˜å”¯ä¸€æ€§")
    print()
    
    input("æŒ‰ Enter é”®å¼€å§‹ç”Ÿæˆæ•°æ®...")
    
    total_start = time.time()
    
    # ç”Ÿæˆä¸‰ç§æ•°æ®
    files = []
    files.append(generate_uniform_data())
    files.append(generate_skewed_data())
    files.append(generate_unique_data())
    
    total_time = time.time() - total_start
    
    # æ€»ç»“
    print("\n" + "â–ˆ"*70)
    print("  æ‰€æœ‰æ•°æ®ç”Ÿæˆå®Œæˆ!")
    print("â–ˆ"*70)
    print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
    for i, f in enumerate(files, 1):
        size = os.path.getsize(f) / (1024 * 1024)
        print(f"  {i}. {os.path.basename(f):20s} ({size:.2f} MB)")
    
    print(f"\næ€»è€—æ—¶: {total_time:.2f} ç§’")
    print(f"\nä¸‹ä¸€æ­¥:")
    print(f"  1. è¿è¡Œ validate_data.py éªŒè¯æ•°æ®ç‰¹å¾")
    print(f"  2. ä½¿ç”¨ scp æˆ– WinSCP ä¸Šä¼ æ•°æ®åˆ°è™šæ‹Ÿæœº")
    print(f"     scp {OUTPUT_DIR}/*.txt root@192.168.204.132:/export/data/")
    print("â–ˆ"*70 + "\n")

if __name__ == "__main__":
    main()
```

### 2.2 **æ•°æ®éªŒè¯è„šæœ¬**

åˆ›å»º `New-Item validate_data.py -ItemType File`:

```python
"""
éªŒè¯ç”Ÿæˆçš„æ•°æ®ç‰¹å¾
ç»Ÿè®¡æ•°æ®åˆ†å¸ƒæƒ…å†µ
é€‚é…æ–°çš„æ•°æ®æ ¼å¼ï¼šæ¯è¡ŒåŒ…å«å¤šä¸ªå•è¯
"""

import os
from collections import Counter
import time

def analyze_data_distribution(file_path, sample_lines=100000):
    """
    åˆ†ææ•°æ®åˆ†å¸ƒç‰¹å¾
    
    å‚æ•°:
        file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        sample_lines: é‡‡æ ·è¡Œæ•°ï¼ˆç”¨äºå¤§æ–‡ä»¶ï¼‰
    """
    print(f"åˆ†ææ•°æ®æ–‡ä»¶: {file_path}")
    print(f"é‡‡æ ·è¡Œæ•°: {sample_lines:,}")
    print("-" * 60)
    
    if not os.path.exists(file_path):
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    start_time = time.time()
    
    # è¯»å–é‡‡æ ·æ•°æ®
    word_counts = Counter()
    total_lines = 0
    total_words = 0
    words_per_line_list = []
    
    print("æ­£åœ¨è¯»å–å’Œåˆ†ææ•°æ®...")
    with open(file_path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= sample_lines:
                break
            
            words = line.strip().split()
            words_per_line_list.append(len(words))
            
            for word in words:
                if word:
                    word_counts[word] += 1
                    total_words += 1
            
            total_lines += 1
            
            # è¿›åº¦æç¤º
            if (i + 1) % 10000 == 0:
                print(f"  å·²å¤„ç†: {i+1:,} è¡Œ", end='\r')
    
    print(f"  å·²å¤„ç†: {total_lines:,} è¡Œ âœ“")
    
    # ç»Ÿè®¡ä¿¡æ¯
    unique_words = len(word_counts)
    top_words = word_counts.most_common(20)
    bottom_words = word_counts.most_common()[-10:] if len(word_counts) > 10 else []
    
    # è®¡ç®—åˆ†å¸ƒæŒ‡æ ‡
    counts = list(word_counts.values())
    avg_count = sum(counts) / len(counts) if counts else 0
    max_count = max(counts) if counts else 0
    min_count = min(counts) if counts else 0
    
    # è®¡ç®—å‰20%å•è¯çš„é¢‘ç‡å æ¯”
    sorted_words = word_counts.most_common()
    top_20_percent = max(1, int(unique_words * 0.2))
    top_20_freq = sum(count for _, count in sorted_words[:top_20_percent])
    top_20_ratio = top_20_freq / total_words if total_words > 0 else 0
    
    # è®¡ç®—æ¯è¡Œå•è¯æ•°ç»Ÿè®¡
    avg_words_per_line = sum(words_per_line_list) / len(words_per_line_list) if words_per_line_list else 0
    min_words_per_line = min(words_per_line_list) if words_per_line_list else 0
    max_words_per_line = max(words_per_line_list) if words_per_line_list else 0
    
    elapsed = time.time() - start_time
    
    # è¾“å‡ºç»“æœ
    print()
    print("="*60)
    print("æ•°æ®ç»Ÿè®¡ç»“æœ")
    print("="*60)
    print(f"\nã€åŸºæœ¬ä¿¡æ¯ã€‘")
    print(f"  æ€»è¡Œæ•°: {total_lines:,}")
    print(f"  æ€»å•è¯æ•°: {total_words:,}")
    print(f"  å”¯ä¸€å•è¯æ•°: {unique_words:,}")
    print(f"  å”¯ä¸€æ€§æ¯”ä¾‹: {unique_words/total_words*100:.2f}%")
    
    print(f"\nã€æ¯è¡Œå•è¯æ•°ã€‘")
    print(f"  å¹³å‡: {avg_words_per_line:.1f}")
    print(f"  æœ€å°: {min_words_per_line}")
    print(f"  æœ€å¤§: {max_words_per_line}")
    
    print(f"\nã€å•è¯é¢‘ç‡åˆ†å¸ƒã€‘")
    print(f"  å¹³å‡å‡ºç°æ¬¡æ•°: {avg_count:.2f}")
    print(f"  æœ€é«˜å‡ºç°æ¬¡æ•°: {max_count:,}")
    print(f"  æœ€ä½å‡ºç°æ¬¡æ•°: {min_count:,}")
    print(f"  å€¾æ–œåº¦ (æœ€é«˜/å¹³å‡): {max_count/avg_count:.2f}" if avg_count > 0 else "  å€¾æ–œåº¦: N/A")
    
    print(f"\nã€æ•°æ®å€¾æ–œåˆ†æã€‘")
    print(f"  å‰20%å•è¯çš„é¢‘ç‡å æ¯”: {top_20_ratio*100:.1f}%")
    if top_20_ratio > 0.6:
        print(f"  âš ï¸  æ•°æ®å­˜åœ¨æ˜¾è‘—å€¾æ–œï¼ˆé«˜äº60%ï¼‰")
    elif top_20_ratio > 0.4:
        print(f"  â„¹ï¸  æ•°æ®å­˜åœ¨ä¸­ç­‰å€¾æ–œï¼ˆ40-60%ï¼‰")
    else:
        print(f"  âœ“  æ•°æ®åˆ†å¸ƒè¾ƒä¸ºå‡åŒ€ï¼ˆä½äº40%ï¼‰")
    
    print(f"\nã€å‡ºç°é¢‘ç‡æœ€é«˜çš„20ä¸ªå•è¯ã€‘")
    for idx, (word, count) in enumerate(top_words, 1):
        percentage = count / total_words * 100
        bar_length = int(percentage * 2)
        bar = 'â–ˆ' * bar_length
        print(f"  {idx:2d}. {word:20s}: {count:>8,} æ¬¡ ({percentage:5.2f}%) {bar}")
    
    if bottom_words:
        print(f"\nã€å‡ºç°é¢‘ç‡æœ€ä½çš„10ä¸ªå•è¯ã€‘")
        for word, count in bottom_words:
            percentage = count / total_words * 100
            print(f"  {word:20s}: {count:>8,} æ¬¡ ({percentage:5.2f}%)")
    
    print(f"\nã€Combineræ•ˆæœé¢„ä¼°ã€‘")
    compression_ratio = total_words / unique_words if unique_words > 0 else 1
    print(f"  Mapè¾“å‡ºè®°å½•æ•°: {total_words:,}")
    print(f"  Combineråè®°å½•æ•°: ~{unique_words:,}")
    print(f"  æ•°æ®å‹ç¼©æ¯”: {compression_ratio:.2f}:1")
    
    if compression_ratio > 10:
        print(f"  âœ“  Combineré¢„æœŸæ•ˆæœæä½³ï¼ˆå‹ç¼©æ¯”>{compression_ratio:.0f}:1ï¼‰")
    elif compression_ratio > 3:
        print(f"  âœ“  Combineré¢„æœŸæ•ˆæœè‰¯å¥½ï¼ˆå‹ç¼©æ¯”={compression_ratio:.1f}:1ï¼‰")
    elif compression_ratio > 1.5:
        print(f"  âš ï¸  Combineræ•ˆæœä¸€èˆ¬ï¼ˆå‹ç¼©æ¯”={compression_ratio:.1f}:1ï¼‰")
    else:
        print(f"  âœ—  Combinerå‡ ä¹æ— æ•ˆï¼ˆå‹ç¼©æ¯”={compression_ratio:.1f}:1ï¼‰")
    
    print(f"\nã€æ€§èƒ½æŒ‡æ ‡ã€‘")
    print(f"  åˆ†æè€—æ—¶: {elapsed:.2f} ç§’")
    print(f"  å¤„ç†é€Ÿåº¦: {total_lines/elapsed:.0f} è¡Œ/ç§’")
    print("="*60)

if __name__ == "__main__":
    data_files = [
        "uniform_data.txt",
        "skewed_data.txt",
        "unique_data.txt"
    ]
    
    for data_file in data_files:
        print("\n" * 2)
        print("â–ˆ" * 60)
        print(f"  åˆ†ææ–‡ä»¶: {data_file}")
        print("â–ˆ" * 60)
        
        if os.path.exists(data_file):
            analyze_data_distribution(data_file)
        else:
            print(f"\næ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
            print("è¯·å…ˆè¿è¡Œå¯¹åº”çš„ç”Ÿæˆè„šæœ¬")
        
        print("\n")
```

------

## 3. åœ¨Windowsä¸Šæ‰§è¡Œæ•°æ®ç”Ÿæˆ

**æ£€æŸ¥Pythonç¯å¢ƒ**

åœ¨PowerShellæˆ–CMDä¸­æ‰§è¡Œï¼š

```powershell
python --version
```

å¦‚æœæ²¡æœ‰å®‰è£…Pythonï¼Œè¯·ä» [Pythonå®˜ç½‘](https://www.python.org/downloads/) ä¸‹è½½å®‰è£…ã€‚

**è¿è¡Œç”Ÿæˆè„šæœ¬**

```powershell
cd /code/data-generator

# ç”Ÿæˆå‡åŒ€åˆ†å¸ƒæ•°æ®
python generate_all_data.py

# éªŒè¯æ•°æ®åˆ†å¸ƒ
python validate_data.py
```

------

## 4. ä¸Šä¼ æ•°æ®åˆ°è™šæ‹Ÿæœº

**ä½¿ç”¨rzæŒ‡ä»¤**

```bash
# åœ¨è™šæ‹Ÿæœºä¸Šæ‰§è¡Œ
cd /export/data

scp -r code root@192.168.204.132:/export/data/
# ä¸Šä¼ æ–‡ä»¶ï¼ˆåœ¨SecureCRTæˆ–Xshellä¸­æ‰§è¡Œrzä¼šå¼¹å‡ºæ–‡ä»¶é€‰æ‹©çª—å£ï¼‰
rz
# é€‰æ‹©æ•´ä¸ªæ–‡ä»¶å¤¹/code
```

# Step 3ï¼šä»£ç ç¼–å†™

## 1. åˆ›å»ºJavaä»£ç ç›®å½•å¹¶ç¼–å†™ä»£ç 

### ä¸å¸¦Combinerç‰ˆæœ¬

#### 1. WordCountMapper.java

```java
package com.hadoop.wordcount.without;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import java.io.IOException;

public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    @Override
    protected void map(LongWritable key, Text value, Context context)
            throws IOException, InterruptedException {

        String line = value.toString();
        String[] words = line.split("\\s+");

        for (String w : words) {
            if (w.length() > 0) {
                word.set(w);
                context.write(word, one);
            }
        }
    }
}
```

#### 2. WordCountReducer.java

```java
package com.hadoop.wordcount.without;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
import java.io.IOException;

public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

    private IntWritable result = new IntWritable();

    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context)
            throws IOException, InterruptedException {

        int sum = 0;
        for (IntWritable val : values) {
            sum += val.get();
        }

        result.set(sum);
        context.write(key, result);
    }
}
```

#### 3. WordCountDriver.java

```java
package com.hadoop.wordcount.without;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCountDriver {

    public static void main(String[] args) throws Exception {

        if (args.length != 2) {
            System.err.println("Usage: WordCountDriver <input path> <output path>");
            System.exit(-1);
        }

        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "WordCount Without Combiner");

        job.setJarByClass(WordCountDriver.class);
        job.setMapperClass(WordCountMapper.class);
        job.setReducerClass(WordCountReducer.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        System.out.println("========================================");
        System.out.println("WordCount Without Combiner");
        System.out.println("========================================");
        System.out.println("Input:  " + args[0]);
        System.out.println("Output: " + args[1]);
        System.out.println("Combiner: DISABLED");
        System.out.println("========================================");

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
```

### å¸¦Combinerç‰ˆæœ¬

#### 1. WordCountMapper.java

```java
package com.hadoop.wordcount.with;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import java.io.IOException;

public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    @Override
    protected void map(LongWritable key, Text value, Context context)
            throws IOException, InterruptedException {

        String line = value.toString();
        String[] words = line.split("\\s+");

        for (String w : words) {
            if (w.length() > 0) {
                word.set(w);
                context.write(word, one);
            }
        }
    }
}
```

#### 2. WordCountReducer.java

```java
package com.hadoop.wordcount.with;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
import java.io.IOException;

public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

    private IntWritable result = new IntWritable();

    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context)
            throws IOException, InterruptedException {

        int sum = 0;
        for (IntWritable val : values) {
            sum += val.get();
        }

        result.set(sum);
        context.write(key, result);
    }
}
```

#### 3. WordCountDriver.java

```java
package com.hadoop.wordcount.with;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCountDriver {

    public static void main(String[] args) throws Exception {

        if (args.length != 2) {
            System.err.println("Usage: WordCountDriver <input path> <output path>");
            System.exit(-1);
        }

        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "WordCount With Combiner");

        job.setJarByClass(WordCountDriver.class);
        job.setMapperClass(WordCountMapper.class);
        job.setCombinerClass(WordCountReducer.class);  // å…³é”®ï¼šè®¾ç½®Combiner
        job.setReducerClass(WordCountReducer.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        System.out.println("========================================");
        System.out.println("WordCount With Combiner");
        System.out.println("========================================");
        System.out.println("Input:  " + args[0]);
        System.out.println("Output: " + args[1]);
        System.out.println("Combiner: ENABLED");
        System.out.println("========================================");

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
```

## 2. æ·»åŠ è„šæœ¬

#### 1. upload_data.sh

```bash
#!/bin/bash
# ============================================================================
# è„šæœ¬åç§°: compile_and_package.sh
# åŠŸèƒ½æè¿°: ç¼–è¯‘MapReduceæºä»£ç å¹¶æ‰“åŒ…æˆJARæ–‡ä»¶
# æ³¨æ„äº‹é¡¹: éœ€è¦å…ˆé…ç½®å¥½Hadoopç¯å¢ƒå˜é‡
# ============================================================================

# ----------------------------------------------------------------------------
# 1. å…¨å±€é…ç½®
# ----------------------------------------------------------------------------
PROJECT_DIR=/export/data/code
BUILD_DIR=$PROJECT_DIR/build

# å®šä¹‰æºä»£ç è·¯å¾„
WITHOUT_COMBINER_SRC=$PROJECT_DIR/mapreduce/without-combiner
WITH_COMBINER_SRC=$PROJECT_DIR/mapreduce/with-combiner

# å®šä¹‰ç¼–è¯‘è¾“å‡ºè·¯å¾„
WITHOUT_COMBINER_BUILD=$BUILD_DIR/without-combiner
WITH_COMBINER_BUILD=$BUILD_DIR/with-combiner

echo "=========================================="
echo "  ç¼–è¯‘å’Œæ‰“åŒ…MapReduceä»£ç "
echo "=========================================="
echo "é¡¹ç›®ç›®å½•: $PROJECT_DIR"
echo "ç¼–è¯‘ç›®å½•: $BUILD_DIR"
echo ""

# ----------------------------------------------------------------------------
# 2. æ¸…ç†æ—§çš„ç¼–è¯‘æ–‡ä»¶
# ----------------------------------------------------------------------------
echo "æ­¥éª¤1: æ¸…ç†æ—§çš„ç¼–è¯‘æ–‡ä»¶..."

# åˆ é™¤æ—§çš„buildç›®å½•
if [ -d "$BUILD_DIR" ]; then
    echo "  åˆ é™¤æ—§çš„ $BUILD_DIR"
    rm -rf $BUILD_DIR/*
fi

# é‡æ–°åˆ›å»ºç›®å½•ç»“æ„
mkdir -p $WITHOUT_COMBINER_BUILD
mkdir -p $WITH_COMBINER_BUILD

echo "âœ“ æ¸…ç†å®Œæˆ"

# ----------------------------------------------------------------------------
# 3. ç¼–è¯‘ä¸å¸¦Combinerçš„ç‰ˆæœ¬
# ----------------------------------------------------------------------------
echo ""
echo "æ­¥éª¤2: ç¼–è¯‘ without-combiner ç‰ˆæœ¬..."
echo "  æºä»£ç ç›®å½•: $WITHOUT_COMBINER_SRC"

# è¿›å…¥æºä»£ç ç›®å½•
cd $WITHOUT_COMBINER_SRC

# æ£€æŸ¥æºä»£ç æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if [ ! -d "./" ]; then
    echo "âœ— é”™è¯¯: æ‰¾ä¸åˆ°æºä»£ç ç›®å½•!"
    echo "è¯·æ£€æŸ¥: $WITHOUT_COMBINER_SRC"
    exit 1
fi

echo "  æ‰¾åˆ°æºæ–‡ä»¶:"
ls -la ./*.java

# javac ç¼–è¯‘å‚æ•°è¯´æ˜ï¼š
# -classpath $(hadoop classpath): æ·»åŠ Hadoopçš„æ‰€æœ‰ä¾èµ–åº“
# -d $WITHOUT_COMBINER_BUILD: æŒ‡å®šç¼–è¯‘åçš„classæ–‡ä»¶è¾“å‡ºç›®å½•
# .out/*.java: ç¼–è¯‘æ‰€æœ‰Javaæºæ–‡ä»¶
echo ""
echo "  å¼€å§‹ç¼–è¯‘..."
javac -classpath $(hadoop classpath) \
      -d $WITHOUT_COMBINER_BUILD \
      ./*.java

# æ£€æŸ¥ç¼–è¯‘æ˜¯å¦æˆåŠŸ
if [ $? -eq 0 ]; then
    echo "  âœ“ ç¼–è¯‘æˆåŠŸ"
    echo "  ç¼–è¯‘åçš„classæ–‡ä»¶:"
    find $WITHOUT_COMBINER_BUILD -name "*.class"
else
    echo "  âœ— ç¼–è¯‘å¤±è´¥ï¼"
    echo "  è¯·æ£€æŸ¥:"
    echo "  1. Javaç‰ˆæœ¬æ˜¯å¦æ­£ç¡® (java -version)"
    echo "  2. Hadoopç¯å¢ƒå˜é‡æ˜¯å¦é…ç½® (hadoop version)"
    echo "  3. æºä»£ç æ˜¯å¦æœ‰è¯­æ³•é”™è¯¯"
    exit 1
fi

# æ‰“åŒ…æˆJARæ–‡ä»¶
echo ""
echo "  æ‰“åŒ… JAR æ–‡ä»¶..."
cd $WITHOUT_COMBINER_BUILD

# jar å‘½ä»¤å‚æ•°è¯´æ˜ï¼š
# -c: åˆ›å»ºæ–°çš„JARæ–‡ä»¶
# -v: æ˜¾ç¤ºè¯¦ç»†è¾“å‡ºï¼ˆverboseï¼‰
# -f: æŒ‡å®šJARæ–‡ä»¶å
# ./*.class: è¦æ‰“åŒ…çš„classæ–‡ä»¶
jar -cvf wordcount-without-combiner.jar \
    ./*.class > /dev/null 2>&1

if [ -f "wordcount-without-combiner.jar" ]; then
    echo "  âœ“ ç”Ÿæˆ: wordcount-without-combiner.jar"
    ls -lh wordcount-without-combiner.jar
else
    echo "  âœ— JARæ‰“åŒ…å¤±è´¥"
    exit 1
fi

# ----------------------------------------------------------------------------
# 4. ç¼–è¯‘å¸¦Combinerçš„ç‰ˆæœ¬
# ----------------------------------------------------------------------------
echo ""
echo "æ­¥éª¤3: ç¼–è¯‘ with-combiner ç‰ˆæœ¬..."
echo "  æºä»£ç ç›®å½•: $WITH_COMBINER_SRC"

cd $WITH_COMBINER_SRC

# æ£€æŸ¥æºä»£ç æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if [ ! -d "." ]; then
    echo "âœ— é”™è¯¯: æ‰¾ä¸åˆ°æºä»£ç ç›®å½•!"
    echo "è¯·æ£€æŸ¥: $WITH_COMBINER_SRC/./"
    exit 1
fi

echo "  æ‰¾åˆ°æºæ–‡ä»¶:"
ls -la ./*.java

echo ""
echo "  å¼€å§‹ç¼–è¯‘..."
javac -classpath $(hadoop classpath) \
      -d $WITH_COMBINER_BUILD \
      ./*.java

if [ $? -eq 0 ]; then
    echo "  âœ“ ç¼–è¯‘æˆåŠŸ"
    echo "  ç¼–è¯‘åçš„classæ–‡ä»¶:"
    find $WITH_COMBINER_BUILD -name "*.class"
else
    echo "  âœ— ç¼–è¯‘å¤±è´¥ï¼"
    exit 1
fi

# æ‰“åŒ…æˆJARæ–‡ä»¶
echo ""
echo "  æ‰“åŒ… JAR æ–‡ä»¶..."
cd $WITH_COMBINER_BUILD

jar -cvf wordcount-with-combiner.jar \
    ./*.class > /dev/null 2>&1

if [ -f "wordcount-with-combiner.jar" ]; then
    echo "  âœ“ ç”Ÿæˆ: wordcount-with-combiner.jar"
    ls -lh wordcount-with-combiner.jar
else
    echo "  âœ— JARæ‰“åŒ…å¤±è´¥"
    exit 1
fi

# ----------------------------------------------------------------------------
# 5. éªŒè¯JARæ–‡ä»¶å†…å®¹
# ----------------------------------------------------------------------------
echo ""
echo "æ­¥éª¤4: éªŒè¯JARæ–‡ä»¶å†…å®¹..."

echo ""
echo "--- without-combiner JAR å†…å®¹ ---"
jar -tf $WITHOUT_COMBINER_BUILD/wordcount-without-combiner.jar | grep -E "\.class$"

echo ""
echo "--- with-combiner JAR å†…å®¹ ---"
jar -tf $WITH_COMBINER_BUILD/wordcount-with-combiner.jar | grep -E "\.class$"

# ----------------------------------------------------------------------------
# 6. å®Œæˆæç¤º
# ----------------------------------------------------------------------------
echo ""
echo "=========================================="
echo "  âœ“ ç¼–è¯‘æ‰“åŒ…å®Œæˆ!"
echo "=========================================="
echo ""
echo "ç”Ÿæˆçš„JARæ–‡ä»¶:"
echo "1. Without Combiner:"
ls -lh $WITHOUT_COMBINER_BUILD/*.jar
echo ""
echo "2. With Combiner:"
ls -lh $WITH_COMBINER_BUILD/*.jar

echo ""
echo "ä¸‹ä¸€æ­¥æ“ä½œ:"
echo "  è¿è¡Œå®éªŒ: ./run_experiments.sh"
echo "=========================================="
```

#### 2. compile_and_package.sh

```bash
#!/bin/bash
# ============================================================================
# è„šæœ¬åç§°: compile_and_package.sh
# åŠŸèƒ½æè¿°: ç¼–è¯‘MapReduceæºä»£ç å¹¶æ‰“åŒ…æˆJARæ–‡ä»¶
# æ³¨æ„äº‹é¡¹: éœ€è¦å…ˆé…ç½®å¥½Hadoopç¯å¢ƒå˜é‡
# ============================================================================

# ----------------------------------------------------------------------------
# 1. å…¨å±€é…ç½®
# ----------------------------------------------------------------------------
PROJECT_DIR=/export/data/code
BUILD_DIR=$PROJECT_DIR/build

# å®šä¹‰æºä»£ç è·¯å¾„
WITHOUT_COMBINER_SRC=$PROJECT_DIR/mapreduce/without-combiner
WITH_COMBINER_SRC=$PROJECT_DIR/mapreduce/with-combiner

# å®šä¹‰ç¼–è¯‘è¾“å‡ºè·¯å¾„
WITHOUT_COMBINER_BUILD=$BUILD_DIR/without-combiner
WITH_COMBINER_BUILD=$BUILD_DIR/with-combiner

echo "=========================================="
echo "  ç¼–è¯‘å’Œæ‰“åŒ…MapReduceä»£ç "
echo "=========================================="
echo "é¡¹ç›®ç›®å½•: $PROJECT_DIR"
echo "ç¼–è¯‘ç›®å½•: $BUILD_DIR"
echo ""

# ----------------------------------------------------------------------------
# 2. æ¸…ç†æ—§çš„ç¼–è¯‘æ–‡ä»¶
# ----------------------------------------------------------------------------
echo "æ­¥éª¤1: æ¸…ç†æ—§çš„ç¼–è¯‘æ–‡ä»¶..."

# åˆ é™¤æ—§çš„buildç›®å½•
if [ -d "$BUILD_DIR" ]; then
    echo "  åˆ é™¤æ—§çš„ $BUILD_DIR"
    rm -rf $BUILD_DIR/*
fi

# é‡æ–°åˆ›å»ºç›®å½•ç»“æ„
mkdir -p $WITHOUT_COMBINER_BUILD
mkdir -p $WITH_COMBINER_BUILD

echo "âœ“ æ¸…ç†å®Œæˆ"

# ----------------------------------------------------------------------------
# 3. ç¼–è¯‘ä¸å¸¦Combinerçš„ç‰ˆæœ¬
# ----------------------------------------------------------------------------
echo ""
echo "æ­¥éª¤2: ç¼–è¯‘ without-combiner ç‰ˆæœ¬..."
echo "  æºä»£ç ç›®å½•: $WITHOUT_COMBINER_SRC"

# è¿›å…¥æºä»£ç ç›®å½•
cd $WITHOUT_COMBINER_SRC

# æ£€æŸ¥æºä»£ç æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if [ ! -d "com/hadoop/wordcount/without" ]; then
    echo "âœ— é”™è¯¯: æ‰¾ä¸åˆ°æºä»£ç ç›®å½•!"
    echo "è¯·æ£€æŸ¥: $WITHOUT_COMBINER_SRC/com/hadoop/wordcount/without/"
    exit 1
fi

echo "  æ‰¾åˆ°æºæ–‡ä»¶:"
ls -la com/hadoop/wordcount/without/*.java

# javac ç¼–è¯‘å‚æ•°è¯´æ˜ï¼š
# -classpath $(hadoop classpath): æ·»åŠ Hadoopçš„æ‰€æœ‰ä¾èµ–åº“
# -d $WITHOUT_COMBINER_BUILD: æŒ‡å®šç¼–è¯‘åçš„classæ–‡ä»¶è¾“å‡ºç›®å½•
# com/hadoop/wordcount/without/*.java: ç¼–è¯‘æ‰€æœ‰Javaæºæ–‡ä»¶
echo ""
echo "  å¼€å§‹ç¼–è¯‘..."
javac -classpath $(hadoop classpath) \
      -d $WITHOUT_COMBINER_BUILD \
      com/hadoop/wordcount/without/*.java

# æ£€æŸ¥ç¼–è¯‘æ˜¯å¦æˆåŠŸ
if [ $? -eq 0 ]; then
    echo "  âœ“ ç¼–è¯‘æˆåŠŸ"
    echo "  ç¼–è¯‘åçš„classæ–‡ä»¶:"
    find $WITHOUT_COMBINER_BUILD -name "*.class"
else
    echo "  âœ— ç¼–è¯‘å¤±è´¥ï¼"
    echo "  è¯·æ£€æŸ¥:"
    echo "  1. Javaç‰ˆæœ¬æ˜¯å¦æ­£ç¡® (java -version)"
    echo "  2. Hadoopç¯å¢ƒå˜é‡æ˜¯å¦é…ç½® (hadoop version)"
    echo "  3. æºä»£ç æ˜¯å¦æœ‰è¯­æ³•é”™è¯¯"
    exit 1
fi

# æ‰“åŒ…æˆJARæ–‡ä»¶
echo ""
echo "  æ‰“åŒ… JAR æ–‡ä»¶..."
cd $WITHOUT_COMBINER_BUILD

# jar å‘½ä»¤å‚æ•°è¯´æ˜ï¼š
# -c: åˆ›å»ºæ–°çš„JARæ–‡ä»¶
# -v: æ˜¾ç¤ºè¯¦ç»†è¾“å‡ºï¼ˆverboseï¼‰
# -f: æŒ‡å®šJARæ–‡ä»¶å
# com/hadoop/wordcount/without/*.class: è¦æ‰“åŒ…çš„classæ–‡ä»¶
jar -cvf wordcount-without-combiner.jar \
    com/hadoop/wordcount/without/*.class > /dev/null 2>&1

if [ -f "wordcount-without-combiner.jar" ]; then
    echo "  âœ“ ç”Ÿæˆ: wordcount-without-combiner.jar"
    ls -lh wordcount-without-combiner.jar
else
    echo "  âœ— JARæ‰“åŒ…å¤±è´¥"
    exit 1
fi

# ----------------------------------------------------------------------------
# 4. ç¼–è¯‘å¸¦Combinerçš„ç‰ˆæœ¬
# ----------------------------------------------------------------------------
echo ""
echo "æ­¥éª¤3: ç¼–è¯‘ with-combiner ç‰ˆæœ¬..."
echo "  æºä»£ç ç›®å½•: $WITH_COMBINER_SRC"

cd $WITH_COMBINER_SRC

# æ£€æŸ¥æºä»£ç æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if [ ! -d "com/hadoop/wordcount/with" ]; then
    echo "âœ— é”™è¯¯: æ‰¾ä¸åˆ°æºä»£ç ç›®å½•!"
    echo "è¯·æ£€æŸ¥: $WITH_COMBINER_SRC/com/hadoop/wordcount/with/"
    exit 1
fi

echo "  æ‰¾åˆ°æºæ–‡ä»¶:"
ls -la com/hadoop/wordcount/with/*.java

echo ""
echo "  å¼€å§‹ç¼–è¯‘..."
javac -classpath $(hadoop classpath) \
      -d $WITH_COMBINER_BUILD \
      com/hadoop/wordcount/with/*.java

if [ $? -eq 0 ]; then
    echo "  âœ“ ç¼–è¯‘æˆåŠŸ"
    echo "  ç¼–è¯‘åçš„classæ–‡ä»¶:"
    find $WITH_COMBINER_BUILD -name "*.class"
else
    echo "  âœ— ç¼–è¯‘å¤±è´¥ï¼"
    exit 1
fi

# æ‰“åŒ…æˆJARæ–‡ä»¶
echo ""
echo "  æ‰“åŒ… JAR æ–‡ä»¶..."
cd $WITH_COMBINER_BUILD

jar -cvf wordcount-with-combiner.jar \
    com/hadoop/wordcount/with/*.class > /dev/null 2>&1

if [ -f "wordcount-with-combiner.jar" ]; then
    echo "  âœ“ ç”Ÿæˆ: wordcount-with-combiner.jar"
    ls -lh wordcount-with-combiner.jar
else
    echo "  âœ— JARæ‰“åŒ…å¤±è´¥"
    exit 1
fi

# ----------------------------------------------------------------------------
# 5. éªŒè¯JARæ–‡ä»¶å†…å®¹
# ----------------------------------------------------------------------------
echo ""
echo "æ­¥éª¤4: éªŒè¯JARæ–‡ä»¶å†…å®¹..."

echo ""
echo "--- without-combiner JAR å†…å®¹ ---"
jar -tf $WITHOUT_COMBINER_BUILD/wordcount-without-combiner.jar | grep -E "\.class$"

echo ""
echo "--- with-combiner JAR å†…å®¹ ---"
jar -tf $WITH_COMBINER_BUILD/wordcount-with-combiner.jar | grep -E "\.class$"

# ----------------------------------------------------------------------------
# 6. å®Œæˆæç¤º
# ----------------------------------------------------------------------------
echo ""
echo "=========================================="
echo "  âœ“ ç¼–è¯‘æ‰“åŒ…å®Œæˆ!"
echo "=========================================="
echo ""
echo "ç”Ÿæˆçš„JARæ–‡ä»¶:"
echo "1. Without Combiner:"
ls -lh $WITHOUT_COMBINER_BUILD/*.jar
echo ""
echo "2. With Combiner:"
ls -lh $WITH_COMBINER_BUILD/*.jar

echo ""
echo "ä¸‹ä¸€æ­¥æ“ä½œ:"
echo "  è¿è¡Œå®éªŒ: ./run_experiments.sh"
echo "=========================================="
```

#### 3. run_experiments.sh

```bash
#!/bin/bash
# ============================================================================
# è„šæœ¬åç§°: run_experiments.sh
# åŠŸèƒ½æè¿°: è¿è¡Œæ‰€æœ‰MapReduceå®éªŒå¹¶è®°å½•æ€§èƒ½æ•°æ®
# å®éªŒè®¾è®¡:
#   - 3ç§æ•°æ®é›† (uniform, skewed, unique)
#   - 2ç§é…ç½® (with/without Combiner)
#   - å…±6ä¸ªå®éªŒ
# ============================================================================

# ----------------------------------------------------------------------------
# 1. å…¨å±€é…ç½® - æ ¹æ®å½“å‰ç”¨æˆ·è°ƒæ•´
# ----------------------------------------------------------------------------
PROJECT_DIR=/export/data/code
BUILD_DIR=$PROJECT_DIR/build
RESULTS_DIR=$PROJECT_DIR/results

# å¦‚æœæ˜¯ root ç”¨æˆ·:
HDFS_INPUT_DIR=/user/root/input
HDFS_OUTPUT_DIR=/user/root/output

# å¦‚æœæ˜¯ hadoop ç”¨æˆ·ï¼ˆæ³¨é‡Šæ‰ä¸Šé¢çš„ï¼Œå–æ¶ˆæ³¨é‡Šä¸‹é¢çš„ï¼‰:
# HDFS_INPUT_DIR=/user/hadoop/input
# HDFS_OUTPUT_DIR=/user/hadoop/output

echo "=========================================="
echo "  MapReduce Combiner æ€§èƒ½å¯¹æ¯”å®éªŒ"
echo "=========================================="
echo "å½“å‰ç”¨æˆ·: $(whoami)"
echo "é¡¹ç›®ç›®å½•: $PROJECT_DIR"
echo "HDFSè¾“å…¥: $HDFS_INPUT_DIR"
echo "HDFSè¾“å‡º: $HDFS_OUTPUT_DIR"
echo "ç»“æœç›®å½•: $RESULTS_DIR"
echo ""

# ----------------------------------------------------------------------------
# 2. åˆ›å»ºç»“æœç›®å½•
# ----------------------------------------------------------------------------
echo "å‡†å¤‡å®éªŒç¯å¢ƒ..."

# åˆ›å»ºæœ¬åœ°ç»“æœç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
if [ ! -d "$RESULTS_DIR" ]; then
    mkdir -p $RESULTS_DIR
    echo "âœ“ åˆ›å»ºç»“æœç›®å½•: $RESULTS_DIR"
fi

# ----------------------------------------------------------------------------
# 3. æ¸…ç†æ—§çš„HDFSè¾“å‡º
# ----------------------------------------------------------------------------
echo "æ¸…ç†æ—§çš„HDFSè¾“å‡ºç›®å½•..."

# åˆ é™¤æ‰€æœ‰æ—§çš„è¾“å‡ºç›®å½•
# -r: é€’å½’åˆ é™¤
# 2>/dev/null: å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œä¸æ˜¾ç¤ºé”™è¯¯
hdfs dfs -rm -r $HDFS_OUTPUT_DIR/* 2>/dev/null

if [ $? -eq 0 ]; then
    echo "âœ“ æ¸…ç†å®Œæˆ"
else
    echo "âš ï¸  è­¦å‘Š: æ¸…ç†å¯èƒ½å¤±è´¥ï¼ˆå¦‚æœæ˜¯é¦–æ¬¡è¿è¡Œå¯ä»¥å¿½ç•¥ï¼‰"
fi

# ----------------------------------------------------------------------------
# 4. éªŒè¯JARæ–‡ä»¶å­˜åœ¨
# ----------------------------------------------------------------------------
echo ""
echo "æ£€æŸ¥JARæ–‡ä»¶..."

WITHOUT_JAR=$BUILD_DIR/without-combiner/wordcount-without-combiner.jar
WITH_JAR=$BUILD_DIR/with-combiner/wordcount-with-combiner.jar

if [ ! -f "$WITHOUT_JAR" ]; then
    echo "âœ— é”™è¯¯: æ‰¾ä¸åˆ° $WITHOUT_JAR"
    echo "è¯·å…ˆè¿è¡Œ: ./compile_and_package.sh"
    exit 1
fi

if [ ! -f "$WITH_JAR" ]; then
    echo "âœ— é”™è¯¯: æ‰¾ä¸åˆ° $WITH_JAR"
    echo "è¯·å…ˆè¿è¡Œ: ./compile_and_package.sh"
    exit 1
fi

echo "âœ“ JARæ–‡ä»¶æ£€æŸ¥é€šè¿‡"

# ----------------------------------------------------------------------------
# 5. å®šä¹‰å®éªŒé…ç½®
# ----------------------------------------------------------------------------

# æ•°æ®é›†æ–‡ä»¶åæ•°ç»„
declare -a datasets=("uniform_data.txt" "skewed_data.txt" "unique_data.txt")

# æ•°æ®é›†ç®€ç§°æ•°ç»„ï¼ˆç”¨äºå‘½åè¾“å‡ºç›®å½•å’Œæ—¥å¿—æ–‡ä»¶ï¼‰
declare -a dataset_names=("uniform" "skewed" "unique")

# æ•°æ®é›†æè¿°ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
declare -a dataset_descriptions=(
    "å‡åŒ€åˆ†å¸ƒæ•°æ® - è¯é¢‘ç›¸è¿‘"
    "æ•°æ®å€¾æ–œ - çƒ­ç‚¹è¯å ä¸»å¯¼"
    "é«˜å”¯ä¸€æ€§ - å‡ ä¹æ— é‡å¤"
)

# ----------------------------------------------------------------------------
# 6. è¿è¡Œå®éªŒå¾ªç¯
# ----------------------------------------------------------------------------
echo ""
echo "=========================================="
echo "  å¼€å§‹å®éªŒ (å…± 6 ä¸ª)"
echo "=========================================="

# å®éªŒè®¡æ•°å™¨
experiment_num=0
total_experiments=6

# éå†æ¯ä¸ªæ•°æ®é›†
for i in "${!datasets[@]}"; do
    # è·å–å½“å‰æ•°æ®é›†çš„å„ç§ä¿¡æ¯
    dataset="${datasets[$i]}"              # æ–‡ä»¶å: uniform_data.txt
    name="${dataset_names[$i]}"            # ç®€ç§°: uniform
    description="${dataset_descriptions[$i]}"  # æè¿°

    echo ""
    echo "=========================================="
    echo "  æ•°æ®é›† $((i+1))/3: $dataset"
    echo "  è¯´æ˜: $description"
    echo "=========================================="

    # ------------------------------------------------------------------------
    # å®éªŒA: ä¸å¸¦Combiner
    # ------------------------------------------------------------------------
    experiment_num=$((experiment_num + 1))
    echo ""
    echo ">>> å®éªŒ $experiment_num/$total_experiments: ${name}_without_combiner"
    echo "å¼€å§‹æ—¶é—´: $(date '+%Y-%m-%d %H:%M:%S')"

    # HDFSè¾“å‡ºç›®å½•ï¼ˆæ¯ä¸ªå®éªŒç‹¬ç«‹çš„è¾“å‡ºç›®å½•ï¼‰
    output_dir="$HDFS_OUTPUT_DIR/${name}_without_combiner"

    # æœ¬åœ°æ—¥å¿—æ–‡ä»¶
    log_file="$RESULTS_DIR/${name}_without_combiner.log"

    # è®°å½•å¼€å§‹æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
    start_time=$(date +%s)

    # è¿è¡ŒMapReduceä»»åŠ¡
    # hadoop jar: è¿è¡ŒJARåŒ…ä¸­çš„MapReduceç¨‹åº
    # å‚æ•°1: JARæ–‡ä»¶è·¯å¾„
    # å‚æ•°2: ä¸»ç±»åï¼ˆåŒ…å«mainæ–¹æ³•ï¼‰
    # å‚æ•°3: HDFSè¾“å…¥è·¯å¾„
    # å‚æ•°4: HDFSè¾“å‡ºè·¯å¾„
    # > $log_file 2>&1: å°†æ ‡å‡†è¾“å‡ºå’Œé”™è¯¯è¾“å‡ºéƒ½é‡å®šå‘åˆ°æ—¥å¿—æ–‡ä»¶
    hadoop jar $WITHOUT_JAR \
        com.hadoop.wordcount.without.WordCountDriver \
        $HDFS_INPUT_DIR/$dataset \
        $output_dir \
        > $log_file 2>&1

    # è®°å½•ç»“æŸæ—¶é—´æˆ³
    end_time=$(date +%s)

    # è®¡ç®—æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
    duration=$((end_time - start_time))

    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦æˆåŠŸ
    # $?: ä¸Šä¸€ä¸ªå‘½ä»¤çš„é€€å‡ºçŠ¶æ€ç ï¼Œ0è¡¨ç¤ºæˆåŠŸ
    if [ $? -eq 0 ]; then
        echo "   âœ“ æˆåŠŸ! è€—æ—¶: ${duration}ç§’"
        # å°†æ‰§è¡Œæ—¶é—´è¿½åŠ åˆ°æ—¥å¿—æ–‡ä»¶æœ«å°¾ï¼Œæ–¹ä¾¿åç»­åˆ†æ
        echo "EXECUTION_TIME=${duration}" >> $log_file
        echo "   æ—¥å¿—: $log_file"
    else
        echo "   âœ— å¤±è´¥! æŸ¥çœ‹æ—¥å¿—: $log_file"
        echo "   å¸¸è§åŸå› :"
        echo "   1. HDFSè¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨"
        echo "   2. è¾“å‡ºç›®å½•å·²å­˜åœ¨"
        echo "   3. MapReduceé…ç½®é—®é¢˜"
    fi

    # ç­‰å¾…3ç§’ï¼Œé¿å…èµ„æºå†²çª
    sleep 3

    # ------------------------------------------------------------------------
    # å®éªŒB: å¸¦Combiner
    # ------------------------------------------------------------------------
    experiment_num=$((experiment_num + 1))
    echo ""
    echo ">>> å®éªŒ $experiment_num/$total_experiments: ${name}_with_combiner"
    echo "å¼€å§‹æ—¶é—´: $(date '+%Y-%m-%d %H:%M:%S')"

    output_dir="$HDFS_OUTPUT_DIR/${name}_with_combiner"
    log_file="$RESULTS_DIR/${name}_with_combiner.log"

    start_time=$(date +%s)

    hadoop jar $WITH_JAR \
        com.hadoop.wordcount.with.WordCountDriver \
        $HDFS_INPUT_DIR/$dataset \
        $output_dir \
        > $log_file 2>&1

    end_time=$(date +%s)
    duration=$((end_time - start_time))

    if [ $? -eq 0 ]; then
        echo "   âœ“ æˆåŠŸ! è€—æ—¶: ${duration}ç§’"
        echo "EXECUTION_TIME=${duration}" >> $log_file
        echo "   æ—¥å¿—: $log_file"
    else
        echo "   âœ— å¤±è´¥! æŸ¥çœ‹æ—¥å¿—: $log_file"
    fi

    # ç­‰å¾…3ç§’
    sleep 3
done

# ----------------------------------------------------------------------------
# 7. å®éªŒå®Œæˆæ€»ç»“
# ----------------------------------------------------------------------------
echo ""
echo "=========================================="
echo "  âœ“ æ‰€æœ‰å®éªŒå®Œæˆ!"
echo "=========================================="
echo ""
echo "å®éªŒç»“æœæ±‡æ€»:"
echo "  æ€»å®éªŒæ•°: $total_experiments"
echo "  æˆåŠŸå®éªŒ: $(ls $RESULTS_DIR/*.log 2>/dev/null | wc -l)"
echo ""
echo "æ—¥å¿—æ–‡ä»¶ä½ç½®: $RESULTS_DIR"
ls -lh $RESULTS_DIR/*.log 2>/dev/null

echo ""
echo "HDFSè¾“å‡ºä½ç½®:"
hdfs dfs -ls $HDFS_OUTPUT_DIR/

echo ""
echo "=========================================="
echo "ä¸‹ä¸€æ­¥æ“ä½œ:"
echo "  åˆ†æç»“æœ: ./analyze_results.sh"
echo "=========================================="
```

#### 4. analyze_results.sh

```bash
#!/bin/bash
# ============================================================================
# è„šæœ¬åç§°: analyze_results.sh
# åŠŸèƒ½æè¿°: åˆ†æMapReduceå®éªŒç»“æœï¼Œç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š
# è¾“å…¥: results/*.log æ—¥å¿—æ–‡ä»¶
# è¾“å‡º: performance_metrics.csv + æ§åˆ¶å°åˆ†ææŠ¥å‘Š
# ============================================================================

# ----------------------------------------------------------------------------
# 1. å…¨å±€é…ç½®
# ----------------------------------------------------------------------------
PROJECT_DIR=/export/data/code
RESULTS_DIR=$PROJECT_DIR/results

echo "=========================================="
echo "  MapReduce Combiner å®éªŒç»“æœåˆ†æ"
echo "=========================================="
echo "ç»“æœç›®å½•: $RESULTS_DIR"
echo ""

# ----------------------------------------------------------------------------
# 2. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶æ˜¯å¦å­˜åœ¨
# ----------------------------------------------------------------------------
echo "æ£€æŸ¥å®éªŒæ—¥å¿—æ–‡ä»¶..."

if [ ! -d "$RESULTS_DIR" ]; then
    echo "âœ— é”™è¯¯: ç»“æœç›®å½•ä¸å­˜åœ¨ - $RESULTS_DIR"
    echo "è¯·å…ˆè¿è¡Œ: ./run_experiments.sh"
    exit 1
fi

# ç»Ÿè®¡æ—¥å¿—æ–‡ä»¶æ•°é‡
log_count=$(ls $RESULTS_DIR/*.log 2>/dev/null | wc -l)

if [ $log_count -eq 0 ]; then
    echo "âœ— é”™è¯¯: æ‰¾ä¸åˆ°ä»»ä½•æ—¥å¿—æ–‡ä»¶ (.log)"
    echo "è¯·å…ˆè¿è¡Œ: ./run_experiments.sh"
    exit 1
elif [ $log_count -lt 6 ]; then
    echo "âš ï¸  è­¦å‘Š: åªæ‰¾åˆ° $log_count ä¸ªæ—¥å¿—æ–‡ä»¶ï¼Œåº”è¯¥æœ‰6ä¸ª"
    echo "éƒ¨åˆ†å®éªŒå¯èƒ½å¤±è´¥ï¼Œåˆ†æå°†ç»§ç»­..."
else
    echo "âœ“ æ‰¾åˆ° $log_count ä¸ªæ—¥å¿—æ–‡ä»¶"
fi

# åˆ—å‡ºæ‰€æœ‰æ—¥å¿—æ–‡ä»¶
echo ""
echo "æ—¥å¿—æ–‡ä»¶åˆ—è¡¨:"
ls -lh $RESULTS_DIR/*.log

# ----------------------------------------------------------------------------
# 3. åˆ›å»ºCSVç»“æœæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´
# ----------------------------------------------------------------------------
csv_file="$RESULTS_DIR/performance_metrics.csv"

echo ""
echo "æ­£åœ¨ç”ŸæˆCSVç»“æœæ–‡ä»¶: $csv_file"

# CSVè¡¨å¤´å®šä¹‰
# å„åˆ—å«ä¹‰:
# - å®éªŒåç§°: uniform_with_combinerç­‰
# - æ‰§è¡Œæ—¶é—´: ä»»åŠ¡æ€»è€—æ—¶ï¼ˆç§’ï¼‰
# - Mapè¾“å…¥è®°å½•: Mapé˜¶æ®µè¯»å–çš„è®°å½•æ•°
# - Mapè¾“å‡ºè®°å½•: Mapé˜¶æ®µè¾“å‡ºçš„é”®å€¼å¯¹æ•°
# - Combineè¾“å…¥è®°å½•: Combineræ¥æ”¶çš„è®°å½•æ•°
# - Combineè¾“å‡ºè®°å½•: Combinerè¾“å‡ºçš„è®°å½•æ•°
# - Reduceè¾“å…¥ç»„: Reduceræ¥æ”¶çš„keyç»„æ•°
# - Reduceè¾“å‡ºè®°å½•: æœ€ç»ˆè¾“å‡ºçš„è®°å½•æ•°
# - æ•°æ®å‹ç¼©æ¯”: Mapè¾“å‡º/Combinerè¾“å‡ºï¼Œè¡¡é‡Combineræ•ˆæœ
cat > $csv_file << 'EOF'
å®éªŒåç§°,æ‰§è¡Œæ—¶é—´(ç§’),Mapè¾“å…¥è®°å½•,Mapè¾“å‡ºè®°å½•,Combineè¾“å…¥è®°å½•,Combineè¾“å‡ºè®°å½•,Reduceè¾“å…¥ç»„,Reduceè¾“å‡ºè®°å½•,æ•°æ®å‹ç¼©æ¯”
EOF

echo "âœ“ CSVè¡¨å¤´åˆ›å»ºå®Œæˆ"

# ----------------------------------------------------------------------------
# 4. æå–æ¯ä¸ªå®éªŒçš„æ€§èƒ½æŒ‡æ ‡
# ----------------------------------------------------------------------------
echo ""
echo "æ­£åœ¨ä»æ—¥å¿—ä¸­æå–æ€§èƒ½æŒ‡æ ‡..."
echo ""

# éå†æ‰€æœ‰æ—¥å¿—æ–‡ä»¶
for log in $RESULTS_DIR/*.log; do
    # è·å–å®éªŒåç§°ï¼ˆå»æ‰è·¯å¾„å’Œ.logåç¼€ï¼‰
    exp_name=$(basename $log .log)

    echo "  å¤„ç†: $exp_name"

    # ----------------------------------------------------------------------
    # ä»æ—¥å¿—ä¸­æå–å„é¡¹æŒ‡æ ‡
    # ----------------------------------------------------------------------

    # æ‰§è¡Œæ—¶é—´ï¼ˆæˆ‘ä»¬åœ¨è¿è¡Œè„šæœ¬ä¸­æ‰‹åŠ¨æ·»åŠ çš„ï¼‰
    exec_time=$(grep "EXECUTION_TIME" $log | cut -d'=' -f2)

    # Mapè¾“å…¥è®°å½•æ•°
    # grep: æŸ¥æ‰¾åŒ…å«"Map input records"çš„è¡Œ
    # tail -1: å–æœ€åä¸€è¡Œï¼ˆé˜²æ­¢æœ‰å¤šæ¬¡é‡è¯•ï¼‰
    # awk '{print $NF}': æ‰“å°æœ€åä¸€ä¸ªå­—æ®µï¼ˆæ•°å­—ï¼‰
    map_input=$(grep "Map input records" $log | tail -1 | awk '{print $NF}')

    # Mapè¾“å‡ºè®°å½•æ•°
    map_output=$(grep "Map output records" $log | tail -1 | awk '{print $NF}')

    # Combinerè¾“å…¥è®°å½•æ•°ï¼ˆåªæœ‰with-combinerç‰ˆæœ¬æœ‰è¿™ä¸ªæŒ‡æ ‡ï¼‰
    combine_input=$(grep "Combine input records" $log | tail -1 | awk '{print $NF}')

    # Combinerè¾“å‡ºè®°å½•æ•°
    combine_output=$(grep "Combine output records" $log | tail -1 | awk '{print $NF}')

    # Reducerè¾“å…¥ç»„æ•°
    reduce_input=$(grep "Reduce input groups" $log | tail -1 | awk '{print $NF}')

    # Reducerè¾“å‡ºè®°å½•æ•°
    reduce_output=$(grep "Reduce output records" $log | tail -1 | awk '{print $NF}')

    # ----------------------------------------------------------------------
    # è®¡ç®—æ•°æ®å‹ç¼©æ¯”
    # ----------------------------------------------------------------------

    # å¦‚æœæ²¡æœ‰Combinerï¼ˆwithoutç‰ˆæœ¬ï¼‰ï¼Œè®¾ç½®ä¸ºN/A
    if [ -z "$combine_input" ] || [ "$combine_input" == "0" ]; then
        combine_input="N/A"
        combine_output="N/A"
        compression_ratio="1.0"  # æ— å‹ç¼©
    else
        # è®¡ç®—å‹ç¼©æ¯” = Mapè¾“å‡ºè®°å½•æ•° / Combinerè¾“å‡ºè®°å½•æ•°
        # ä¾‹å¦‚: Mapè¾“å‡º1000æ¡ï¼ŒCombinerè¾“å‡º100æ¡ï¼Œå‹ç¼©æ¯”=10
        # å‹ç¼©æ¯”è¶Šå¤§ï¼Œè¯´æ˜Combineræ•ˆæœè¶Šå¥½
        if [ "$combine_output" != "0" ]; then
            # bc: å‘½ä»¤è¡Œè®¡ç®—å™¨ï¼Œscale=2è¡¨ç¤ºä¿ç•™2ä½å°æ•°
            compression_ratio=$(echo "scale=2; $map_output / $combine_output" | bc)
        else
            compression_ratio="N/A"
        fi
    fi

    # ----------------------------------------------------------------------
    # å°†æ•°æ®å†™å…¥CSV
    # ----------------------------------------------------------------------
    echo "$exp_name,$exec_time,$map_input,$map_output,$combine_input,$combine_output,$reduce_input,$reduce_output,$compression_ratio" >> $csv_file
done

echo ""
echo "âœ“ æ€§èƒ½æŒ‡æ ‡æå–å®Œæˆ"

# ----------------------------------------------------------------------------
# 5. æ˜¾ç¤ºCSVè¡¨æ ¼
# ----------------------------------------------------------------------------
echo ""
echo "=========================================="
echo "  æ€§èƒ½æŒ‡æ ‡æ€»è§ˆ"
echo "=========================================="

# columnå‘½ä»¤ï¼šå°†CSVæ ¼å¼åŒ–ä¸ºå¯¹é½çš„è¡¨æ ¼
# -t: è‡ªåŠ¨å¯¹é½åˆ—
# -s',': ä½¿ç”¨é€—å·ä½œä¸ºåˆ†éš”ç¬¦
column -t -s',' $csv_file

# ----------------------------------------------------------------------------
# 6. è¯¦ç»†åˆ†æ - Uniformæ•°æ®é›†
# ----------------------------------------------------------------------------
echo ""
echo "=========================================="
echo "  è¯¦ç»†åˆ†ææŠ¥å‘Š"
echo "=========================================="

echo ""
echo "ã€æ•°æ®é›†1: Uniform - å‡åŒ€åˆ†å¸ƒã€‘"
echo "ç‰¹å¾: æ‰€æœ‰å•è¯å‡ºç°é¢‘ç‡ç›¸è¿‘ï¼Œæ•°æ®åˆ†å¸ƒå‡åŒ€"
echo "-------------------------------------------"

# ä»CSVä¸­æå–uniformçš„ä¸¤ä¸ªå®éªŒæ•°æ®
uniform_without=$(grep "uniform_without" $csv_file | cut -d',' -f2)
uniform_with=$(grep "uniform_with" $csv_file | cut -d',' -f2)

# æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
if [ -n "$uniform_without" ] && [ -n "$uniform_with" ]; then
    # è®¡ç®—æ€§èƒ½æå‡ç™¾åˆ†æ¯”
    # å…¬å¼: (ä¸å¸¦Combinerè€—æ—¶ - å¸¦Combinerè€—æ—¶) / ä¸å¸¦Combinerè€—æ—¶ * 100
    improvement=$(echo "scale=2; ($uniform_without - $uniform_with) / $uniform_without * 100" | bc)

    echo "  Without Combiner: ${uniform_without}ç§’"
    echo "  With Combiner:    ${uniform_with}ç§’"
    echo "  æ€§èƒ½æå‡:         ${improvement}%"

    # æå–å‹ç¼©æ¯”
    compression=$(grep "uniform_with" $csv_file | cut -d',' -f9)
    echo "  æ•°æ®å‹ç¼©æ¯”:       ${compression}:1"

    # åˆ†æç»“è®º
    if (( $(echo "$improvement > 20" | bc -l) )); then
        echo "  âœ“ ç»“è®º: Combineræ˜¾è‘—æå‡æ€§èƒ½ï¼Œå‡å°‘äº†ç½‘ç»œä¼ è¾“"
    else
        echo "  âš ï¸  ç»“è®º: Combineræå‡æœ‰é™"
    fi
else
    echo "  âœ— è­¦å‘Š: ç¼ºå°‘å®éªŒæ•°æ®"
fi

# ----------------------------------------------------------------------------
# 7. è¯¦ç»†åˆ†æ - Skewedæ•°æ®é›†
# ----------------------------------------------------------------------------
echo ""
echo "ã€æ•°æ®é›†2: Skewed - æ•°æ®å€¾æ–œã€‘"
echo "ç‰¹å¾: å°‘æ•°çƒ­ç‚¹è¯(å¦‚'the','of')å æ®å¤§é‡è®°å½•"
echo "-------------------------------------------"

skewed_without=$(grep "skewed_without" $csv_file | cut -d',' -f2)
skewed_with=$(grep "skewed_with" $csv_file | cut -d',' -f2)

if [ -n "$skewed_without" ] && [ -n "$skewed_with" ]; then
    improvement=$(echo "scale=2; ($skewed_without - $skewed_with) / $skewed_without * 100" | bc)

    echo "  Without Combiner: ${skewed_without}ç§’"
    echo "  With Combiner:    ${skewed_with}ç§’"
    echo "  æ€§èƒ½æå‡:         ${improvement}%"

    compression=$(grep "skewed_with" $csv_file | cut -d',' -f9)
    echo "  æ•°æ®å‹ç¼©æ¯”:       ${compression}:1"

    if (( $(echo "$improvement > 30" | bc -l) )); then
        echo "  âœ“ ç»“è®º: Combinerå¯¹çƒ­ç‚¹è¯åˆå¹¶æ•ˆæœæä½³"
    else
        echo "  âš ï¸  ç»“è®º: æ•ˆæœä½äºé¢„æœŸ"
    fi
else
    echo "  âœ— è­¦å‘Š: ç¼ºå°‘å®éªŒæ•°æ®"
fi

# ----------------------------------------------------------------------------
# 8. è¯¦ç»†åˆ†æ - Uniqueæ•°æ®é›†
# ----------------------------------------------------------------------------
echo ""
echo "ã€æ•°æ®é›†3: Unique - é«˜å”¯ä¸€æ€§ã€‘"
echo "ç‰¹å¾: å‡ ä¹æ¯ä¸ªå•è¯éƒ½å”¯ä¸€ï¼Œé‡å¤åº¦æä½"
echo "-------------------------------------------"

unique_without=$(grep "unique_without" $csv_file | cut -d',' -f2)
unique_with=$(grep "unique_with" $csv_file | cut -d',' -f2)

if [ -n "$unique_without" ] && [ -n "$unique_with" ]; then
    # å¯¹äºuniqueæ•°æ®ï¼ŒCombinerå¯èƒ½åè€Œå¢åŠ è€—æ—¶
    diff=$(echo "scale=2; $unique_with - $unique_without" | bc)

    echo "  Without Combiner: ${unique_without}ç§’"
    echo "  With Combiner:    ${unique_with}ç§’"
    echo "  æ—¶é—´å·®å¼‚:         ${diff}ç§’"

    compression=$(grep "unique_with" $csv_file | cut -d',' -f9)
    echo "  æ•°æ®å‹ç¼©æ¯”:       ${compression}:1"

    # åˆ†æCombinerçš„è´Ÿé¢å½±å“
    if (( $(echo "$compression < 1.5" | bc -l) )); then
        echo "  âš ï¸  è­¦å‘Š: å‹ç¼©æ¯”æ¥è¿‘1ï¼ŒCombinerå‡ ä¹æ— æ•ˆ"
    fi

    if (( $(echo "$diff > 0" | bc -l) )); then
        echo "  âš ï¸  è­¦å‘Š: Combineråè€Œå¢åŠ äº† ${diff}ç§’"
        echo "     åŸå› : æ•°æ®æ— é‡å¤ï¼ŒCombineré¢å¤–å¤„ç†æˆæœ¬ > æ”¶ç›Š"
    fi

    echo "  âœ“ ç»“è®º: é«˜å”¯ä¸€æ€§æ•°æ®ä¸é€‚åˆä½¿ç”¨Combiner"
else
    echo "  âœ— è­¦å‘Š: ç¼ºå°‘å®éªŒæ•°æ®"
fi

# ----------------------------------------------------------------------------
# 9. æ€»ç»“å’Œå»ºè®®
# ----------------------------------------------------------------------------
echo ""
echo "=========================================="
echo "  å®éªŒç»“è®ºä¸å»ºè®®"
echo "=========================================="
echo ""
echo "ã€æ ¸å¿ƒå‘ç°ã€‘"
echo "1. å‡åŒ€åˆ†å¸ƒæ•°æ®(Uniform):"
echo "   - Combineræ˜¾è‘—å‡å°‘ç½‘ç»œä¼ è¾“é‡"
echo "   - æ€§èƒ½æå‡æ˜æ˜¾"
echo ""
echo "2. æ•°æ®å€¾æ–œåœºæ™¯(Skewed):"
echo "   - Combinerå¯¹çƒ­ç‚¹è¯çš„åˆå¹¶æ•ˆæœæœ€ä½³"
echo "   - å¤§å¹…å‡å°‘Reducerè´Ÿè½½"
echo ""
echo "3. é«˜å”¯ä¸€æ€§æ•°æ®(Unique):"
echo "   - Combinerå‡ ä¹æ— æ•ˆ"
echo "   - é¢å¤–å¤„ç†æˆæœ¬å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™"
echo ""
echo "ã€å®è·µå»ºè®®ã€‘"
echo "âœ“ ä½¿ç”¨Combinerçš„åœºæ™¯:"
echo "  - æ•°æ®é‡å¤åº¦é«˜ (>30%)"
echo "  - å­˜åœ¨çƒ­ç‚¹key"
echo "  - ç½‘ç»œå¸¦å®½æ˜¯ç“¶é¢ˆ"
echo ""
echo "âœ— é¿å…ä½¿ç”¨Combinerçš„åœºæ™¯:"
echo "  - æ•°æ®å”¯ä¸€æ€§é«˜ (>90%)"
echo "  - æ•°æ®é‡å¾ˆå°"
echo "  - Combineré€»è¾‘å¤æ‚è€—æ—¶"
echo ""
echo "ã€æ€§èƒ½è°ƒä¼˜å»ºè®®ã€‘"
echo "1. å…ˆåˆ†ææ•°æ®ç‰¹å¾ (å”¯ä¸€æ€§ã€é‡å¤åº¦)"
echo "2. å°è§„æ¨¡æµ‹è¯•å¯¹æ¯”æ€§èƒ½"
echo "3. ç›‘æ§ç½‘ç»œä¼ è¾“å’ŒCPUå¼€é”€"
echo "4. æ ¹æ®å®é™…åœºæ™¯åŠ¨æ€è°ƒæ•´"
echo ""
echo "=========================================="
echo "  åˆ†æå®Œæˆ!"
echo "=========================================="
echo ""
echo "ç»“æœæ–‡ä»¶: $csv_file"
echo "æ—¥å¿—ç›®å½•: $RESULTS_DIR"
echo "=========================================="
```

## 3. å°†æ–‡ä»¶ä¼ è‡³è™šæ‹Ÿæœº

```bash
#powershellä¸Šè¿›è¡Œ
scp -r code root@192.168.204.132:/export/data/
```

## 4. æ·»åŠ æ‰§è¡Œæƒé™å¹¶è¿è¡Œ

```bash
cd /export/data/code/scripts

# æ·»åŠ æ‰§è¡Œæƒé™
chmod +x upload_data.sh
chmod +x compile_and_package.sh
chmod +x run_experiments.sh
chmod +x analyze_results.sh

# æŒ‰é¡ºåºæ‰§è¡Œ
./upload_data.sh              # ä¸Šä¼ æ•°æ®åˆ°HDFS
./compile_and_package.sh      # ç¼–è¯‘æ‰“åŒ…Javaä»£ç 
./run_experiments.sh          # è¿è¡Œæ‰€æœ‰å®éªŒ
./analyze_results.sh          # åˆ†æç»“æœ
```

## 5. ä¸‹è½½ç»“æœåˆ°Windows

å®éªŒå®Œæˆåï¼ŒæŠŠresultsç›®å½•ä¸‹è½½å›Windowsï¼š

~~~powershell
# åœ¨Windows PowerShellæ‰§è¡Œ
cd D:/Minjie/Desktop/mapreduce-combiner-analysis
scp -r root@192.168.204.132:/export/data/code/results ./code/
```

---

## æœ€ç»ˆç›®å½•ç»“æ„


â”œâ”€code
â”‚  â”œâ”€cluster-config
â”‚  â”œâ”€data
â”‚  â”‚  â”‚  skewed_data.txt
â”‚  â”‚  â”‚  uniform_data.txt
â”‚  â”‚  â”‚  unique_data.txt
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€data-generator
â”‚  â”‚          generate_all_data.py
â”‚  â”‚          validate_data.py
â”‚  â”‚
â”‚  â”œâ”€mapreduce
â”‚  â”‚  â”œâ”€with-combiner
â”‚  â”‚  â”‚  â””â”€com
â”‚  â”‚  â”‚      â””â”€hadoop
â”‚  â”‚  â”‚          â””â”€wordcount
â”‚  â”‚  â”‚              â””â”€with
â”‚  â”‚  â”‚                      WordCountDriver.java
â”‚  â”‚  â”‚                      WordCountMapper.java
â”‚  â”‚  â”‚                      WordCountReducer.java
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€without-combiner
â”‚  â”‚      â””â”€com
â”‚  â”‚          â””â”€hadoop
â”‚  â”‚              â””â”€wordcount
â”‚  â”‚                  â””â”€without
â”‚  â”‚                          WordCountDriver.java
â”‚  â”‚                          WordCountMapper.java
â”‚  â”‚                          WordCountReducer.java
â”‚  â”‚
â”‚  â”œâ”€results
â”‚  â”‚      performance_metrics.csv
â”‚  â”‚      skewed_without_combiner.log
â”‚  â”‚      skewed_with_combiner.log
â”‚  â”‚      uniform_without_combiner.log
â”‚  â”‚      uniform_with_combiner.log
â”‚  â”‚      unique_without_combiner.log
â”‚  â”‚      unique_with_combiner.log
â”‚  â”‚
â”‚  â””â”€scripts
â”‚          analyze_results.sh
â”‚          compile_and_package.sh
â”‚          run_experiments.sh
â”‚          upload_data.sh
~~~

####  è¿è¡Œç»“æœ

![image-20251126013933483](.\images\image-20251126013933483.png)

![image-20251126014154273](.\images\image-20251126014154273.png)
