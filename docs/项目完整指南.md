# Hadoop MapReduce Combiner性能分析项目

# 项目概述

**研究目标**: 通过实验验证Combiner在不同数据分布下对MapReduce性能的影响

**核心研究问题:**

1. 分析Combiner在MapReduce执行过程中的作用与效果
2. Combiner是否能够有效减少Shuffle阶段的数据量?
3. 在不同的key分布(均匀分布与数据倾斜)下,性能提升效果有何差异?
4. 是否所有场景都适合使用Combiner?

**项目结构:**
```
├─code
│  ├─analysis
│  │      analysis.py              
│  │      four_plots_final.png      
│  │
│  ├─cluster-config                 
│  │
│  ├─data
│  │  │  skewed_data.txt
│  │  │  uniform_data.txt
│  │  │  unique_data.txt
│  │  │
│  │  └─data-generator
│  │          generate_all_data.py
│  │          validate_data.py
│  │
│  ├─mapreduce
│  │  ├─with-combiner
│  │  │  └─com
│  │  │      └─hadoop
│  │  │          └─wordcount
│  │  │              └─with
│  │  │                      WordCountDriver.java
│  │  │                      WordCountMapper.java
│  │  │                      WordCountReducer.java
│  │  │
│  │  └─without-combiner
│  │      └─com
│  │          └─hadoop
│  │              └─wordcount
│  │                  └─without
│  │                          WordCountDriver.java
│  │                          WordCountMapper.java
│  │                          WordCountReducer.java
│  │
│  ├─results
│  │      performance_metrics.csv
│  │      skewed_without_combiner.log
│  │      skewed_with_combiner.log
│  │      uniform_without_combiner.log
│  │      uniform_with_combiner.log
│  │      unique_without_combiner.log
│  │      unique_with_combiner.log
│  │
│  └─scripts
│          analyze_results.sh
│          compile_and_package.sh
│          run_experiments.sh
│          upload_data.sh
```
# Step1：伪分布式集群环境搭建

## 1. 预准备

### 1.1 虚拟机网络配置

**查看IP地址**

~~~bash
ip addr
```

**集群IP规划:**
```
192.168.204.132  master
192.168.204.133  worker01
192.168.204.134  worker02
~~~

**修改主机名** (在各自虚拟机上执行):

```bash
hostnamectl set-hostname master
hostnamectl set-hostname worker01
hostnamectl set-hostname worker02
```

**配置静态IP:**

```bash
vi /etc/sysconfig/network-scripts/ifcfg-ens33
```

修改内容:

```ini
BOOTPROTO="static"
IPADDR=192.168.204.132      # 各节点使用对应IP
NETMASK=255.255.255.0
GATEWAY=192.168.204.2
DNS1=8.8.8.8
```

**配置主机映射:**

~~~bash
vi /etc/hosts
```

添加内容:
```
192.168.204.132  master
192.168.204.133  worker01
192.168.204.134  worker02
~~~

**重启并测试网络:**

```bash
reboot

# 重启后测试
ifconfig
ping baidu.com
```

### 1.2 SSH免密登录配置

**配置YUM源并安装SSH:**

```bash
# 配置阿里云YUM源
vi /etc/yum.repos.d/CentOS-Base.repo
# 修改为: http://mirrors.aliyun.com/centos/$releasever/os/$basearch/

# 安装SSH服务
yum install openssh-server -y
```

**生成并分发密钥:**

```bash
# 生成SSH密钥对
ssh-keygen -t rsa

# 在master节点复制公钥到本地
ssh-copy-id master

# 分发公钥到其他节点
scp /root/.ssh/authorized_keys worker01:/root/.ssh
scp /root/.ssh/authorized_keys worker02:/root/.ssh
```

**创建工作目录:**

```bash
mkdir -p /export/data       # 存放数据文件
mkdir -p /export/servers    # 软件安装目录
mkdir -p /export/software   # 安装包存放目录
```

**安装基础工具:**

```bash
yum install lrzsz -y
```

------

## 2. JDK安装与配置

### 2.1 卸载系统自带JDK



```bash
# 查看已安装的JDK
rpm -qa | grep jdk

# 卸载所有JDK包
rpm -e --nodeps java-1.7.0-openjdk-headless-1.7.0.261-2.6.22.2.el7_8.x86_64
rpm -e --nodeps copy-jdk-configs-3.3-10.el7_5.noarch
rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.262.b10-1.el7.x86_64
rpm -e --nodeps java-1.8.0-openjdk-1.8.0.262.b10-1.el7.x86_64
rpm -e --nodeps java-1.7.0-openjdk-1.7.0.261-2.6.22.2.el7_8.x86_64
```

### 2.2 安装JDK 1.8

**下载地址:** [Oracle JDK 8u181](https://www.oracle.com/java/technologies/javase/javase8-archive-downloads.html)

```bash
cd /export/software

# 上传JDK安装包
rz  # 选择 jdk-8u181-linux-x64.tar.gz

# 解压到安装目录
tar zxvf jdk-8u181-linux-x64.tar.gz -C /export/servers/

# 重命名
cd /export/servers
mv jdk1.8.0_181 jdk
```

**配置环境变量:**

```bash
vi /etc/profile
```

添加以下内容:

```bash
export JAVA_HOME=/export/servers/jdk
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
```

重新加载并验证:

```bash
source /etc/profile
java -version
```

**分发JDK到其他节点:**

```bash
cd /export/servers
scp -r jdk worker01:$PWD
scp -r jdk worker02:$PWD

scp /etc/profile worker01:/etc/profile
scp /etc/profile worker02:/etc/profile

ssh worker01 "source /etc/profile"
ssh worker02 "source /etc/profile"
```

------

## 3. Hadoop安装与配置

### 3.1 安装Hadoop

**下载地址:** [Hadoop 2.7.3](https://archive.apache.org/dist/hadoop/core/hadoop-2.7.3/hadoop-2.7.3.tar.gz)

```bash
cd /export/software
rz  # 选择 hadoop-2.7.3.tar.gz

tar zxvf hadoop-2.7.3.tar.gz -C /export/servers/
cd /export/servers
mv hadoop-2.7.3 hadoop
```

**配置环境变量:**

```bash
vi /etc/profile
```

添加:

```bash
export HADOOP_HOME=/export/servers/hadoop
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
```

```bash
source /etc/profile
hadoop version
```

### 3.2 修改Hadoop配置文件

配置文件位置: `/export/servers/hadoop/etc/hadoop`

**(1) hadoop-env.sh**

```bash
vi hadoop-env.sh

#添加以下内容
export JAVA_HOME=/export/servers/jdk
```

**(2) core-site.xml**

```bash
vi core-site.xml
```

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://master:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/export/servers/hadoop/tmp</value>
    </property>
</configuration>
```

**(3) hdfs-site.xml**

```bash
vi hdfs-site.xml
```

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>worker01:50090</value>
    </property>
</configuration>
```

**(4) mapred-site.xml**

```bash
cp mapred-site.xml.template mapred-site.xml
vi mapred-site.xml
```

```xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
```

**(5) yarn-site.xml**

```xml
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>
```

**(6) slaves**

~~~bash
vi slaves
```

修改为:
```
master
worker01
worker02
~~~

**分发Hadoop到其他节点:**

```bash
scp /etc/profile worker01:/etc/profile
scp /etc/profile worker02:/etc/profile

scp -r /export/ worker01:/
scp -r /export/ worker02:/

ssh worker01 "source /etc/profile"
ssh worker02 "source /etc/profile"
```

------

## 4. 启动Hadoop集群

### 4.1 格式化HDFS

```bash
# 注意:仅在首次启动时执行一次
hdfs namenode -format
```

### 4.2 关闭防火墙

```bash
# 在所有节点执行
systemctl stop firewalld
systemctl disable firewalld
```

### 4.3 启动集群

```bash
start-dfs.sh    # 启动HDFS
start-yarn.sh   # 启动YARN
# 或一键启动: start-all.sh
```

### 4.4 验证集群状态

```bash
jps
```

**预期输出:**

- **master节点:** NameNode, SecondaryNameNode, DataNode, ResourceManager, NodeManager
- **worker节点:** DataNode, NodeManager

### 4.5 Web界面访问

- **HDFS管理界面:** http://192.168.204.132:50070
  - ![image-20251125003542119](.\images\image-20251125003542119.png)
- **YARN资源管理界面:** http://192.168.204.132:8088
  - ![image-20251125003455570](.\images\image-20251125003455570.png)

# Step 2：数据生成

本部分将生成两种不同分布特征的测试数据，用于验证Combiner在不同场景下的性能表现。

## 1. 数据生成策略

我们将生成三种数据集来模拟不同的实际场景：

- **uniform**: 10000个word_xxxxx均匀分布
- **skewed**: 200个hot_word_xxx (80%频率) + 9800个cold_word_xxxx (20%频率)
- **unique**: 50000000个uuid_xxxxxxxx (每个唯一)

------

## 2. Python数据生成脚本

**建议方案：在Windows本地生成数据，然后通过rz上传到虚拟机**

- 不需要在虚拟机安装Python环境
- Windows上Python环境配置更方便
- 生成速度可能更快（本地资源更充足）
- 可以在本地先验证数据再上传

**在本地Windows创建项目目录**

```powershell
# 在你的项目目录下
cd code/data-generator
```

### 2.1 生成数据

创建 `New-Item generate_all_data.py -ItemType File`:

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
统一的数据生成脚本
生成三种不同分布特征的WordCount测试数据
所有数据格式统一：每行包含多个单词
"""

import random
import time
import os

# ==================== 统一配置 ====================
NUM_LINES = 1000000          # 总行数：100万行
WORDS_PER_LINE = 50          # 每行单词数：50个
NUM_UNIQUE_WORDS = 10000     # 唯一单词总数：1万个

# 数据倾斜配置
HOT_WORD_COUNT = 200         # 热点单词数：200个 (2%)
COLD_WORD_COUNT = 9800       # 冷门单词数：9800个 (98%)
HOT_RATIO = 0.8              # 热点单词频率占比：80%

# 输出目录（上级目录的data文件夹）
OUTPUT_DIR = "../data"

# ==================== 工具函数 ====================
def print_header(title):
    """打印标题"""
    print("\n" + "="*70)
    print(f"  {title}")
    print("="*70)

def print_progress(current, total, start_time, interval=100000):
    """打印进度条"""
    if (current + 1) % interval == 0:
        elapsed = time.time() - start_time
        progress = (current + 1) / total * 100
        speed = (current + 1) / elapsed
        eta = (total - current - 1) / speed
        print(f"进度: {progress:5.1f}% ({current+1:>8,}/{total:,}) | "
              f"耗时: {elapsed:6.1f}s | 速度: {speed:6.0f} 行/s | "
              f"预计剩余: {eta:6.1f}s")

def print_summary(output_file, num_lines, words_per_line, num_unique, 
                 total_time, extra_info=None):
    """打印生成摘要"""
    total_words = num_lines * words_per_line
    file_size = os.path.getsize(output_file) / (1024 * 1024)  # MB
    
    print("\n" + "-"*70)
    print("【文件信息】")
    print(f"  文件路径: {output_file}")
    print(f"  文件大小: {file_size:.2f} MB")
    print(f"  总行数: {num_lines:,}")
    print(f"  总单词数: {total_words:,}")
    print(f"  唯一单词数: {num_unique:,}")
    
    print(f"\n【性能指标】")
    print(f"  生成耗时: {total_time:.2f} 秒")
    print(f"  处理速度: {num_lines/total_time:.0f} 行/秒")
    
    print(f"\n【Combiner效果预估】")
    print(f"  Map输出记录数: {total_words:,}")
    print(f"  Combiner后记录数: ~{num_unique:,}")
    print(f"  数据压缩比: ~{total_words/num_unique:.1f}:1")
    
    if extra_info:
        print(f"\n【特殊说明】")
        for line in extra_info:
            print(f"  {line}")
    
    print("="*70)

# ==================== 1. 均匀分布数据 ====================
def generate_uniform_data():
    """生成均匀分布的数据"""
    output_file = os.path.join(OUTPUT_DIR, "uniform_data.txt")
    
    print_header("生成均匀分布数据 - 模拟结构化日志场景")
    print(f"配置: {NUM_LINES:,}行 × {WORDS_PER_LINE}词/行 = {NUM_LINES*WORDS_PER_LINE:,}个单词")
    print(f"唯一单词数: {NUM_UNIQUE_WORDS:,}")
    print(f"理论每个单词出现次数: {NUM_LINES*WORDS_PER_LINE/NUM_UNIQUE_WORDS:.0f}")
    print()
    
    start_time = time.time()
    
    # 生成单词池
    print("生成单词池...")
    word_pool = [f"word_{i:05d}" for i in range(NUM_UNIQUE_WORDS)]
    
    # 写入数据
    print(f"写入数据: {output_file}")
    print()
    
    with open(output_file, 'w') as f:
        for i in range(NUM_LINES):
            words = [random.choice(word_pool) for _ in range(WORDS_PER_LINE)]
            f.write(' '.join(words) + '\n')
            print_progress(i, NUM_LINES, start_time)
    
    total_time = time.time() - start_time
    
    extra_info = [
        "✓ 所有单词出现频率相近",
        "✓ Combiner预期效果显著",
        "✓ 适合验证Combiner的基本功能"
    ]
    
    print_summary(output_file, NUM_LINES, WORDS_PER_LINE, 
                 NUM_UNIQUE_WORDS, total_time, extra_info)
    
    return output_file

# ==================== 2. 数据倾斜分布 ====================
def generate_skewed_data():
    """生成数据倾斜的数据"""
    output_file = os.path.join(OUTPUT_DIR, "skewed_data.txt")
    
    print_header("生成数据倾斜分布数据 - 模拟热点词汇场景")
    print(f"配置: {NUM_LINES:,}行 × {WORDS_PER_LINE}词/行 = {NUM_LINES*WORDS_PER_LINE:,}个单词")
    print(f"热点单词: {HOT_WORD_COUNT}个 (占{HOT_WORD_COUNT/(HOT_WORD_COUNT+COLD_WORD_COUNT)*100:.1f}%), 频率占比{HOT_RATIO*100:.0f}%")
    print(f"冷门单词: {COLD_WORD_COUNT}个 (占{COLD_WORD_COUNT/(HOT_WORD_COUNT+COLD_WORD_COUNT)*100:.1f}%), 频率占比{(1-HOT_RATIO)*100:.0f}%")
    
    total_words = NUM_LINES * WORDS_PER_LINE
    hot_total = int(total_words * HOT_RATIO)
    cold_total = total_words - hot_total
    hot_avg = hot_total / HOT_WORD_COUNT
    cold_avg = cold_total / COLD_WORD_COUNT
    
    print(f"倾斜比例: {hot_avg/cold_avg:.1f}:1 (热点词平均出现{hot_avg:.0f}次 vs 冷门词{cold_avg:.0f}次)")
    print()
    
    start_time = time.time()
    
    # 生成单词池
    print("生成单词池...")
    hot_words = [f"hot_word_{i:03d}" for i in range(HOT_WORD_COUNT)]
    cold_words = [f"cold_word_{i:04d}" for i in range(COLD_WORD_COUNT)]
    
    # 写入数据
    print(f"写入数据: {output_file}")
    print()
    
    with open(output_file, 'w') as f:
        for i in range(NUM_LINES):
            words = []
            for _ in range(WORDS_PER_LINE):
                if random.random() < HOT_RATIO:
                    words.append(random.choice(hot_words))
                else:
                    words.append(random.choice(cold_words))
            f.write(' '.join(words) + '\n')
            print_progress(i, NUM_LINES, start_time)
    
    total_time = time.time() - start_time
    
    extra_info = [
        f"⚠️  数据存在显著倾斜 (80-20原则)",
        f"✓ Combiner对热点词效果显著",
        f"⚠️  可能导致Reducer负载不均衡",
        f"✓ 适合验证Combiner在倾斜场景的表现"
    ]
    
    print_summary(output_file, NUM_LINES, WORDS_PER_LINE,
                 HOT_WORD_COUNT + COLD_WORD_COUNT, total_time, extra_info)
    
    return output_file

# ==================== 3. 高唯一性数据 ====================
def generate_unique_data():
    """生成高唯一性的数据（几乎无重复）"""
    output_file = os.path.join(OUTPUT_DIR, "unique_data.txt")
    
    print_header("生成高唯一性数据 - 模拟唯一标识符场景")
    print(f"配置: {NUM_LINES:,}行 × {WORDS_PER_LINE}词/行 = {NUM_LINES*WORDS_PER_LINE:,}个单词")
    print(f"目标唯一性: >95%")
    print(f"场景: UUID、会话ID、唯一标识符等")
    print()
    
    start_time = time.time()
    
    # 写入数据
    print(f"写入数据: {output_file}")
    print()
    
    word_counter = 0
    with open(output_file, 'w') as f:
        for i in range(NUM_LINES):
            words = []
            for _ in range(WORDS_PER_LINE):
                # 格式: uuid_序号 (保证唯一性)
                words.append(f"uuid_{word_counter:08d}")
                word_counter += 1
            f.write(' '.join(words) + '\n')
            print_progress(i, NUM_LINES, start_time)
    
    total_time = time.time() - start_time
    
    total_words = NUM_LINES * WORDS_PER_LINE
    
    extra_info = [
        f"✗ 几乎每个单词都是唯一的 (100%唯一性)",
        f"✗ Combiner无法减少数据量",
        f"✗ 使用Combiner反而增加计算开销",
        f"✓ 适合验证'不是所有场景都适合Combiner'"
    ]
    
    print_summary(output_file, NUM_LINES, WORDS_PER_LINE,
                 total_words, total_time, extra_info)
    
    return output_file

# ==================== 主函数 ====================
def main():
    """主函数：生成所有测试数据"""
    # 确保输出目录存在
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        print(f"创建输出目录: {OUTPUT_DIR}")
    
    print("\n" + "█"*70)
    print("  MapReduce Combiner 性能分析 - 测试数据生成器")
    print("█"*70)
    print(f"\n统一配置:")
    print(f"  - 总行数: {NUM_LINES:,}")
    print(f"  - 每行单词数: {WORDS_PER_LINE}")
    print(f"  - 总单词数: {NUM_LINES * WORDS_PER_LINE:,}")
    print(f"  - 输出目录: {OUTPUT_DIR}")
    print(f"\n将生成三种数据集:")
    print(f"  1. uniform_data.txt  - 均匀分布")
    print(f"  2. skewed_data.txt   - 数据倾斜")
    print(f"  3. unique_data.txt   - 高唯一性")
    print()
    
    input("按 Enter 键开始生成数据...")
    
    total_start = time.time()
    
    # 生成三种数据
    files = []
    files.append(generate_uniform_data())
    files.append(generate_skewed_data())
    files.append(generate_unique_data())
    
    total_time = time.time() - total_start
    
    # 总结
    print("\n" + "█"*70)
    print("  所有数据生成完成!")
    print("█"*70)
    print(f"\n生成的文件:")
    for i, f in enumerate(files, 1):
        size = os.path.getsize(f) / (1024 * 1024)
        print(f"  {i}. {os.path.basename(f):20s} ({size:.2f} MB)")
    
    print(f"\n总耗时: {total_time:.2f} 秒")
    print(f"\n下一步:")
    print(f"  1. 运行 validate_data.py 验证数据特征")
    print(f"  2. 使用 scp 或 WinSCP 上传数据到虚拟机")
    print(f"     scp {OUTPUT_DIR}/*.txt root@192.168.204.132:/export/data/")
    print("█"*70 + "\n")

if __name__ == "__main__":
    main()
```

### 2.2 **数据验证脚本**

创建 `New-Item validate_data.py -ItemType File`:

```python
"""
验证生成的数据特征
统计数据分布情况
适配新的数据格式：每行包含多个单词
"""

import os
from collections import Counter
import time

def analyze_data_distribution(file_path, sample_lines=100000):
    """
    分析数据分布特征
    
    参数:
        file_path: 数据文件路径
        sample_lines: 采样行数（用于大文件）
    """
    print(f"分析数据文件: {file_path}")
    print(f"采样行数: {sample_lines:,}")
    print("-" * 60)
    
    if not os.path.exists(file_path):
        print(f"错误: 文件不存在")
        return
    
    start_time = time.time()
    
    # 读取采样数据
    word_counts = Counter()
    total_lines = 0
    total_words = 0
    words_per_line_list = []
    
    print("正在读取和分析数据...")
    with open(file_path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= sample_lines:
                break
            
            words = line.strip().split()
            words_per_line_list.append(len(words))
            
            for word in words:
                if word:
                    word_counts[word] += 1
                    total_words += 1
            
            total_lines += 1
            
            # 进度提示
            if (i + 1) % 10000 == 0:
                print(f"  已处理: {i+1:,} 行", end='\r')
    
    print(f"  已处理: {total_lines:,} 行 ✓")
    
    # 统计信息
    unique_words = len(word_counts)
    top_words = word_counts.most_common(20)
    bottom_words = word_counts.most_common()[-10:] if len(word_counts) > 10 else []
    
    # 计算分布指标
    counts = list(word_counts.values())
    avg_count = sum(counts) / len(counts) if counts else 0
    max_count = max(counts) if counts else 0
    min_count = min(counts) if counts else 0
    
    # 计算前20%单词的频率占比
    sorted_words = word_counts.most_common()
    top_20_percent = max(1, int(unique_words * 0.2))
    top_20_freq = sum(count for _, count in sorted_words[:top_20_percent])
    top_20_ratio = top_20_freq / total_words if total_words > 0 else 0
    
    # 计算每行单词数统计
    avg_words_per_line = sum(words_per_line_list) / len(words_per_line_list) if words_per_line_list else 0
    min_words_per_line = min(words_per_line_list) if words_per_line_list else 0
    max_words_per_line = max(words_per_line_list) if words_per_line_list else 0
    
    elapsed = time.time() - start_time
    
    # 输出结果
    print()
    print("="*60)
    print("数据统计结果")
    print("="*60)
    print(f"\n【基本信息】")
    print(f"  总行数: {total_lines:,}")
    print(f"  总单词数: {total_words:,}")
    print(f"  唯一单词数: {unique_words:,}")
    print(f"  唯一性比例: {unique_words/total_words*100:.2f}%")
    
    print(f"\n【每行单词数】")
    print(f"  平均: {avg_words_per_line:.1f}")
    print(f"  最小: {min_words_per_line}")
    print(f"  最大: {max_words_per_line}")
    
    print(f"\n【单词频率分布】")
    print(f"  平均出现次数: {avg_count:.2f}")
    print(f"  最高出现次数: {max_count:,}")
    print(f"  最低出现次数: {min_count:,}")
    print(f"  倾斜度 (最高/平均): {max_count/avg_count:.2f}" if avg_count > 0 else "  倾斜度: N/A")
    
    print(f"\n【数据倾斜分析】")
    print(f"  前20%单词的频率占比: {top_20_ratio*100:.1f}%")
    if top_20_ratio > 0.6:
        print(f"  ⚠️  数据存在显著倾斜（高于60%）")
    elif top_20_ratio > 0.4:
        print(f"  ℹ️  数据存在中等倾斜（40-60%）")
    else:
        print(f"  ✓  数据分布较为均匀（低于40%）")
    
    print(f"\n【出现频率最高的20个单词】")
    for idx, (word, count) in enumerate(top_words, 1):
        percentage = count / total_words * 100
        bar_length = int(percentage * 2)
        bar = '█' * bar_length
        print(f"  {idx:2d}. {word:20s}: {count:>8,} 次 ({percentage:5.2f}%) {bar}")
    
    if bottom_words:
        print(f"\n【出现频率最低的10个单词】")
        for word, count in bottom_words:
            percentage = count / total_words * 100
            print(f"  {word:20s}: {count:>8,} 次 ({percentage:5.2f}%)")
    
    print(f"\n【Combiner效果预估】")
    compression_ratio = total_words / unique_words if unique_words > 0 else 1
    print(f"  Map输出记录数: {total_words:,}")
    print(f"  Combiner后记录数: ~{unique_words:,}")
    print(f"  数据压缩比: {compression_ratio:.2f}:1")
    
    if compression_ratio > 10:
        print(f"  ✓  Combiner预期效果极佳（压缩比>{compression_ratio:.0f}:1）")
    elif compression_ratio > 3:
        print(f"  ✓  Combiner预期效果良好（压缩比={compression_ratio:.1f}:1）")
    elif compression_ratio > 1.5:
        print(f"  ⚠️  Combiner效果一般（压缩比={compression_ratio:.1f}:1）")
    else:
        print(f"  ✗  Combiner几乎无效（压缩比={compression_ratio:.1f}:1）")
    
    print(f"\n【性能指标】")
    print(f"  分析耗时: {elapsed:.2f} 秒")
    print(f"  处理速度: {total_lines/elapsed:.0f} 行/秒")
    print("="*60)

if __name__ == "__main__":
    data_files = [
        "uniform_data.txt",
        "skewed_data.txt",
        "unique_data.txt"
    ]
    
    for data_file in data_files:
        print("\n" * 2)
        print("█" * 60)
        print(f"  分析文件: {data_file}")
        print("█" * 60)
        
        if os.path.exists(data_file):
            analyze_data_distribution(data_file)
        else:
            print(f"\n文件不存在: {data_file}")
            print("请先运行对应的生成脚本")
        
        print("\n")
```

------

## 3. 在Windows上执行数据生成

**检查Python环境**

在PowerShell或CMD中执行：

```powershell
python --version
```

如果没有安装Python，请从 [Python官网](https://www.python.org/downloads/) 下载安装。

**运行生成脚本**

```powershell
cd /code/data-generator

# 生成均匀分布数据
python generate_all_data.py

# 验证数据分布
python validate_data.py
```

------

## 4. 上传数据到虚拟机

**使用rz指令**

```bash
# 在虚拟机上执行
cd /export/data

scp -r code root@192.168.204.132:/export/data/
# 上传文件（在SecureCRT或Xshell中执行rz会弹出文件选择窗口）
rz
# 选择整个文件夹/code
```

# Step 3：代码编写

## 1. 创建Java代码目录并编写代码

### 不带Combiner版本

#### 1. WordCountMapper.java

```java
package com.hadoop.wordcount.without;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import java.io.IOException;

public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    @Override
    protected void map(LongWritable key, Text value, Context context)
            throws IOException, InterruptedException {

        String line = value.toString();
        String[] words = line.split("\\s+");

        for (String w : words) {
            if (w.length() > 0) {
                word.set(w);
                context.write(word, one);
            }
        }
    }
}
```

#### 2. WordCountReducer.java

```java
package com.hadoop.wordcount.without;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
import java.io.IOException;

public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

    private IntWritable result = new IntWritable();

    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context)
            throws IOException, InterruptedException {

        int sum = 0;
        for (IntWritable val : values) {
            sum += val.get();
        }

        result.set(sum);
        context.write(key, result);
    }
}
```

#### 3. WordCountDriver.java

```java
package com.hadoop.wordcount.without;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCountDriver {

    public static void main(String[] args) throws Exception {

        if (args.length != 2) {
            System.err.println("Usage: WordCountDriver <input path> <output path>");
            System.exit(-1);
        }

        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "WordCount Without Combiner");

        job.setJarByClass(WordCountDriver.class);
        job.setMapperClass(WordCountMapper.class);
        job.setReducerClass(WordCountReducer.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        System.out.println("========================================");
        System.out.println("WordCount Without Combiner");
        System.out.println("========================================");
        System.out.println("Input:  " + args[0]);
        System.out.println("Output: " + args[1]);
        System.out.println("Combiner: DISABLED");
        System.out.println("========================================");

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
```

### 带Combiner版本

#### 1. WordCountMapper.java

```java
package com.hadoop.wordcount.with;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import java.io.IOException;

public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    @Override
    protected void map(LongWritable key, Text value, Context context)
            throws IOException, InterruptedException {

        String line = value.toString();
        String[] words = line.split("\\s+");

        for (String w : words) {
            if (w.length() > 0) {
                word.set(w);
                context.write(word, one);
            }
        }
    }
}
```

#### 2. WordCountReducer.java

```java
package com.hadoop.wordcount.with;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
import java.io.IOException;

public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

    private IntWritable result = new IntWritable();

    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context)
            throws IOException, InterruptedException {

        int sum = 0;
        for (IntWritable val : values) {
            sum += val.get();
        }

        result.set(sum);
        context.write(key, result);
    }
}
```

#### 3. WordCountDriver.java

```java
package com.hadoop.wordcount.with;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCountDriver {

    public static void main(String[] args) throws Exception {

        if (args.length != 2) {
            System.err.println("Usage: WordCountDriver <input path> <output path>");
            System.exit(-1);
        }

        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "WordCount With Combiner");

        job.setJarByClass(WordCountDriver.class);
        job.setMapperClass(WordCountMapper.class);
        job.setCombinerClass(WordCountReducer.class);  // 关键：设置Combiner
        job.setReducerClass(WordCountReducer.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        System.out.println("========================================");
        System.out.println("WordCount With Combiner");
        System.out.println("========================================");
        System.out.println("Input:  " + args[0]);
        System.out.println("Output: " + args[1]);
        System.out.println("Combiner: ENABLED");
        System.out.println("========================================");

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
```

## 2. 添加脚本

#### 1. upload_data.sh

```bash
#!/bin/bash
# ============================================================================
# 脚本名称: compile_and_package.sh
# 功能描述: 编译MapReduce源代码并打包成JAR文件
# 注意事项: 需要先配置好Hadoop环境变量
# ============================================================================

# ----------------------------------------------------------------------------
# 1. 全局配置
# ----------------------------------------------------------------------------
PROJECT_DIR=/export/data/code
BUILD_DIR=$PROJECT_DIR/build

# 定义源代码路径
WITHOUT_COMBINER_SRC=$PROJECT_DIR/mapreduce/without-combiner
WITH_COMBINER_SRC=$PROJECT_DIR/mapreduce/with-combiner

# 定义编译输出路径
WITHOUT_COMBINER_BUILD=$BUILD_DIR/without-combiner
WITH_COMBINER_BUILD=$BUILD_DIR/with-combiner

echo "=========================================="
echo "  编译和打包MapReduce代码"
echo "=========================================="
echo "项目目录: $PROJECT_DIR"
echo "编译目录: $BUILD_DIR"
echo ""

# ----------------------------------------------------------------------------
# 2. 清理旧的编译文件
# ----------------------------------------------------------------------------
echo "步骤1: 清理旧的编译文件..."

# 删除旧的build目录
if [ -d "$BUILD_DIR" ]; then
    echo "  删除旧的 $BUILD_DIR"
    rm -rf $BUILD_DIR/*
fi

# 重新创建目录结构
mkdir -p $WITHOUT_COMBINER_BUILD
mkdir -p $WITH_COMBINER_BUILD

echo "✓ 清理完成"

# ----------------------------------------------------------------------------
# 3. 编译不带Combiner的版本
# ----------------------------------------------------------------------------
echo ""
echo "步骤2: 编译 without-combiner 版本..."
echo "  源代码目录: $WITHOUT_COMBINER_SRC"

# 进入源代码目录
cd $WITHOUT_COMBINER_SRC

# 检查源代码文件是否存在
if [ ! -d "./" ]; then
    echo "✗ 错误: 找不到源代码目录!"
    echo "请检查: $WITHOUT_COMBINER_SRC"
    exit 1
fi

echo "  找到源文件:"
ls -la ./*.java

# javac 编译参数说明：
# -classpath $(hadoop classpath): 添加Hadoop的所有依赖库
# -d $WITHOUT_COMBINER_BUILD: 指定编译后的class文件输出目录
# .out/*.java: 编译所有Java源文件
echo ""
echo "  开始编译..."
javac -classpath $(hadoop classpath) \
      -d $WITHOUT_COMBINER_BUILD \
      ./*.java

# 检查编译是否成功
if [ $? -eq 0 ]; then
    echo "  ✓ 编译成功"
    echo "  编译后的class文件:"
    find $WITHOUT_COMBINER_BUILD -name "*.class"
else
    echo "  ✗ 编译失败！"
    echo "  请检查:"
    echo "  1. Java版本是否正确 (java -version)"
    echo "  2. Hadoop环境变量是否配置 (hadoop version)"
    echo "  3. 源代码是否有语法错误"
    exit 1
fi

# 打包成JAR文件
echo ""
echo "  打包 JAR 文件..."
cd $WITHOUT_COMBINER_BUILD

# jar 命令参数说明：
# -c: 创建新的JAR文件
# -v: 显示详细输出（verbose）
# -f: 指定JAR文件名
# ./*.class: 要打包的class文件
jar -cvf wordcount-without-combiner.jar \
    ./*.class > /dev/null 2>&1

if [ -f "wordcount-without-combiner.jar" ]; then
    echo "  ✓ 生成: wordcount-without-combiner.jar"
    ls -lh wordcount-without-combiner.jar
else
    echo "  ✗ JAR打包失败"
    exit 1
fi

# ----------------------------------------------------------------------------
# 4. 编译带Combiner的版本
# ----------------------------------------------------------------------------
echo ""
echo "步骤3: 编译 with-combiner 版本..."
echo "  源代码目录: $WITH_COMBINER_SRC"

cd $WITH_COMBINER_SRC

# 检查源代码文件是否存在
if [ ! -d "." ]; then
    echo "✗ 错误: 找不到源代码目录!"
    echo "请检查: $WITH_COMBINER_SRC/./"
    exit 1
fi

echo "  找到源文件:"
ls -la ./*.java

echo ""
echo "  开始编译..."
javac -classpath $(hadoop classpath) \
      -d $WITH_COMBINER_BUILD \
      ./*.java

if [ $? -eq 0 ]; then
    echo "  ✓ 编译成功"
    echo "  编译后的class文件:"
    find $WITH_COMBINER_BUILD -name "*.class"
else
    echo "  ✗ 编译失败！"
    exit 1
fi

# 打包成JAR文件
echo ""
echo "  打包 JAR 文件..."
cd $WITH_COMBINER_BUILD

jar -cvf wordcount-with-combiner.jar \
    ./*.class > /dev/null 2>&1

if [ -f "wordcount-with-combiner.jar" ]; then
    echo "  ✓ 生成: wordcount-with-combiner.jar"
    ls -lh wordcount-with-combiner.jar
else
    echo "  ✗ JAR打包失败"
    exit 1
fi

# ----------------------------------------------------------------------------
# 5. 验证JAR文件内容
# ----------------------------------------------------------------------------
echo ""
echo "步骤4: 验证JAR文件内容..."

echo ""
echo "--- without-combiner JAR 内容 ---"
jar -tf $WITHOUT_COMBINER_BUILD/wordcount-without-combiner.jar | grep -E "\.class$"

echo ""
echo "--- with-combiner JAR 内容 ---"
jar -tf $WITH_COMBINER_BUILD/wordcount-with-combiner.jar | grep -E "\.class$"

# ----------------------------------------------------------------------------
# 6. 完成提示
# ----------------------------------------------------------------------------
echo ""
echo "=========================================="
echo "  ✓ 编译打包完成!"
echo "=========================================="
echo ""
echo "生成的JAR文件:"
echo "1. Without Combiner:"
ls -lh $WITHOUT_COMBINER_BUILD/*.jar
echo ""
echo "2. With Combiner:"
ls -lh $WITH_COMBINER_BUILD/*.jar

echo ""
echo "下一步操作:"
echo "  运行实验: ./run_experiments.sh"
echo "=========================================="
```

#### 2. compile_and_package.sh

```bash
#!/bin/bash
# ============================================================================
# 脚本名称: compile_and_package.sh
# 功能描述: 编译MapReduce源代码并打包成JAR文件
# 注意事项: 需要先配置好Hadoop环境变量
# ============================================================================

# ----------------------------------------------------------------------------
# 1. 全局配置
# ----------------------------------------------------------------------------
PROJECT_DIR=/export/data/code
BUILD_DIR=$PROJECT_DIR/build

# 定义源代码路径
WITHOUT_COMBINER_SRC=$PROJECT_DIR/mapreduce/without-combiner
WITH_COMBINER_SRC=$PROJECT_DIR/mapreduce/with-combiner

# 定义编译输出路径
WITHOUT_COMBINER_BUILD=$BUILD_DIR/without-combiner
WITH_COMBINER_BUILD=$BUILD_DIR/with-combiner

echo "=========================================="
echo "  编译和打包MapReduce代码"
echo "=========================================="
echo "项目目录: $PROJECT_DIR"
echo "编译目录: $BUILD_DIR"
echo ""

# ----------------------------------------------------------------------------
# 2. 清理旧的编译文件
# ----------------------------------------------------------------------------
echo "步骤1: 清理旧的编译文件..."

# 删除旧的build目录
if [ -d "$BUILD_DIR" ]; then
    echo "  删除旧的 $BUILD_DIR"
    rm -rf $BUILD_DIR/*
fi

# 重新创建目录结构
mkdir -p $WITHOUT_COMBINER_BUILD
mkdir -p $WITH_COMBINER_BUILD

echo "✓ 清理完成"

# ----------------------------------------------------------------------------
# 3. 编译不带Combiner的版本
# ----------------------------------------------------------------------------
echo ""
echo "步骤2: 编译 without-combiner 版本..."
echo "  源代码目录: $WITHOUT_COMBINER_SRC"

# 进入源代码目录
cd $WITHOUT_COMBINER_SRC

# 检查源代码文件是否存在
if [ ! -d "com/hadoop/wordcount/without" ]; then
    echo "✗ 错误: 找不到源代码目录!"
    echo "请检查: $WITHOUT_COMBINER_SRC/com/hadoop/wordcount/without/"
    exit 1
fi

echo "  找到源文件:"
ls -la com/hadoop/wordcount/without/*.java

# javac 编译参数说明：
# -classpath $(hadoop classpath): 添加Hadoop的所有依赖库
# -d $WITHOUT_COMBINER_BUILD: 指定编译后的class文件输出目录
# com/hadoop/wordcount/without/*.java: 编译所有Java源文件
echo ""
echo "  开始编译..."
javac -classpath $(hadoop classpath) \
      -d $WITHOUT_COMBINER_BUILD \
      com/hadoop/wordcount/without/*.java

# 检查编译是否成功
if [ $? -eq 0 ]; then
    echo "  ✓ 编译成功"
    echo "  编译后的class文件:"
    find $WITHOUT_COMBINER_BUILD -name "*.class"
else
    echo "  ✗ 编译失败！"
    echo "  请检查:"
    echo "  1. Java版本是否正确 (java -version)"
    echo "  2. Hadoop环境变量是否配置 (hadoop version)"
    echo "  3. 源代码是否有语法错误"
    exit 1
fi

# 打包成JAR文件
echo ""
echo "  打包 JAR 文件..."
cd $WITHOUT_COMBINER_BUILD

# jar 命令参数说明：
# -c: 创建新的JAR文件
# -v: 显示详细输出（verbose）
# -f: 指定JAR文件名
# com/hadoop/wordcount/without/*.class: 要打包的class文件
jar -cvf wordcount-without-combiner.jar \
    com/hadoop/wordcount/without/*.class > /dev/null 2>&1

if [ -f "wordcount-without-combiner.jar" ]; then
    echo "  ✓ 生成: wordcount-without-combiner.jar"
    ls -lh wordcount-without-combiner.jar
else
    echo "  ✗ JAR打包失败"
    exit 1
fi

# ----------------------------------------------------------------------------
# 4. 编译带Combiner的版本
# ----------------------------------------------------------------------------
echo ""
echo "步骤3: 编译 with-combiner 版本..."
echo "  源代码目录: $WITH_COMBINER_SRC"

cd $WITH_COMBINER_SRC

# 检查源代码文件是否存在
if [ ! -d "com/hadoop/wordcount/with" ]; then
    echo "✗ 错误: 找不到源代码目录!"
    echo "请检查: $WITH_COMBINER_SRC/com/hadoop/wordcount/with/"
    exit 1
fi

echo "  找到源文件:"
ls -la com/hadoop/wordcount/with/*.java

echo ""
echo "  开始编译..."
javac -classpath $(hadoop classpath) \
      -d $WITH_COMBINER_BUILD \
      com/hadoop/wordcount/with/*.java

if [ $? -eq 0 ]; then
    echo "  ✓ 编译成功"
    echo "  编译后的class文件:"
    find $WITH_COMBINER_BUILD -name "*.class"
else
    echo "  ✗ 编译失败！"
    exit 1
fi

# 打包成JAR文件
echo ""
echo "  打包 JAR 文件..."
cd $WITH_COMBINER_BUILD

jar -cvf wordcount-with-combiner.jar \
    com/hadoop/wordcount/with/*.class > /dev/null 2>&1

if [ -f "wordcount-with-combiner.jar" ]; then
    echo "  ✓ 生成: wordcount-with-combiner.jar"
    ls -lh wordcount-with-combiner.jar
else
    echo "  ✗ JAR打包失败"
    exit 1
fi

# ----------------------------------------------------------------------------
# 5. 验证JAR文件内容
# ----------------------------------------------------------------------------
echo ""
echo "步骤4: 验证JAR文件内容..."

echo ""
echo "--- without-combiner JAR 内容 ---"
jar -tf $WITHOUT_COMBINER_BUILD/wordcount-without-combiner.jar | grep -E "\.class$"

echo ""
echo "--- with-combiner JAR 内容 ---"
jar -tf $WITH_COMBINER_BUILD/wordcount-with-combiner.jar | grep -E "\.class$"

# ----------------------------------------------------------------------------
# 6. 完成提示
# ----------------------------------------------------------------------------
echo ""
echo "=========================================="
echo "  ✓ 编译打包完成!"
echo "=========================================="
echo ""
echo "生成的JAR文件:"
echo "1. Without Combiner:"
ls -lh $WITHOUT_COMBINER_BUILD/*.jar
echo ""
echo "2. With Combiner:"
ls -lh $WITH_COMBINER_BUILD/*.jar

echo ""
echo "下一步操作:"
echo "  运行实验: ./run_experiments.sh"
echo "=========================================="
```

#### 3. run_experiments.sh

```bash
#!/bin/bash
# ============================================================================
# 脚本名称: run_experiments.sh
# 功能描述: 运行所有MapReduce实验并记录性能数据
# 实验设计:
#   - 3种数据集 (uniform, skewed, unique)
#   - 2种配置 (with/without Combiner)
#   - 共6个实验
# ============================================================================

# ----------------------------------------------------------------------------
# 1. 全局配置 - 根据当前用户调整
# ----------------------------------------------------------------------------
PROJECT_DIR=/export/data/code
BUILD_DIR=$PROJECT_DIR/build
RESULTS_DIR=$PROJECT_DIR/results

# 如果是 root 用户:
HDFS_INPUT_DIR=/user/root/input
HDFS_OUTPUT_DIR=/user/root/output

# 如果是 hadoop 用户（注释掉上面的，取消注释下面的）:
# HDFS_INPUT_DIR=/user/hadoop/input
# HDFS_OUTPUT_DIR=/user/hadoop/output

echo "=========================================="
echo "  MapReduce Combiner 性能对比实验"
echo "=========================================="
echo "当前用户: $(whoami)"
echo "项目目录: $PROJECT_DIR"
echo "HDFS输入: $HDFS_INPUT_DIR"
echo "HDFS输出: $HDFS_OUTPUT_DIR"
echo "结果目录: $RESULTS_DIR"
echo ""

# ----------------------------------------------------------------------------
# 2. 创建结果目录
# ----------------------------------------------------------------------------
echo "准备实验环境..."

# 创建本地结果目录（如果不存在）
if [ ! -d "$RESULTS_DIR" ]; then
    mkdir -p $RESULTS_DIR
    echo "✓ 创建结果目录: $RESULTS_DIR"
fi

# ----------------------------------------------------------------------------
# 3. 清理旧的HDFS输出
# ----------------------------------------------------------------------------
echo "清理旧的HDFS输出目录..."

# 删除所有旧的输出目录
# -r: 递归删除
# 2>/dev/null: 如果目录不存在，不显示错误
hdfs dfs -rm -r $HDFS_OUTPUT_DIR/* 2>/dev/null

if [ $? -eq 0 ]; then
    echo "✓ 清理完成"
else
    echo "⚠️  警告: 清理可能失败（如果是首次运行可以忽略）"
fi

# ----------------------------------------------------------------------------
# 4. 验证JAR文件存在
# ----------------------------------------------------------------------------
echo ""
echo "检查JAR文件..."

WITHOUT_JAR=$BUILD_DIR/without-combiner/wordcount-without-combiner.jar
WITH_JAR=$BUILD_DIR/with-combiner/wordcount-with-combiner.jar

if [ ! -f "$WITHOUT_JAR" ]; then
    echo "✗ 错误: 找不到 $WITHOUT_JAR"
    echo "请先运行: ./compile_and_package.sh"
    exit 1
fi

if [ ! -f "$WITH_JAR" ]; then
    echo "✗ 错误: 找不到 $WITH_JAR"
    echo "请先运行: ./compile_and_package.sh"
    exit 1
fi

echo "✓ JAR文件检查通过"

# ----------------------------------------------------------------------------
# 5. 定义实验配置
# ----------------------------------------------------------------------------

# 数据集文件名数组
declare -a datasets=("uniform_data.txt" "skewed_data.txt" "unique_data.txt")

# 数据集简称数组（用于命名输出目录和日志文件）
declare -a dataset_names=("uniform" "skewed" "unique")

# 数据集描述（用于显示）
declare -a dataset_descriptions=(
    "均匀分布数据 - 词频相近"
    "数据倾斜 - 热点词占主导"
    "高唯一性 - 几乎无重复"
)

# ----------------------------------------------------------------------------
# 6. 运行实验循环
# ----------------------------------------------------------------------------
echo ""
echo "=========================================="
echo "  开始实验 (共 6 个)"
echo "=========================================="

# 实验计数器
experiment_num=0
total_experiments=6

# 遍历每个数据集
for i in "${!datasets[@]}"; do
    # 获取当前数据集的各种信息
    dataset="${datasets[$i]}"              # 文件名: uniform_data.txt
    name="${dataset_names[$i]}"            # 简称: uniform
    description="${dataset_descriptions[$i]}"  # 描述

    echo ""
    echo "=========================================="
    echo "  数据集 $((i+1))/3: $dataset"
    echo "  说明: $description"
    echo "=========================================="

    # ------------------------------------------------------------------------
    # 实验A: 不带Combiner
    # ------------------------------------------------------------------------
    experiment_num=$((experiment_num + 1))
    echo ""
    echo ">>> 实验 $experiment_num/$total_experiments: ${name}_without_combiner"
    echo "开始时间: $(date '+%Y-%m-%d %H:%M:%S')"

    # HDFS输出目录（每个实验独立的输出目录）
    output_dir="$HDFS_OUTPUT_DIR/${name}_without_combiner"

    # 本地日志文件
    log_file="$RESULTS_DIR/${name}_without_combiner.log"

    # 记录开始时间戳（秒）
    start_time=$(date +%s)

    # 运行MapReduce任务
    # hadoop jar: 运行JAR包中的MapReduce程序
    # 参数1: JAR文件路径
    # 参数2: 主类名（包含main方法）
    # 参数3: HDFS输入路径
    # 参数4: HDFS输出路径
    # > $log_file 2>&1: 将标准输出和错误输出都重定向到日志文件
    hadoop jar $WITHOUT_JAR \
        com.hadoop.wordcount.without.WordCountDriver \
        $HDFS_INPUT_DIR/$dataset \
        $output_dir \
        > $log_file 2>&1

    # 记录结束时间戳
    end_time=$(date +%s)

    # 计算执行时间（秒）
    duration=$((end_time - start_time))

    # 检查任务是否成功
    # $?: 上一个命令的退出状态码，0表示成功
    if [ $? -eq 0 ]; then
        echo "   ✓ 成功! 耗时: ${duration}秒"
        # 将执行时间追加到日志文件末尾，方便后续分析
        echo "EXECUTION_TIME=${duration}" >> $log_file
        echo "   日志: $log_file"
    else
        echo "   ✗ 失败! 查看日志: $log_file"
        echo "   常见原因:"
        echo "   1. HDFS输入文件不存在"
        echo "   2. 输出目录已存在"
        echo "   3. MapReduce配置问题"
    fi

    # 等待3秒，避免资源冲突
    sleep 3

    # ------------------------------------------------------------------------
    # 实验B: 带Combiner
    # ------------------------------------------------------------------------
    experiment_num=$((experiment_num + 1))
    echo ""
    echo ">>> 实验 $experiment_num/$total_experiments: ${name}_with_combiner"
    echo "开始时间: $(date '+%Y-%m-%d %H:%M:%S')"

    output_dir="$HDFS_OUTPUT_DIR/${name}_with_combiner"
    log_file="$RESULTS_DIR/${name}_with_combiner.log"

    start_time=$(date +%s)

    hadoop jar $WITH_JAR \
        com.hadoop.wordcount.with.WordCountDriver \
        $HDFS_INPUT_DIR/$dataset \
        $output_dir \
        > $log_file 2>&1

    end_time=$(date +%s)
    duration=$((end_time - start_time))

    if [ $? -eq 0 ]; then
        echo "   ✓ 成功! 耗时: ${duration}秒"
        echo "EXECUTION_TIME=${duration}" >> $log_file
        echo "   日志: $log_file"
    else
        echo "   ✗ 失败! 查看日志: $log_file"
    fi

    # 等待3秒
    sleep 3
done

# ----------------------------------------------------------------------------
# 7. 实验完成总结
# ----------------------------------------------------------------------------
echo ""
echo "=========================================="
echo "  ✓ 所有实验完成!"
echo "=========================================="
echo ""
echo "实验结果汇总:"
echo "  总实验数: $total_experiments"
echo "  成功实验: $(ls $RESULTS_DIR/*.log 2>/dev/null | wc -l)"
echo ""
echo "日志文件位置: $RESULTS_DIR"
ls -lh $RESULTS_DIR/*.log 2>/dev/null

echo ""
echo "HDFS输出位置:"
hdfs dfs -ls $HDFS_OUTPUT_DIR/

echo ""
echo "=========================================="
echo "下一步操作:"
echo "  分析结果: ./analyze_results.sh"
echo "=========================================="
```

#### 4. analyze_results.sh

```bash
#!/bin/bash
# ============================================================================
# 脚本名称: analyze_results.sh
# 功能描述: 分析MapReduce实验结果，生成性能对比报告
# 输入: results/*.log 日志文件
# 输出: performance_metrics.csv + 控制台分析报告
# ============================================================================

# ----------------------------------------------------------------------------
# 1. 全局配置
# ----------------------------------------------------------------------------
PROJECT_DIR=/export/data/code
RESULTS_DIR=$PROJECT_DIR/results

echo "=========================================="
echo "  MapReduce Combiner 实验结果分析"
echo "=========================================="
echo "结果目录: $RESULTS_DIR"
echo ""

# ----------------------------------------------------------------------------
# 2. 检查日志文件是否存在
# ----------------------------------------------------------------------------
echo "检查实验日志文件..."

if [ ! -d "$RESULTS_DIR" ]; then
    echo "✗ 错误: 结果目录不存在 - $RESULTS_DIR"
    echo "请先运行: ./run_experiments.sh"
    exit 1
fi

# 统计日志文件数量
log_count=$(ls $RESULTS_DIR/*.log 2>/dev/null | wc -l)

if [ $log_count -eq 0 ]; then
    echo "✗ 错误: 找不到任何日志文件 (.log)"
    echo "请先运行: ./run_experiments.sh"
    exit 1
elif [ $log_count -lt 6 ]; then
    echo "⚠️  警告: 只找到 $log_count 个日志文件，应该有6个"
    echo "部分实验可能失败，分析将继续..."
else
    echo "✓ 找到 $log_count 个日志文件"
fi

# 列出所有日志文件
echo ""
echo "日志文件列表:"
ls -lh $RESULTS_DIR/*.log

# ----------------------------------------------------------------------------
# 3. 创建CSV结果文件并写入表头
# ----------------------------------------------------------------------------
csv_file="$RESULTS_DIR/performance_metrics.csv"

echo ""
echo "正在生成CSV结果文件: $csv_file"

# CSV表头定义
# 各列含义:
# - 实验名称: uniform_with_combiner等
# - 执行时间: 任务总耗时（秒）
# - Map输入记录: Map阶段读取的记录数
# - Map输出记录: Map阶段输出的键值对数
# - Combine输入记录: Combiner接收的记录数
# - Combine输出记录: Combiner输出的记录数
# - Reduce输入组: Reducer接收的key组数
# - Reduce输出记录: 最终输出的记录数
# - 数据压缩比: Map输出/Combiner输出，衡量Combiner效果
cat > $csv_file << 'EOF'
实验名称,执行时间(秒),Map输入记录,Map输出记录,Combine输入记录,Combine输出记录,Reduce输入组,Reduce输出记录,数据压缩比
EOF

echo "✓ CSV表头创建完成"

# ----------------------------------------------------------------------------
# 4. 提取每个实验的性能指标
# ----------------------------------------------------------------------------
echo ""
echo "正在从日志中提取性能指标..."
echo ""

# 遍历所有日志文件
for log in $RESULTS_DIR/*.log; do
    # 获取实验名称（去掉路径和.log后缀）
    exp_name=$(basename $log .log)

    echo "  处理: $exp_name"

    # ----------------------------------------------------------------------
    # 从日志中提取各项指标
    # ----------------------------------------------------------------------

    # 执行时间（我们在运行脚本中手动添加的）
    exec_time=$(grep "EXECUTION_TIME" $log | cut -d'=' -f2)

    # Map输入记录数
    # grep: 查找包含"Map input records"的行
    # tail -1: 取最后一行（防止有多次重试）
    # awk '{print $NF}': 打印最后一个字段（数字）
    map_input=$(grep "Map input records" $log | tail -1 | awk '{print $NF}')

    # Map输出记录数
    map_output=$(grep "Map output records" $log | tail -1 | awk '{print $NF}')

    # Combiner输入记录数（只有with-combiner版本有这个指标）
    combine_input=$(grep "Combine input records" $log | tail -1 | awk '{print $NF}')

    # Combiner输出记录数
    combine_output=$(grep "Combine output records" $log | tail -1 | awk '{print $NF}')

    # Reducer输入组数
    reduce_input=$(grep "Reduce input groups" $log | tail -1 | awk '{print $NF}')

    # Reducer输出记录数
    reduce_output=$(grep "Reduce output records" $log | tail -1 | awk '{print $NF}')

    # ----------------------------------------------------------------------
    # 计算数据压缩比
    # ----------------------------------------------------------------------

    # 如果没有Combiner（without版本），设置为N/A
    if [ -z "$combine_input" ] || [ "$combine_input" == "0" ]; then
        combine_input="N/A"
        combine_output="N/A"
        compression_ratio="1.0"  # 无压缩
    else
        # 计算压缩比 = Map输出记录数 / Combiner输出记录数
        # 例如: Map输出1000条，Combiner输出100条，压缩比=10
        # 压缩比越大，说明Combiner效果越好
        if [ "$combine_output" != "0" ]; then
            # bc: 命令行计算器，scale=2表示保留2位小数
            compression_ratio=$(echo "scale=2; $map_output / $combine_output" | bc)
        else
            compression_ratio="N/A"
        fi
    fi

    # ----------------------------------------------------------------------
    # 将数据写入CSV
    # ----------------------------------------------------------------------
    echo "$exp_name,$exec_time,$map_input,$map_output,$combine_input,$combine_output,$reduce_input,$reduce_output,$compression_ratio" >> $csv_file
done

echo ""
echo "✓ 性能指标提取完成"

# ----------------------------------------------------------------------------
# 5. 显示CSV表格
# ----------------------------------------------------------------------------
echo ""
echo "=========================================="
echo "  性能指标总览"
echo "=========================================="

# column命令：将CSV格式化为对齐的表格
# -t: 自动对齐列
# -s',': 使用逗号作为分隔符
column -t -s',' $csv_file

# ----------------------------------------------------------------------------
# 6. 详细分析 - Uniform数据集
# ----------------------------------------------------------------------------
echo ""
echo "=========================================="
echo "  详细分析报告"
echo "=========================================="

echo ""
echo "【数据集1: Uniform - 均匀分布】"
echo "特征: 所有单词出现频率相近，数据分布均匀"
echo "-------------------------------------------"

# 从CSV中提取uniform的两个实验数据
uniform_without=$(grep "uniform_without" $csv_file | cut -d',' -f2)
uniform_with=$(grep "uniform_with" $csv_file | cut -d',' -f2)

# 检查数据是否存在
if [ -n "$uniform_without" ] && [ -n "$uniform_with" ]; then
    # 计算性能提升百分比
    # 公式: (不带Combiner耗时 - 带Combiner耗时) / 不带Combiner耗时 * 100
    improvement=$(echo "scale=2; ($uniform_without - $uniform_with) / $uniform_without * 100" | bc)

    echo "  Without Combiner: ${uniform_without}秒"
    echo "  With Combiner:    ${uniform_with}秒"
    echo "  性能提升:         ${improvement}%"

    # 提取压缩比
    compression=$(grep "uniform_with" $csv_file | cut -d',' -f9)
    echo "  数据压缩比:       ${compression}:1"

    # 分析结论
    if (( $(echo "$improvement > 20" | bc -l) )); then
        echo "  ✓ 结论: Combiner显著提升性能，减少了网络传输"
    else
        echo "  ⚠️  结论: Combiner提升有限"
    fi
else
    echo "  ✗ 警告: 缺少实验数据"
fi

# ----------------------------------------------------------------------------
# 7. 详细分析 - Skewed数据集
# ----------------------------------------------------------------------------
echo ""
echo "【数据集2: Skewed - 数据倾斜】"
echo "特征: 少数热点词(如'the','of')占据大量记录"
echo "-------------------------------------------"

skewed_without=$(grep "skewed_without" $csv_file | cut -d',' -f2)
skewed_with=$(grep "skewed_with" $csv_file | cut -d',' -f2)

if [ -n "$skewed_without" ] && [ -n "$skewed_with" ]; then
    improvement=$(echo "scale=2; ($skewed_without - $skewed_with) / $skewed_without * 100" | bc)

    echo "  Without Combiner: ${skewed_without}秒"
    echo "  With Combiner:    ${skewed_with}秒"
    echo "  性能提升:         ${improvement}%"

    compression=$(grep "skewed_with" $csv_file | cut -d',' -f9)
    echo "  数据压缩比:       ${compression}:1"

    if (( $(echo "$improvement > 30" | bc -l) )); then
        echo "  ✓ 结论: Combiner对热点词合并效果极佳"
    else
        echo "  ⚠️  结论: 效果低于预期"
    fi
else
    echo "  ✗ 警告: 缺少实验数据"
fi

# ----------------------------------------------------------------------------
# 8. 详细分析 - Unique数据集
# ----------------------------------------------------------------------------
echo ""
echo "【数据集3: Unique - 高唯一性】"
echo "特征: 几乎每个单词都唯一，重复度极低"
echo "-------------------------------------------"

unique_without=$(grep "unique_without" $csv_file | cut -d',' -f2)
unique_with=$(grep "unique_with" $csv_file | cut -d',' -f2)

if [ -n "$unique_without" ] && [ -n "$unique_with" ]; then
    # 对于unique数据，Combiner可能反而增加耗时
    diff=$(echo "scale=2; $unique_with - $unique_without" | bc)

    echo "  Without Combiner: ${unique_without}秒"
    echo "  With Combiner:    ${unique_with}秒"
    echo "  时间差异:         ${diff}秒"

    compression=$(grep "unique_with" $csv_file | cut -d',' -f9)
    echo "  数据压缩比:       ${compression}:1"

    # 分析Combiner的负面影响
    if (( $(echo "$compression < 1.5" | bc -l) )); then
        echo "  ⚠️  警告: 压缩比接近1，Combiner几乎无效"
    fi

    if (( $(echo "$diff > 0" | bc -l) )); then
        echo "  ⚠️  警告: Combiner反而增加了 ${diff}秒"
        echo "     原因: 数据无重复，Combiner额外处理成本 > 收益"
    fi

    echo "  ✓ 结论: 高唯一性数据不适合使用Combiner"
else
    echo "  ✗ 警告: 缺少实验数据"
fi

# ----------------------------------------------------------------------------
# 9. 总结和建议
# ----------------------------------------------------------------------------
echo ""
echo "=========================================="
echo "  实验结论与建议"
echo "=========================================="
echo ""
echo "【核心发现】"
echo "1. 均匀分布数据(Uniform):"
echo "   - Combiner显著减少网络传输量"
echo "   - 性能提升明显"
echo ""
echo "2. 数据倾斜场景(Skewed):"
echo "   - Combiner对热点词的合并效果最佳"
echo "   - 大幅减少Reducer负载"
echo ""
echo "3. 高唯一性数据(Unique):"
echo "   - Combiner几乎无效"
echo "   - 额外处理成本可能导致性能下降"
echo ""
echo "【实践建议】"
echo "✓ 使用Combiner的场景:"
echo "  - 数据重复度高 (>30%)"
echo "  - 存在热点key"
echo "  - 网络带宽是瓶颈"
echo ""
echo "✗ 避免使用Combiner的场景:"
echo "  - 数据唯一性高 (>90%)"
echo "  - 数据量很小"
echo "  - Combiner逻辑复杂耗时"
echo ""
echo "【性能调优建议】"
echo "1. 先分析数据特征 (唯一性、重复度)"
echo "2. 小规模测试对比性能"
echo "3. 监控网络传输和CPU开销"
echo "4. 根据实际场景动态调整"
echo ""
echo "=========================================="
echo "  分析完成!"
echo "=========================================="
echo ""
echo "结果文件: $csv_file"
echo "日志目录: $RESULTS_DIR"
echo "=========================================="
```

## 3. 将文件传至虚拟机

```bash
#powershell上进行
scp -r code root@192.168.204.132:/export/data/
```

## 4. 添加执行权限并运行

```bash
cd /export/data/code/scripts

# 添加执行权限
chmod +x upload_data.sh
chmod +x compile_and_package.sh
chmod +x run_experiments.sh
chmod +x analyze_results.sh

# 按顺序执行
./upload_data.sh              # 上传数据到HDFS
./compile_and_package.sh      # 编译打包Java代码
./run_experiments.sh          # 运行所有实验
./analyze_results.sh          # 分析结果
```

## 5. 下载结果到Windows

实验完成后，把results目录下载回Windows：

~~~powershell
# 在Windows PowerShell执行
cd D:/Minjie/Desktop/mapreduce-combiner-analysis
scp -r root@192.168.204.132:/export/data/code/results ./code/
```

---

## 最终目录结构


├─code
│  ├─cluster-config
│  ├─data
│  │  │  skewed_data.txt
│  │  │  uniform_data.txt
│  │  │  unique_data.txt
│  │  │
│  │  └─data-generator
│  │          generate_all_data.py
│  │          validate_data.py
│  │
│  ├─mapreduce
│  │  ├─with-combiner
│  │  │  └─com
│  │  │      └─hadoop
│  │  │          └─wordcount
│  │  │              └─with
│  │  │                      WordCountDriver.java
│  │  │                      WordCountMapper.java
│  │  │                      WordCountReducer.java
│  │  │
│  │  └─without-combiner
│  │      └─com
│  │          └─hadoop
│  │              └─wordcount
│  │                  └─without
│  │                          WordCountDriver.java
│  │                          WordCountMapper.java
│  │                          WordCountReducer.java
│  │
│  ├─results
│  │      performance_metrics.csv
│  │      skewed_without_combiner.log
│  │      skewed_with_combiner.log
│  │      uniform_without_combiner.log
│  │      uniform_with_combiner.log
│  │      unique_without_combiner.log
│  │      unique_with_combiner.log
│  │
│  └─scripts
│          analyze_results.sh
│          compile_and_package.sh
│          run_experiments.sh
│          upload_data.sh
~~~

####  运行结果

![image-20251126013933483](images/image-20251126013933483.png)

![image-20251126014154273](images/image-20251126014154273.png)

---

# STEP 4: 实验结果与深度分析
在将results下载回Windows后，对结果进行进一步分析，使用Python可视化方法直观呈现出Combiner在MapReduce执行过程中的作用与效果

本实验围绕三类数据分布（Skewed、Uniform、Unique），分别在 **启用 Combiner** 和 **未启用 Combiner** 的情况下进行对比测试。我们从运行时间、Shuffle 数据量、磁盘溢写以及 CPU 资源消耗多个角度进行了评估。
### 1. 核心性能指标汇总

基于集群日志（Log）与执行记录（CSV）的深度解析，我们提取了以下关键性能指标。其中 **并行度 (Average Parallelism)** 计算公式为 `总CPU时间 / 实际执行时间`，反映了集群资源的利用效率。

| 实验组 (Experiment) | 实际执行时间 (Wall Time) | Map CPU时间 | Reduce CPU时间 | **Shuffle 数据量** | **并行度 (Avg Cores)** | 性能评价     |
| :------------------ | :----------------------- | :---------- | :------------- | :----------------- | :--------------------- | :----------- |
| **Skewed (With)**   | **100s**                 | 366.9s      | **5.7s**       | **1.0 MB**         | 3.73                   | **极快**     |
| Skewed (Without)    | 732s                     | 1555.6s     | 446.6s         | 970 MB             | 2.74                   | 极慢         |
| **Uniform (With)**  | **122s**                 | 562.9s      | 60.2s          | **0.8 MB**         | **5.11**               | **均衡高效** |
| Uniform (Without)   | 224s                     | 716.9s      | 86.3s          | 850 MB             | 3.59                   | 一般         |
| **Unique (With)**   | **212s**                 | **727.1s**  | 75.7s          | 1000 MB            | 3.79                   | **负优化**   |
| Unique (Without)    | 189s                     | 562.4s      | 119.1s         | 1000 MB            | 3.61                   | 正常         |

### 2. 综合性能分析图表

下图展示了 Combiner 在不同场景下对 CPU 负载、网络 I/O、磁盘 I/O 的综合影响：

![four_plots_final.png](images/four_plots_final.png)

**图表说明：**

1.  **左上 (Execution Time)**: 作业执行时间对比 - 直观展示Combiner对整体运行速度的提升（或拖慢）。
2.  **右上 (Shuffle Data Volume (Bytes))**: 展示Map输出后，真正进入Shuffle（网络传输/Reduce端）的数据记录数。纵坐标使用对数坐标以显示巨大差异。
3.  **左下 (Spilled Records)**: 溢写记录数（磁盘I/O）
4.  **右下 (CPU Time Breakdown: Map vs Reduce)**: 展示各种方法Map和Reducde具体花费的时间

### 3. 具体分析：
**图1**
- 在 **Skewed** 数据分布中，Combiner优化效果极佳，显著降低整体运行时间。  
- 在 **Unique** 数据集下，Combiner反而会增加整体的运行时间,带来负优化。 

**图2**
- 对于 **Skewed** 和**Uniform**数据集,Combiner 显著减少了 Shuffle 阶段的传输数据量。
- 对于 **Unique** 分布（所有 key 都不同），Combiner 几乎无法减少任何记录，Shuffle 大小变化不大。


**图3** 
- 启用 Combiner 后，Map 端输出记录减少，大幅降低溢写次数。  
- “Skewed + Without Combiner” 是溢写最严重的场景，因为重复键数量极大，未提前合并会导致频繁磁盘写入。


**图4**
- Map CPU 时间在所有测试中占主导地位，但在 **Skewed 数据集 + Without Combiner** 的情况下Reduce CPU 时间明显升高，说明 Reducer 的负载显著增加。  
- 启用 Combiner 有效将部分 Reduce 的工作提前在 Map 阶段完成，从而减少 Reduce CPU 消耗。 尤其是在键分布高度倾斜（Skewed）的场景中，Combiner 的 CPU 节省效果最为显著。
- 对于**Unique**数据集，使用Combiner技术时Map阶段所用时间反而增加


### 4. 深度探究：为什么 Unique 场景变慢了？

实验数据显示，在 **Unique (唯一值)** 数据集下，开启 Combiner 反而导致作业时间增加了 **12%** (189s -> 212s)。

* **无效计算**: 日志显示 `Combine Input Records` = `Combine Output Records`。由于 Key 互不相同，Combiner 无法合并任何数据。
* **CPU 惩罚**: 尽管没有 I/O 收益，Map 阶段依然执行了完整的序列化、反序列化和 Combiner 函数调用。
* **资源浪费**: Map 端 CPU 时间从 **562s 激增至 727s** (+29%)，这种纯粹的计算资源浪费导致了整体性能的退化。

---

##  结论 (Conclusions)

本实验通过对比在 **倾斜 (Skewed)**、**均匀 (Uniform)** 和 **唯一 (Unique)** 三种数据分布下开启与关闭 Combiner 的各项性能指标，深入探究了 Combiner 组件对 MapReduce 作业性能的影响机制。研究主要发现如下：

### 1. I/O 优化是性能提升的决定性因素 (Filter Early Principle)
实验数据强有力地证明了 **"Filter Early" (尽早过滤)** 策略的有效性。Combiner 不仅节省了网络带宽，更实现了全链路的 I/O 卸载：
* **网络 I/O**: 在存在重复 Key 的场景（Skewed & Uniform）下，Shuffle 阶段传输的数据量呈现**对数级下降**（从 ~970 MB 骤降至 ~1 MB），数据压缩比高达 **99.9%**。
* **磁盘 I/O**: Map 端的本地磁盘溢写记录数（Spilled Records）同步减少了 **99%** 以上，极大地延长了磁盘寿命并降低了 I/O 等待时间。

### 2. 对“数据倾斜”场景具有特效 (Critical for Skewed Data)
Combiner 在“数据倾斜”的场景中收益最大，实现了 **7倍的性能加速** (100s vs 732s)。
* **消除长尾效应**: 在未开启 Combiner 时，少数热点 Key 导致个别 Reducer 负载过重，拖慢了整体进度。开启 Combiner 后，热点数据在 Map 端被预聚合，有效地实现了负载均衡。
* **提升并行度**: 集群的平均并行度从 **2.74** 提升至 **3.73**，证明 Combiner 有效减少了资源闲置，提升了集群的整体吞吐量。

### 3. 适用性边界与负优化风险 (The Trade-off)
实验揭示了 Combiner **并非在所有场景下都有效**，其引入了计算与 I/O 的权衡（Trade-off）：
* **负优化**: 在 Key 唯一性极高（Unique）的场景下，Combiner 导致作业总执行时间增加了 **12%** (212s vs 189s)，且 CPU 资源消耗增加了 **29%**。
* **原因**: 当 `Combine Input Records` == `Combine Output Records` 时，Combiner 无法合并任何数据，但 MapReduce 框架依然需要支付**序列化、反序列化和函数调用**的 CPU 成本。这种纯粹的计算开销在没有 I/O 收益的情况下，导致了性能退化。



## 成员分工

| 姓名   | 学号        | 贡献占比 | 具体工作内容                                        |
| :----- | :---------- | :------- | :-------------------------------------------------- |
| 钟民杰 | 51285903099 | 27%      | 搭建Hadoop集群、设计实验方案、编写核心MapReduce代码 |
| 杨牧之 | 51285903025 | 23%      | 搭建Hadoop集群、设计实验方案、编写核心MapReduce代码 |
| 周冠廷 | 51285903023 | 25%      | 数据集准备、实验结果分析、编写readme文件            |
| 郭星豪 | 51285903100 | 25%      | 制作ppt、演示视频                                   |
