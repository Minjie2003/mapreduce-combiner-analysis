# é¡¹ç›®å®Œæ•´æŒ‡å—

------

## ğŸ“‹ é¡¹ç›®æ€»è§ˆ

**ç›®æ ‡**: é€šè¿‡å®éªŒéªŒè¯Combineråœ¨ä¸åŒæ•°æ®åˆ†å¸ƒä¸‹å¯¹MapReduceæ€§èƒ½çš„å½±å“

####  **ç ”ç©¶å†…å®¹**

1. åˆ†æCombineråœ¨MapReduceæ‰§è¡Œè¿‡ç¨‹ä¸­çš„ä½œç”¨ä¸æ•ˆæœã€‚
2. å…·ä½“åŒ…æ‹¬ï¼šCombineræ˜¯å¦èƒ½ å¤Ÿæœ‰æ•ˆå‡å°‘Shuffleé˜¶æ®µçš„æ•°æ®é‡ï¼Ÿ
3. åœ¨ä¸åŒçš„keyåˆ†å¸ƒï¼ˆå‡åŒ€åˆ†å¸ƒä¸æ•°æ®å€¾æ–œï¼‰ä¸‹ï¼Œå…¶æ€§èƒ½æå‡æ•ˆæœæœ‰ä½•å·®å¼‚ï¼Ÿ
4. æ˜¯å¦æ‰€æœ‰åœºæ™¯éƒ½é€‚åˆä½¿ç”¨Combinerï¼Ÿ

------

# ğŸ—“ï¸ Part 1: ç¯å¢ƒæ­å»º 

### Step 1:å®‰è£…WSL

**WSLåŠŸèƒ½çš„å¼€å¯**

1. æŒ‰ `Win`ï¼Œæœç´¢â€œå¯ç”¨æˆ–å…³é—­ Windows åŠŸèƒ½â€å¹¶æ‰“å¼€ã€‚

2. å¼€å¯ç›¸å…³åŠŸèƒ½-â€œé€‚ç”¨äº Linux çš„ Windows å­ç³»ç»Ÿâ€ å’Œ  â€œè™šæ‹Ÿæœºå¹³å°ï¼ˆVirtual Machine Platformï¼‰

   ![image-20251117210411627](docs\images\image-20251117212255838.png)

**å®‰è£…WSL**

```bash
 wsl --install   #å®‰è£…wslï¼Œå› ä¸ºæœ€æ–°windowsæ›´æ–°åçš„å…¼å®¹é—®é¢˜å¯¼è‡´å¿…é¡»å®‰è£…wslï¼Œä¼šè‡ªåŠ¨å®‰è£…ubuntu
 wsl --set-default-version 2  # wsl --set-default-version 2

#æµ‹è¯•wslæ˜¯å¦æœ‰å®‰è£…ä¸Šï¼Œå¦åˆ™åç»­æ‰“å¼€dockerä¼šæŠ¥é”™
wsl -v

```

### Step 2: å®‰è£…Docker Desktop

**Windowsç”¨æˆ·ï¼š**

```bash
# 1. ä¸‹è½½Docker Desktop
# è®¿é—®: https://www.docker.com/products/docker-desktop/

# 2. å®‰è£…å¹¶é‡å¯ç”µè„‘

# 3. éªŒè¯å®‰è£…
docker --version
docker-compose --version
```

**é…ç½®Dockerèµ„æºï¼š**

- æ‰“å¼€Docker Desktop
- Settings â†’ Resources
- Memory: è‡³å°‘ 8GB
- CPUs: è‡³å°‘ 4æ ¸
- Disk: è‡³å°‘ 50GB

### Step 3: åˆ›å»ºé¡¹ç›®ç»“æ„

```bash
# åˆ›å»ºé¡¹ç›®æ ¹ç›®å½•
mkdir mapreduce-combiner-analysis
cd mapreduce-combiner-analysis

# åˆ›å»ºå­ç›®å½•
mkdir -p code/mapreduce/with-combiner
mkdir -p code/mapreduce/without-combiner
mkdir -p code/data-generator
mkdir -p code/cluster-config
mkdir -p code/results
mkdir -p code/scripts
mkdir -p docs/images  #å†™markdownç”¨åˆ°çš„å›¾ç‰‡é“¾æ¥


```

### Step 4: åˆ›å»ºDockeré…ç½®

**åˆ›å»º `cluster-config/docker-compose.yml`:**

docker-compose.yml

Code 

version: '3.8' services:  namenode:    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8    container_name: namenode    restart: always    ports:      - 9870:9870  # NameNode Web UI      - 9000:9000  # HDFSç«¯å£    volumes:      - hadoop_

**åˆ›å»º `cluster-config/hadoop.env`:**

hadoop.env

Code 

\# Coreé…ç½® CORE_CONF_fs_defaultFS=hdfs://namenode:9000 CORE_CONF_hadoop_http_staticuser_user=root CORE_CONF_hadoop_proxyuser_hue_hosts=* CORE_CONF_hadoop_proxyuser_hue_groups=* CORE_CONF_io_compression_codecs=org.apache.hadoop.io.compress.SnappyCodec

### Step 5: å¯åŠ¨é›†ç¾¤

bash

```bash
# è¿›å…¥é…ç½®ç›®å½•
cd cluster-config

# åˆ›å»ºå…±äº«ç›®å½•
mkdir shared

# å¯åŠ¨é›†ç¾¤ï¼ˆé¦–æ¬¡å¯åŠ¨ä¼šä¸‹è½½é•œåƒï¼Œéœ€è¦5-10åˆ†é’Ÿï¼‰
docker-compose up -d

# æŸ¥çœ‹å¯åŠ¨çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—ï¼ˆç¡®ä¿æ²¡æœ‰é”™è¯¯ï¼‰
docker-compose logs -f

# ç­‰å¾…æ‰€æœ‰æœåŠ¡å¯åŠ¨ï¼ˆçº¦2-3åˆ†é’Ÿï¼‰
```

### Step 6: éªŒè¯é›†ç¾¤

**1. æ£€æŸ¥Webç•Œé¢ï¼š**

- NameNode: http://localhost:9870
- YARN: http://localhost:8088
- JobHistory: http://localhost:8188

**æˆªå›¾è¦æ±‚ï¼š**

- æ‰“å¼€NameNodeç•Œé¢ï¼Œæˆªå›¾æ˜¾ç¤º"Live Nodes: 3"
- æ‰“å¼€YARNç•Œé¢ï¼Œæˆªå›¾æ˜¾ç¤ºResourceManagerè¿è¡Œæ­£å¸¸
- ç¡®ä¿æˆªå›¾ä¸­åŒ…å«ä½ çš„ç”µè„‘ç”¨æˆ·åæˆ–æ—¶é—´æˆ³

**2. æµ‹è¯•HDFSï¼š**

bash

```bash
# è¿›å…¥namenodeå®¹å™¨
docker exec -it namenode bash

# æŸ¥çœ‹HDFSçŠ¶æ€
hdfs dfsadmin -report

# åº”è¯¥çœ‹åˆ°3ä¸ªDataNodeï¼Œæ¯ä¸ªçŠ¶æ€ä¸ºLive

# åˆ›å»ºæµ‹è¯•ç›®å½•
hdfs dfs -mkdir -p /user/root/test

# æµ‹è¯•æ–‡ä»¶ä¸Šä¼ 
echo "Hello Hadoop Cluster" > /tmp/test.txt
hdfs dfs -put /tmp/test.txt /user/root/test/

# æŸ¥çœ‹æ–‡ä»¶
hdfs dfs -ls /user/root/test/
hdfs dfs -cat /user/root/test/test.txt

# é€€å‡ºå®¹å™¨
exit
```

------

------

## ğŸ“š å‚è€ƒèµ„æ–™

1. Hadoopå®˜æ–¹æ–‡æ¡£: https://hadoop.apache.org/docs/r3.2.1/
2. MapReduceè®ºæ–‡: Dean, Jeffrey, and Sanjay Ghemawat. "MapReduce: simplified data processing on large clusters."
3. Dockeræ–‡æ¡£: https://docs.docker.com/

